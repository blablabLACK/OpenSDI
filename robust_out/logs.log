[DEBUG] Local IMDLBenCo path: /mnt/data01/AIGC/projects/IMDLBenCo-main[DEBUG] Local IMDLBenCo path: /mnt/data01/AIGC/projects/IMDLBenCo-main[DEBUG] Local IMDLBenCo path: /mnt/data01/AIGC/projects/IMDLBenCo-main[DEBUG] Local IMDLBenCo path: /mnt/data01/AIGC/projects/IMDLBenCo-main



2.7.1+cu126
Current Torch version: 2.7
2.7.1+cu126
Current Torch version: 2.7
2.7.1+cu126
Current Torch version: 2.7
2.7.1+cu126
Current Torch version: 2.7
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
[03:01:55.899391] [INFO] Loading MaskCLIP ...
[03:02:06.490714] [DEBUG] datasets path: [['coverage', '/home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json'], ['casia_v1', '/home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json'], ['columbia', '/home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json'], ['nist2016', '/home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json']]
[03:02:06.490855] 
[ROBUST TEST] RotationWrapper param=0
[03:02:06.491973] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:02:08.151816] Test: [0]  [0/2]  eta: 0:00:03  pixel-level F1: [local: 0.7129 | reduced: 0.7129]  time: 1.6561  data: 0.9723  max mem: 5662
[03:02:08.411794] ====================
[03:02:08.411873] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:02:08.411893] ====================
[03:02:08.431152] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.6005 | reduced: 0.6591]  time: 0.9675  data: 0.4862  max mem: 5662
[03:02:08.534722] Test: [0] Total time: 0:00:02 (1.0197 s / it)
[03:02:08.534851] ***************************************************************
[03:02:08.534878] ****An extra tail dataset should exist for accracy metrics!****
[03:02:08.534897] ***************************************************************
[03:02:08.534916] **** Length of tail: 43 ****
[03:02:09.198034] Actual Batchsize/ world_size {'_n': 3.0}
[03:02:09.198243] {'pixel-level F1': tensor(1.8837, device='cuda:0', dtype=torch.float64)}
[03:02:09.246370] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6279 | reduced: 0.6555]  time: 0.7108  data: 0.3689  max mem: 5752
[03:02:09.781081] Actual Batchsize/ world_size {'_n': 3.0}
[03:02:09.781204] {'pixel-level F1': tensor(1.5304, device='cuda:0', dtype=torch.float64)}
[03:02:09.803283] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.6005 | reduced: 0.6405]  time: 0.6337  data: 0.3054  max mem: 5776
[03:02:10.471676] Actual Batchsize/ world_size {'_n': 3.0}
[03:02:10.471799] {'pixel-level F1': tensor(0.9833, device='cuda:0', dtype=torch.float64)}
[03:02:10.493791] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.6005 | reduced: 0.6112]  time: 0.6525  data: 0.3287  max mem: 5776
[03:02:10.855043] ====================
[03:02:10.855155] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:02:10.855177] ====================
[03:02:10.855552] Actual Batchsize/ world_size {'_n': 1.75}
[03:02:10.855612] {'pixel-level F1': tensor(0.7251, device='cuda:0', dtype=torch.float64)}
[03:02:10.867228] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5101 | reduced: 0.6010]  time: 0.5826  data: 0.2913  max mem: 5776
[03:02:10.867357] Test <remaining>: [0] Total time: 0:00:02 (0.5830 s / it)
[03:02:10.874433] ---syncronized---
[03:02:10.874475] pixel-level F1 reduced_count 135
[03:02:10.874508] pixel-level F1 reduced_sum 66.58892175709488
[03:02:10.874540] ---syncronized done ---
[03:02:13.533099] Averaged stats: pixel-level F1: [local: 0.5101 | reduced: 0.4933]
[03:02:13.536367] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:02:14.886061] Test: [0]  [ 0/20]  eta: 0:00:26  pixel-level F1: [local: 0.4670 | reduced: 0.4670]  time: 1.3431  data: 0.9665  max mem: 5776
[03:02:20.612609] ====================
[03:02:20.612701] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:02:20.612720] ====================
[03:02:20.614069] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.8060 | reduced: 0.7651]  time: 0.3535  data: 0.0484  max mem: 5776
[03:02:20.695442] Test: [0] Total time: 0:00:07 (0.3577 s / it)
[03:02:20.695547] ***************************************************************
[03:02:20.695571] ****An extra tail dataset should exist for accracy metrics!****
[03:02:20.695589] ***************************************************************
[03:02:20.695608] **** Length of tail: 8 ****
[03:02:21.011972] ====================
[03:02:21.012058] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:02:21.012078] ====================
[03:02:21.012472] Actual Batchsize/ world_size {'_n': 2.0}
[03:02:21.012545] {'pixel-level F1': tensor(1.8922, device='cuda:0', dtype=torch.float64)}
[03:02:21.025902] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.8293 | reduced: 0.7667]  time: 0.3298  data: 0.1173  max mem: 5776
[03:02:21.026018] Test <remaining>: [0] Total time: 0:00:00 (0.3302 s / it)
[03:02:21.026618] ---syncronized---
[03:02:21.026654] pixel-level F1 reduced_count 928
[03:02:21.026691] pixel-level F1 reduced_sum 713.0841862657405
[03:02:21.026726] ---syncronized done ---
[03:02:21.432224] Averaged stats: pixel-level F1: [local: 0.8293 | reduced: 0.7684]
[03:02:21.433398] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:02:23.200107] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8216 | reduced: 0.8216]  time: 1.7625  data: 1.4529  max mem: 5776
[03:02:24.038890] ====================
[03:02:24.039014] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:02:24.039035] ====================
[03:02:24.054081] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8216 | reduced: 0.8470]  time: 0.6540  data: 0.3633  max mem: 5776
[03:02:24.127486] Test: [0] Total time: 0:00:02 (0.6726 s / it)
[03:02:24.127591] ***************************************************************
[03:02:24.127614] ****An extra tail dataset should exist for accracy metrics!****
[03:02:24.127630] ***************************************************************
[03:02:24.127648] **** Length of tail: 36 ****
[03:02:25.229545] Actual Batchsize/ world_size {'_n': 3.0}
[03:02:25.229701] {'pixel-level F1': tensor(2.9562, device='cuda:0', dtype=torch.float64)}
[03:02:25.251351] Test <remaining>: [0]  [0/3]  eta: 0:00:03  pixel-level F1: [local: 0.8384 | reduced: 0.8557]  time: 1.1232  data: 0.8077  max mem: 5776
[03:02:26.149195] Actual Batchsize/ world_size {'_n': 3.0}
[03:02:26.149336] {'pixel-level F1': tensor(2.3591, device='cuda:0', dtype=torch.float64)}
[03:02:26.170997] Test <remaining>: [0]  [1/3]  eta: 0:00:02  pixel-level F1: [local: 0.8216 | reduced: 0.8516]  time: 1.0212  data: 0.7057  max mem: 5776
[03:02:26.990551] Actual Batchsize/ world_size {'_n': 3.0}
[03:02:26.990687] {'pixel-level F1': tensor(2.7250, device='cuda:0', dtype=torch.float64)}
[03:02:27.012590] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8384 | reduced: 0.8547]  time: 0.9612  data: 0.6463  max mem: 5776
[03:02:27.012718] Test <remaining>: [0] Total time: 0:00:02 (0.9616 s / it)
[03:02:27.013179] ---syncronized---
[03:02:27.013214] pixel-level F1 reduced_count 216
[03:02:27.013248] pixel-level F1 reduced_sum 185.4626429171085
[03:02:27.013283] ---syncronized done ---
[03:02:29.369081] Averaged stats: pixel-level F1: [local: 0.8384 | reduced: 0.8586]
[03:02:29.372040] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:02:54.240106] Test: [0]  [ 0/12]  eta: 0:04:58  pixel-level F1: [local: 0.4509 | reduced: 0.4509]  time: 24.8625  data: 24.5518  max mem: 5776
[03:03:04.576788] ====================
[03:03:04.576939] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:03:04.576958] ====================
[03:03:04.592493] Test: [0]  [11/12]  eta: 0:00:02  pixel-level F1: [local: 0.2919 | reduced: 0.3150]  time: 2.9345  data: 2.6321  max mem: 5776
[03:03:04.716140] Test: [0] Total time: 0:00:35 (2.9449 s / it)
[03:03:04.716244] ***************************************************************
[03:03:04.716266] ****An extra tail dataset should exist for accracy metrics!****
[03:03:04.716283] ***************************************************************
[03:03:04.716302] **** Length of tail: 36 ****
[03:03:25.934067] Actual Batchsize/ world_size {'_n': 3.0}
[03:03:25.934224] {'pixel-level F1': tensor(1.0102, device='cuda:0', dtype=torch.float64)}
[03:03:25.955544] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.3367 | reduced: 0.3154]  time: 21.2386  data: 20.9217  max mem: 5776
[03:03:47.032099] Actual Batchsize/ world_size {'_n': 3.0}
[03:03:47.032267] {'pixel-level F1': tensor(0.7156, device='cuda:0', dtype=torch.float64)}
[03:03:47.054060] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.2919 | reduced: 0.3139]  time: 21.1684  data: 20.8523  max mem: 5776
[03:04:06.920579] Actual Batchsize/ world_size {'_n': 3.0}
[03:04:06.920747] {'pixel-level F1': tensor(1.2142, device='cuda:0', dtype=torch.float64)}
[03:04:06.942286] Test <remaining>: [0]  [2/3]  eta: 0:00:20  pixel-level F1: [local: 0.3367 | reduced: 0.3157]  time: 20.7415  data: 20.4254  max mem: 5776
[03:04:06.942430] Test <remaining>: [0] Total time: 0:01:02 (20.7420 s / it)
[03:04:06.943276] ---syncronized---
[03:04:06.943314] pixel-level F1 reduced_count 600
[03:04:06.943347] pixel-level F1 reduced_sum 214.13414037837964
[03:04:06.943380] ---syncronized done ---
[03:04:09.133213] Averaged stats: pixel-level F1: [local: 0.3367 | reduced: 0.3569]
[03:04:09.136750] 
[ROBUST TEST] RotationWrapper param=2
[03:04:09.137759] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:04:10.409367] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.4674 | reduced: 0.4674]  time: 1.2676  data: 0.9577  max mem: 5776
[03:04:10.669240] ====================
[03:04:10.669341] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:04:10.669362] ====================
[03:04:10.688799] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.4172 | reduced: 0.4434]  time: 0.7733  data: 0.4789  max mem: 5776
[03:04:10.755643] Test: [0] Total time: 0:00:01 (0.8071 s / it)
[03:04:10.755758] ***************************************************************
[03:04:10.755781] ****An extra tail dataset should exist for accracy metrics!****
[03:04:10.755799] ***************************************************************
[03:04:10.755819] **** Length of tail: 43 ****
[03:04:11.317814] Actual Batchsize/ world_size {'_n': 3.0}
[03:04:11.317944] {'pixel-level F1': tensor(1.1145, device='cuda:0', dtype=torch.float64)}
[03:04:11.339877] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4172 | reduced: 0.4351]  time: 0.5836  data: 0.2693  max mem: 5776
[03:04:11.837412] Actual Batchsize/ world_size {'_n': 3.0}
[03:04:11.837537] {'pixel-level F1': tensor(0.8092, device='cuda:0', dtype=torch.float64)}
[03:04:11.859483] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.3715 | reduced: 0.4180]  time: 0.5514  data: 0.2371  max mem: 5776
[03:04:12.390128] Actual Batchsize/ world_size {'_n': 3.0}
[03:04:12.390269] {'pixel-level F1': tensor(1.0987, device='cuda:0', dtype=torch.float64)}
[03:04:12.412143] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.3715 | reduced: 0.4131]  time: 0.5517  data: 0.2374  max mem: 5776
[03:04:12.729403] ====================
[03:04:12.729502] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:04:12.729523] ====================
[03:04:12.729926] Actual Batchsize/ world_size {'_n': 1.75}
[03:04:12.729987] {'pixel-level F1': tensor(0.9657, device='cuda:0', dtype=torch.float64)}
[03:04:12.741811] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.3715 | reduced: 0.4203]  time: 0.4961  data: 0.2120  max mem: 5776
[03:04:12.741937] Test <remaining>: [0] Total time: 0:00:01 (0.4965 s / it)
[03:04:12.742564] ---syncronized---
[03:04:12.742605] pixel-level F1 reduced_count 135
[03:04:12.742636] pixel-level F1 reduced_sum 51.0840390281337
[03:04:12.742670] ---syncronized done ---
[03:04:15.502115] Averaged stats: pixel-level F1: [local: 0.3715 | reduced: 0.3784]
[03:04:15.505462] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:04:16.562679] Test: [0]  [ 0/20]  eta: 0:00:21  pixel-level F1: [local: 0.2496 | reduced: 0.2496]  time: 1.0505  data: 0.7364  max mem: 5776
[03:04:22.167965] ====================
[03:04:22.168063] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:04:22.168082] ====================
[03:04:22.169847] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.4960 | reduced: 0.5547]  time: 0.3328  data: 0.0369  max mem: 5776
[03:04:22.249705] Test: [0] Total time: 0:00:06 (0.3369 s / it)
[03:04:22.249807] ***************************************************************
[03:04:22.249830] ****An extra tail dataset should exist for accracy metrics!****
[03:04:22.249846] ***************************************************************
[03:04:22.249865] **** Length of tail: 8 ****
[03:04:22.563784] ====================
[03:04:22.563876] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:04:22.563896] ====================
[03:04:22.564286] Actual Batchsize/ world_size {'_n': 2.0}
[03:04:22.564353] {'pixel-level F1': tensor(1.7284, device='cuda:0', dtype=torch.float64)}
[03:04:22.577908] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.5685 | reduced: 0.5573]  time: 0.3276  data: 0.1152  max mem: 5776
[03:04:22.578026] Test <remaining>: [0] Total time: 0:00:00 (0.3280 s / it)
[03:04:22.578610] ---syncronized---
[03:04:22.578647] pixel-level F1 reduced_count 928
[03:04:22.578679] pixel-level F1 reduced_sum 510.98166227831354
[03:04:22.578713] ---syncronized done ---
[03:04:23.082149] Averaged stats: pixel-level F1: [local: 0.5685 | reduced: 0.5506]
[03:04:23.083352] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:04:24.809053] Test: [0]  [0/4]  eta: 0:00:06  pixel-level F1: [local: 0.8166 | reduced: 0.8166]  time: 1.7214  data: 1.4116  max mem: 5776
[03:04:25.647288] ====================
[03:04:25.647415] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:04:25.647435] ====================
[03:04:25.662932] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8166 | reduced: 0.8535]  time: 0.6437  data: 0.3530  max mem: 5776
[03:04:25.729255] Test: [0] Total time: 0:00:02 (0.6605 s / it)
[03:04:25.729377] ***************************************************************
[03:04:25.729401] ****An extra tail dataset should exist for accracy metrics!****
[03:04:25.729420] ***************************************************************
[03:04:25.729438] **** Length of tail: 36 ****
[03:04:26.705833] Actual Batchsize/ world_size {'_n': 3.0}
[03:04:26.705977] {'pixel-level F1': tensor(2.9243, device='cuda:0', dtype=torch.float64)}
[03:04:26.727759] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.9033 | reduced: 0.8611]  time: 0.9979  data: 0.6822  max mem: 5776
[03:04:27.650942] Actual Batchsize/ world_size {'_n': 3.0}
[03:04:27.651074] {'pixel-level F1': tensor(2.3386, device='cuda:0', dtype=torch.float64)}
[03:04:27.672923] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8166 | reduced: 0.8563]  time: 0.9713  data: 0.6556  max mem: 5776
[03:04:28.528552] Actual Batchsize/ world_size {'_n': 3.0}
[03:04:28.528681] {'pixel-level F1': tensor(2.5203, device='cuda:0', dtype=torch.float64)}
[03:04:28.550606] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8401 | reduced: 0.8554]  time: 0.9400  data: 0.6247  max mem: 5776
[03:04:28.550727] Test <remaining>: [0] Total time: 0:00:02 (0.9404 s / it)
[03:04:28.551214] ---syncronized---
[03:04:28.551247] pixel-level F1 reduced_count 216
[03:04:28.551279] pixel-level F1 reduced_sum 173.6724078215627
[03:04:28.551314] ---syncronized done ---
[03:04:31.005349] Averaged stats: pixel-level F1: [local: 0.8401 | reduced: 0.8040]
[03:04:31.008467] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:04:56.049371] Test: [0]  [ 0/12]  eta: 0:05:00  pixel-level F1: [local: 0.3511 | reduced: 0.3511]  time: 25.0354  data: 24.7241  max mem: 5776
[03:05:07.915190] ====================
[03:05:07.915313] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:05:07.915333] ====================
[03:05:07.930938] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2355 | reduced: 0.2328]  time: 3.0764  data: 2.7741  max mem: 5776
[03:05:08.046856] Test: [0] Total time: 0:00:37 (3.0861 s / it)
[03:05:08.048648] ***************************************************************
[03:05:08.048784] ****An extra tail dataset should exist for accracy metrics!****
[03:05:08.048804] ***************************************************************
[03:05:08.048823] **** Length of tail: 36 ****
[03:05:30.691983] Actual Batchsize/ world_size {'_n': 3.0}
[03:05:30.692305] {'pixel-level F1': tensor(0.7700, device='cuda:0', dtype=torch.float64)}
[03:05:30.713096] Test <remaining>: [0]  [0/3]  eta: 0:01:07  pixel-level F1: [local: 0.2567 | reduced: 0.2333]  time: 22.6618  data: 22.3381  max mem: 5776
[03:05:52.359985] Actual Batchsize/ world_size {'_n': 3.0}
[03:05:52.360163] {'pixel-level F1': tensor(0.4922, device='cuda:0', dtype=torch.float64)}
[03:05:52.381171] Test <remaining>: [0]  [1/3]  eta: 0:00:44  pixel-level F1: [local: 0.2355 | reduced: 0.2319]  time: 22.1647  data: 21.8441  max mem: 5776
[03:06:12.474463] Actual Batchsize/ world_size {'_n': 3.0}
[03:06:12.474629] {'pixel-level F1': tensor(1.2031, device='cuda:0', dtype=torch.float64)}
[03:06:12.495622] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.2567 | reduced: 0.2353]  time: 21.4811  data: 21.1618  max mem: 5776
[03:06:12.495814] Test <remaining>: [0] Total time: 0:01:04 (21.4822 s / it)
[03:06:12.497356] ---syncronized---
[03:06:12.497419] pixel-level F1 reduced_count 600
[03:06:12.497453] pixel-level F1 reduced_sum 170.58566967735732
[03:06:12.497485] ---syncronized done ---
[03:06:14.840283] Averaged stats: pixel-level F1: [local: 0.2567 | reduced: 0.2843]
[03:06:14.844730] 
[ROBUST TEST] RotationWrapper param=4
[03:06:14.846275] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:06:16.122395] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.4773 | reduced: 0.4773]  time: 1.2711  data: 0.9604  max mem: 5776
[03:06:16.382932] ====================
[03:06:16.383011] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:06:16.383037] ====================
[03:06:16.402196] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.4591 | reduced: 0.4686]  time: 0.7752  data: 0.4803  max mem: 5776
[03:06:16.480560] Test: [0] Total time: 0:00:01 (0.8148 s / it)
[03:06:16.480675] ***************************************************************
[03:06:16.480697] ****An extra tail dataset should exist for accracy metrics!****
[03:06:16.480714] ***************************************************************
[03:06:16.480738] **** Length of tail: 43 ****
[03:06:17.049245] Actual Batchsize/ world_size {'_n': 3.0}
[03:06:17.049374] {'pixel-level F1': tensor(1.1616, device='cuda:0', dtype=torch.float64)}
[03:06:17.070951] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4591 | reduced: 0.4592]  time: 0.5897  data: 0.2747  max mem: 5776
[03:06:17.571939] Actual Batchsize/ world_size {'_n': 3.0}
[03:06:17.572054] {'pixel-level F1': tensor(1.1635, device='cuda:0', dtype=torch.float64)}
[03:06:17.593704] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.3878 | reduced: 0.4518]  time: 0.5561  data: 0.2410  max mem: 5776
[03:06:18.139143] Actual Batchsize/ world_size {'_n': 3.0}
[03:06:18.139258] {'pixel-level F1': tensor(0.9620, device='cuda:0', dtype=torch.float64)}
[03:06:18.160923] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.3878 | reduced: 0.4395]  time: 0.5597  data: 0.2446  max mem: 5776
[03:06:18.478349] ====================
[03:06:18.478444] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:06:18.478465] ====================
[03:06:18.478889] Actual Batchsize/ world_size {'_n': 1.75}
[03:06:18.478947] {'pixel-level F1': tensor(0.8728, device='cuda:0', dtype=torch.float64)}
[03:06:18.490474] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.3878 | reduced: 0.4426]  time: 0.5020  data: 0.2172  max mem: 5776
[03:06:18.490596] Test <remaining>: [0] Total time: 0:00:02 (0.5024 s / it)
[03:06:18.491521] ---syncronized---
[03:06:18.491554] pixel-level F1 reduced_count 135
[03:06:18.491585] pixel-level F1 reduced_sum 54.21661209944506
[03:06:18.491615] ---syncronized done ---
[03:06:21.241299] Averaged stats: pixel-level F1: [local: 0.3878 | reduced: 0.4016]
[03:06:21.244799] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:06:22.269816] Test: [0]  [ 0/20]  eta: 0:00:20  pixel-level F1: [local: 0.1488 | reduced: 0.1488]  time: 1.0180  data: 0.7052  max mem: 5776
[03:06:27.880829] ====================
[03:06:27.880919] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:06:27.880939] ====================
[03:06:27.882389] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.3970 | reduced: 0.5086]  time: 0.3315  data: 0.0353  max mem: 5776
[03:06:27.966797] Test: [0] Total time: 0:00:06 (0.3358 s / it)
[03:06:27.966904] ***************************************************************
[03:06:27.966926] ****An extra tail dataset should exist for accracy metrics!****
[03:06:27.966942] ***************************************************************
[03:06:27.966960] **** Length of tail: 8 ****
[03:06:28.288065] ====================
[03:06:28.288170] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:06:28.288190] ====================
[03:06:28.288625] Actual Batchsize/ world_size {'_n': 2.0}
[03:06:28.288692] {'pixel-level F1': tensor(1.7003, device='cuda:0', dtype=torch.float64)}
[03:06:28.301895] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.5548 | reduced: 0.5116]  time: 0.3345  data: 0.1212  max mem: 5776
[03:06:28.302013] Test <remaining>: [0] Total time: 0:00:00 (0.3349 s / it)
[03:06:28.302661] ---syncronized---
[03:06:28.302697] pixel-level F1 reduced_count 928
[03:06:28.302736] pixel-level F1 reduced_sum 478.51582502121084
[03:06:28.302768] ---syncronized done ---
[03:06:28.791477] Averaged stats: pixel-level F1: [local: 0.5548 | reduced: 0.5156]
[03:06:28.792713] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:06:30.596012] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8004 | reduced: 0.8004]  time: 1.7986  data: 1.4884  max mem: 5776
[03:06:31.434879] ====================
[03:06:31.435001] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:06:31.435021] ====================
[03:06:31.450225] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8004 | reduced: 0.8213]  time: 0.6631  data: 0.3722  max mem: 5776
[03:06:31.524261] Test: [0] Total time: 0:00:02 (0.6818 s / it)
[03:06:31.524381] ***************************************************************
[03:06:31.524403] ****An extra tail dataset should exist for accracy metrics!****
[03:06:31.524420] ***************************************************************
[03:06:31.524438] **** Length of tail: 36 ****
[03:06:32.495287] Actual Batchsize/ world_size {'_n': 3.0}
[03:06:32.495419] {'pixel-level F1': tensor(2.9347, device='cuda:0', dtype=torch.float64)}
[03:06:32.516978] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8674 | reduced: 0.8311]  time: 0.9921  data: 0.6764  max mem: 5776
[03:06:33.447279] Actual Batchsize/ world_size {'_n': 3.0}
[03:06:33.447408] {'pixel-level F1': tensor(2.2536, device='cuda:0', dtype=torch.float64)}
[03:06:33.468923] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8004 | reduced: 0.8264]  time: 0.9718  data: 0.6563  max mem: 5776
[03:06:34.343068] Actual Batchsize/ world_size {'_n': 3.0}
[03:06:34.343190] {'pixel-level F1': tensor(2.4371, device='cuda:0', dtype=torch.float64)}
[03:06:34.364834] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8124 | reduced: 0.8256]  time: 0.9464  data: 0.6309  max mem: 5776
[03:06:34.364963] Test <remaining>: [0] Total time: 0:00:02 (0.9468 s / it)
[03:06:34.365586] ---syncronized---
[03:06:34.365625] pixel-level F1 reduced_count 216
[03:06:34.365657] pixel-level F1 reduced_sum 170.23742509070894
[03:06:34.365689] ---syncronized done ---
[03:06:36.825241] Averaged stats: pixel-level F1: [local: 0.8124 | reduced: 0.7881]
[03:06:36.828408] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:07:02.770120] Test: [0]  [ 0/12]  eta: 0:05:11  pixel-level F1: [local: 0.2399 | reduced: 0.2399]  time: 25.9360  data: 25.6247  max mem: 5776
[03:07:14.169838] ====================
[03:07:14.170001] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:07:14.170021] ====================
[03:07:14.185384] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2036 | reduced: 0.1892]  time: 3.1125  data: 2.8096  max mem: 5776
[03:07:14.313288] Test: [0] Total time: 0:00:37 (3.1233 s / it)
[03:07:14.313402] ***************************************************************
[03:07:14.313423] ****An extra tail dataset should exist for accracy metrics!****
[03:07:14.313440] ***************************************************************
[03:07:14.313459] **** Length of tail: 36 ****
[03:07:35.464060] Actual Batchsize/ world_size {'_n': 3.0}
[03:07:35.465675] {'pixel-level F1': tensor(0.7304, device='cuda:0', dtype=torch.float64)}
[03:07:35.485296] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.2060 | reduced: 0.1904]  time: 21.1712  data: 20.8549  max mem: 5776
[03:07:57.263435] Actual Batchsize/ world_size {'_n': 3.0}
[03:07:57.263577] {'pixel-level F1': tensor(0.4613, device='cuda:0', dtype=torch.float64)}
[03:07:57.284835] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.2036 | reduced: 0.1896]  time: 21.4852  data: 21.1689  max mem: 5776
[03:08:17.671067] Actual Batchsize/ world_size {'_n': 3.0}
[03:08:17.671213] {'pixel-level F1': tensor(1.1795, device='cuda:0', dtype=torch.float64)}
[03:08:17.692621] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.2060 | reduced: 0.1937]  time: 21.1259  data: 20.8096  max mem: 5776
[03:08:17.692787] Test <remaining>: [0] Total time: 0:01:03 (21.1264 s / it)
[03:08:17.693662] ---syncronized---
[03:08:17.693705] pixel-level F1 reduced_count 600
[03:08:17.693737] pixel-level F1 reduced_sum 148.95595261385995
[03:08:17.693768] ---syncronized done ---
[03:08:20.019727] Averaged stats: pixel-level F1: [local: 0.2060 | reduced: 0.2483]
[03:08:20.023367] 
[ROBUST TEST] RotationWrapper param=6
[03:08:20.024225] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:08:21.265056] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.5607 | reduced: 0.5607]  time: 1.2367  data: 0.9265  max mem: 5776
[03:08:21.525167] ====================
[03:08:21.525245] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:08:21.525266] ====================
[03:08:21.544478] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.4423 | reduced: 0.5041]  time: 0.7579  data: 0.4633  max mem: 5776
[03:08:21.623449] Test: [0] Total time: 0:00:01 (0.7978 s / it)
[03:08:21.623564] ***************************************************************
[03:08:21.623587] ****An extra tail dataset should exist for accracy metrics!****
[03:08:21.623605] ***************************************************************
[03:08:21.623622] **** Length of tail: 43 ****
[03:08:22.167760] Actual Batchsize/ world_size {'_n': 3.0}
[03:08:22.167881] {'pixel-level F1': tensor(1.2204, device='cuda:0', dtype=torch.float64)}
[03:08:22.189596] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4423 | reduced: 0.4929]  time: 0.5656  data: 0.2510  max mem: 5776
[03:08:22.684436] Actual Batchsize/ world_size {'_n': 3.0}
[03:08:22.684560] {'pixel-level F1': tensor(0.9739, device='cuda:0', dtype=torch.float64)}
[03:08:22.706259] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.4068 | reduced: 0.4755]  time: 0.5409  data: 0.2263  max mem: 5776
[03:08:23.241045] Actual Batchsize/ world_size {'_n': 3.0}
[03:08:23.241164] {'pixel-level F1': tensor(0.9802, device='cuda:0', dtype=torch.float64)}
[03:08:23.262842] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.4068 | reduced: 0.4615]  time: 0.5460  data: 0.2314  max mem: 5776
[03:08:23.575103] ====================
[03:08:23.575199] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:08:23.575219] ====================
[03:08:23.575641] Actual Batchsize/ world_size {'_n': 1.75}
[03:08:23.575701] {'pixel-level F1': tensor(0.7180, device='cuda:0', dtype=torch.float64)}
[03:08:23.587347] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.4068 | reduced: 0.4589]  time: 0.4906  data: 0.2060  max mem: 5776
[03:08:23.587493] Test <remaining>: [0] Total time: 0:00:01 (0.4909 s / it)
[03:08:23.588142] ---syncronized---
[03:08:23.588175] pixel-level F1 reduced_count 135
[03:08:23.588207] pixel-level F1 reduced_sum 49.369649358843276
[03:08:23.588239] ---syncronized done ---
[03:08:26.340745] Averaged stats: pixel-level F1: [local: 0.4068 | reduced: 0.3657]
[03:08:26.344244] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:08:27.636805] Test: [0]  [ 0/20]  eta: 0:00:25  pixel-level F1: [local: 0.0954 | reduced: 0.0954]  time: 1.2858  data: 0.9704  max mem: 5776
[03:08:33.242538] ====================
[03:08:33.242628] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:08:33.242652] ====================
[03:08:33.244350] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.3857 | reduced: 0.4674]  time: 0.3446  data: 0.0486  max mem: 5776
[03:08:33.328656] Test: [0] Total time: 0:00:06 (0.3489 s / it)
[03:08:33.328755] ***************************************************************
[03:08:33.328776] ****An extra tail dataset should exist for accracy metrics!****
[03:08:33.328793] ***************************************************************
[03:08:33.328811] **** Length of tail: 8 ****
[03:08:33.636432] ====================
[03:08:33.636510] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:08:33.636529] ====================
[03:08:33.636950] Actual Batchsize/ world_size {'_n': 2.0}
[03:08:33.637015] {'pixel-level F1': tensor(1.7326, device='cuda:0', dtype=torch.float64)}
[03:08:33.650393] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.5657 | reduced: 0.4708]  time: 0.3211  data: 0.1083  max mem: 5776
[03:08:33.650505] Test <remaining>: [0] Total time: 0:00:00 (0.3215 s / it)
[03:08:33.651241] ---syncronized---
[03:08:33.651274] pixel-level F1 reduced_count 928
[03:08:33.651304] pixel-level F1 reduced_sum 442.0469786520338
[03:08:33.651334] ---syncronized done ---
[03:08:34.154427] Averaged stats: pixel-level F1: [local: 0.5657 | reduced: 0.4763]
[03:08:34.155660] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:08:35.950917] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8683 | reduced: 0.8683]  time: 1.7908  data: 1.4807  max mem: 5776
[03:08:36.790036] ====================
[03:08:36.790144] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:08:36.790164] ====================
[03:08:36.805544] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8116 | reduced: 0.8264]  time: 0.6612  data: 0.3702  max mem: 5776
[03:08:36.885993] Test: [0] Total time: 0:00:02 (0.6816 s / it)
[03:08:36.886113] ***************************************************************
[03:08:36.886135] ****An extra tail dataset should exist for accracy metrics!****
[03:08:36.886152] ***************************************************************
[03:08:36.886170] **** Length of tail: 36 ****
[03:08:37.846926] Actual Batchsize/ world_size {'_n': 3.0}
[03:08:37.847056] {'pixel-level F1': tensor(2.8888, device='cuda:0', dtype=torch.float64)}
[03:08:37.868716] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8683 | reduced: 0.8349]  time: 0.9821  data: 0.6666  max mem: 5776
[03:08:38.780710] Actual Batchsize/ world_size {'_n': 3.0}
[03:08:38.780828] {'pixel-level F1': tensor(2.2115, device='cuda:0', dtype=torch.float64)}
[03:08:38.802387] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8116 | reduced: 0.8292]  time: 0.9577  data: 0.6422  max mem: 5776
[03:08:39.658514] Actual Batchsize/ world_size {'_n': 3.0}
[03:08:39.658627] {'pixel-level F1': tensor(2.3409, device='cuda:0', dtype=torch.float64)}
[03:08:39.680312] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8116 | reduced: 0.8265]  time: 0.9310  data: 0.6156  max mem: 5776
[03:08:39.680443] Test <remaining>: [0] Total time: 0:00:02 (0.9314 s / it)
[03:08:39.680965] ---syncronized---
[03:08:39.681001] pixel-level F1 reduced_count 216
[03:08:39.681032] pixel-level F1 reduced_sum 167.87319802412534
[03:08:39.681064] ---syncronized done ---
[03:08:42.069760] Averaged stats: pixel-level F1: [local: 0.8116 | reduced: 0.7772]
[03:08:42.073001] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:09:08.255239] Test: [0]  [ 0/12]  eta: 0:05:14  pixel-level F1: [local: 0.1854 | reduced: 0.1854]  time: 26.1765  data: 25.8658  max mem: 5776
[03:09:19.281608] ====================
[03:09:19.281743] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:09:19.281771] ====================
[03:09:19.297296] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.1769 | reduced: 0.1848]  time: 3.1015  data: 2.7990  max mem: 5776
[03:09:19.426521] Test: [0] Total time: 0:00:37 (3.1124 s / it)
[03:09:19.426646] ***************************************************************
[03:09:19.426669] ****An extra tail dataset should exist for accracy metrics!****
[03:09:19.426686] ***************************************************************
[03:09:19.426705] **** Length of tail: 36 ****
[03:09:40.490913] Actual Batchsize/ world_size {'_n': 3.0}
[03:09:40.491101] {'pixel-level F1': tensor(0.7149, device='cuda:0', dtype=torch.float64)}
[03:09:40.512170] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.1854 | reduced: 0.1859]  time: 21.0849  data: 20.7683  max mem: 5776
[03:10:02.041790] Actual Batchsize/ world_size {'_n': 3.0}
[03:10:02.041950] {'pixel-level F1': tensor(0.4767, device='cuda:0', dtype=torch.float64)}
[03:10:02.063017] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.1769 | reduced: 0.1854]  time: 21.3176  data: 21.0012  max mem: 5776
[03:10:22.762847] Actual Batchsize/ world_size {'_n': 3.0}
[03:10:22.762977] {'pixel-level F1': tensor(0.9878, device='cuda:0', dtype=torch.float64)}
[03:10:22.784504] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.1854 | reduced: 0.1882]  time: 21.1188  data: 20.8027  max mem: 5776
[03:10:22.784649] Test <remaining>: [0] Total time: 0:01:03 (21.1193 s / it)
[03:10:22.785588] ---syncronized---
[03:10:22.785626] pixel-level F1 reduced_count 600
[03:10:22.785656] pixel-level F1 reduced_sum 135.39872572780573
[03:10:22.785688] ---syncronized done ---
[03:10:25.092708] Averaged stats: pixel-level F1: [local: 0.1854 | reduced: 0.2257]
[03:10:25.096331] 
[ROBUST TEST] RotationWrapper param=8
[03:10:25.097258] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:10:26.374039] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.4408 | reduced: 0.4408]  time: 1.2727  data: 0.9628  max mem: 5776
[03:10:26.633901] ====================
[03:10:26.633976] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:10:26.633998] ====================
[03:10:26.653317] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.4312 | reduced: 0.4362]  time: 0.7758  data: 0.4814  max mem: 5776
[03:10:26.729951] Test: [0] Total time: 0:00:01 (0.8145 s / it)
[03:10:26.730055] ***************************************************************
[03:10:26.730075] ****An extra tail dataset should exist for accracy metrics!****
[03:10:26.730093] ***************************************************************
[03:10:26.730110] **** Length of tail: 43 ****
[03:10:27.282207] Actual Batchsize/ world_size {'_n': 3.0}
[03:10:27.282325] {'pixel-level F1': tensor(1.3651, device='cuda:0', dtype=torch.float64)}
[03:10:27.304028] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4408 | reduced: 0.4384]  time: 0.5735  data: 0.2587  max mem: 5776
[03:10:27.807278] Actual Batchsize/ world_size {'_n': 3.0}
[03:10:27.807401] {'pixel-level F1': tensor(1.0872, device='cuda:0', dtype=torch.float64)}
[03:10:27.829062] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.4312 | reduced: 0.4305]  time: 0.5491  data: 0.2344  max mem: 5776
[03:10:28.360830] Actual Batchsize/ world_size {'_n': 3.0}
[03:10:28.360945] {'pixel-level F1': tensor(0.7848, device='cuda:0', dtype=torch.float64)}
[03:10:28.382665] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.4312 | reduced: 0.4147]  time: 0.5505  data: 0.2357  max mem: 5776
[03:10:28.697059] ====================
[03:10:28.697149] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:10:28.697179] ====================
[03:10:28.697599] Actual Batchsize/ world_size {'_n': 1.75}
[03:10:28.697658] {'pixel-level F1': tensor(0.7463, device='cuda:0', dtype=torch.float64)}
[03:10:28.709205] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.4265 | reduced: 0.4153]  time: 0.4944  data: 0.2098  max mem: 5776
[03:10:28.709324] Test <remaining>: [0] Total time: 0:00:01 (0.4948 s / it)
[03:10:28.709935] ---syncronized---
[03:10:28.709968] pixel-level F1 reduced_count 135
[03:10:28.709999] pixel-level F1 reduced_sum 51.38653719274201
[03:10:28.710030] ---syncronized done ---
[03:10:31.430260] Averaged stats: pixel-level F1: [local: 0.4265 | reduced: 0.3806]
[03:10:31.433905] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:10:32.509044] Test: [0]  [ 0/20]  eta: 0:00:21  pixel-level F1: [local: 0.1011 | reduced: 0.1011]  time: 1.0676  data: 0.7558  max mem: 5776
[03:10:38.116443] ====================
[03:10:38.116553] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:10:38.116576] ====================
[03:10:38.118283] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.3763 | reduced: 0.4486]  time: 0.3338  data: 0.0379  max mem: 5776
[03:10:38.201483] Test: [0] Total time: 0:00:06 (0.3380 s / it)
[03:10:38.201595] ***************************************************************
[03:10:38.201617] ****An extra tail dataset should exist for accracy metrics!****
[03:10:38.201634] ***************************************************************
[03:10:38.201650] **** Length of tail: 8 ****
[03:10:38.510723] ====================
[03:10:38.510805] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:10:38.510825] ====================
[03:10:38.511243] Actual Batchsize/ world_size {'_n': 2.0}
[03:10:38.511307] {'pixel-level F1': tensor(1.8327, device='cuda:0', dtype=torch.float64)}
[03:10:38.524765] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.5723 | reduced: 0.4527]  time: 0.3226  data: 0.1096  max mem: 5776
[03:10:38.524889] Test <remaining>: [0] Total time: 0:00:00 (0.3231 s / it)
[03:10:38.525454] ---syncronized---
[03:10:38.525487] pixel-level F1 reduced_count 928
[03:10:38.525518] pixel-level F1 reduced_sum 431.95607682354773
[03:10:38.525549] ---syncronized done ---
[03:10:39.020287] Averaged stats: pixel-level F1: [local: 0.5723 | reduced: 0.4655]
[03:10:39.021489] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:10:40.825284] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.7830 | reduced: 0.7830]  time: 1.7994  data: 1.4892  max mem: 5776
[03:10:41.664936] ====================
[03:10:41.665042] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:10:41.665061] ====================
[03:10:41.680440] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.7830 | reduced: 0.8255]  time: 0.6635  data: 0.3724  max mem: 5776
[03:10:41.754912] Test: [0] Total time: 0:00:02 (0.6824 s / it)
[03:10:41.755028] ***************************************************************
[03:10:41.755050] ****An extra tail dataset should exist for accracy metrics!****
[03:10:41.755067] ***************************************************************
[03:10:41.755084] **** Length of tail: 36 ****
[03:10:42.719389] Actual Batchsize/ world_size {'_n': 3.0}
[03:10:42.719522] {'pixel-level F1': tensor(2.8982, device='cuda:0', dtype=torch.float64)}
[03:10:42.741201] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8826 | reduced: 0.8343]  time: 0.9856  data: 0.6703  max mem: 5776
[03:10:43.648867] Actual Batchsize/ world_size {'_n': 3.0}
[03:10:43.648990] {'pixel-level F1': tensor(2.2786, device='cuda:0', dtype=torch.float64)}
[03:10:43.670557] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.7830 | reduced: 0.8299]  time: 0.9573  data: 0.6421  max mem: 5776
[03:10:44.526712] Actual Batchsize/ world_size {'_n': 3.0}
[03:10:44.526831] {'pixel-level F1': tensor(2.4610, device='cuda:0', dtype=torch.float64)}
[03:10:44.548606] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8203 | reduced: 0.8294]  time: 0.9308  data: 0.6156  max mem: 5776
[03:10:44.548754] Test <remaining>: [0] Total time: 0:00:02 (0.9312 s / it)
[03:10:44.549244] ---syncronized---
[03:10:44.549278] pixel-level F1 reduced_count 216
[03:10:44.549309] pixel-level F1 reduced_sum 169.0108813260005
[03:10:44.549339] ---syncronized done ---
[03:10:46.946960] Averaged stats: pixel-level F1: [local: 0.8203 | reduced: 0.7825]
[03:10:46.950312] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:11:12.868980] Test: [0]  [ 0/12]  eta: 0:05:10  pixel-level F1: [local: 0.2259 | reduced: 0.2259]  time: 25.9128  data: 25.6021  max mem: 5776
[03:11:24.549014] ====================
[03:11:24.549118] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:11:24.549138] ====================
[03:11:24.564627] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.1532 | reduced: 0.1568]  time: 3.1340  data: 2.8318  max mem: 5776
[03:11:24.720246] Test: [0] Total time: 0:00:37 (3.1470 s / it)
[03:11:24.720371] ***************************************************************
[03:11:24.720392] ****An extra tail dataset should exist for accracy metrics!****
[03:11:24.720408] ***************************************************************
[03:11:24.720426] **** Length of tail: 36 ****
[03:11:45.981247] Actual Batchsize/ world_size {'_n': 3.0}
[03:11:45.981388] {'pixel-level F1': tensor(0.4292, device='cuda:0', dtype=torch.float64)}
[03:11:46.002959] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.1532 | reduced: 0.1565]  time: 21.2820  data: 20.9660  max mem: 5776
[03:12:07.479842] Actual Batchsize/ world_size {'_n': 3.0}
[03:12:07.479967] {'pixel-level F1': tensor(0.4470, device='cuda:0', dtype=torch.float64)}
[03:12:07.501481] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.1490 | reduced: 0.1563]  time: 21.3901  data: 21.0744  max mem: 5776
[03:12:28.165824] Actual Batchsize/ world_size {'_n': 3.0}
[03:12:28.165951] {'pixel-level F1': tensor(0.7307, device='cuda:0', dtype=torch.float64)}
[03:12:28.187647] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.1532 | reduced: 0.1581]  time: 21.1553  data: 20.8398  max mem: 5776
[03:12:28.187799] Test <remaining>: [0] Total time: 0:01:03 (21.1557 s / it)
[03:12:28.188475] ---syncronized---
[03:12:28.188509] pixel-level F1 reduced_count 600
[03:12:28.188545] pixel-level F1 reduced_sum 119.18683801118382
[03:12:28.188582] ---syncronized done ---
[03:12:30.496270] Averaged stats: pixel-level F1: [local: 0.1532 | reduced: 0.1986]
[03:12:30.499874] 
[ROBUST TEST] RotationWrapper param=10
[03:12:30.500673] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:12:31.795391] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.4282 | reduced: 0.4282]  time: 1.2908  data: 0.9808  max mem: 5776
[03:12:32.055210] ====================
[03:12:32.055287] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:12:32.055307] ====================
[03:12:32.074609] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.4282 | reduced: 0.4418]  time: 0.7848  data: 0.4904  max mem: 5776
[03:12:32.154383] Test: [0] Total time: 0:00:01 (0.8251 s / it)
[03:12:32.154488] ***************************************************************
[03:12:32.154509] ****An extra tail dataset should exist for accracy metrics!****
[03:12:32.154535] ***************************************************************
[03:12:32.154552] **** Length of tail: 43 ****
[03:12:32.700795] Actual Batchsize/ world_size {'_n': 3.0}
[03:12:32.700918] {'pixel-level F1': tensor(1.4862, device='cuda:0', dtype=torch.float64)}
[03:12:32.722559] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4567 | reduced: 0.4480]  time: 0.5676  data: 0.2531  max mem: 5776
[03:12:33.226952] Actual Batchsize/ world_size {'_n': 3.0}
[03:12:33.227071] {'pixel-level F1': tensor(1.0229, device='cuda:0', dtype=torch.float64)}
[03:12:33.248709] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.4282 | reduced: 0.4369]  time: 0.5467  data: 0.2320  max mem: 5776
[03:12:33.783568] Actual Batchsize/ world_size {'_n': 3.0}
[03:12:33.783681] {'pixel-level F1': tensor(0.9428, device='cuda:0', dtype=torch.float64)}
[03:12:33.805366] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.4282 | reduced: 0.4254]  time: 0.5499  data: 0.2351  max mem: 5776
[03:12:34.121309] ====================
[03:12:34.121404] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:12:34.121424] ====================
[03:12:34.121847] Actual Batchsize/ world_size {'_n': 1.75}
[03:12:34.121905] {'pixel-level F1': tensor(0.8389, device='cuda:0', dtype=torch.float64)}
[03:12:34.133515] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.4282 | reduced: 0.4282]  time: 0.4944  data: 0.2097  max mem: 5776
[03:12:34.133638] Test <remaining>: [0] Total time: 0:00:01 (0.4947 s / it)
[03:12:34.134204] ---syncronized---
[03:12:34.134236] pixel-level F1 reduced_count 135
[03:12:34.134267] pixel-level F1 reduced_sum 52.78548237300306
[03:12:34.134297] ---syncronized done ---
[03:12:36.840162] Averaged stats: pixel-level F1: [local: 0.4282 | reduced: 0.3910]
[03:12:36.843703] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:12:38.054185] Test: [0]  [ 0/20]  eta: 0:00:24  pixel-level F1: [local: 0.1121 | reduced: 0.1121]  time: 1.2036  data: 0.8923  max mem: 5776
[03:12:43.659326] ====================
[03:12:43.659414] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:12:43.659434] ====================
[03:12:43.661149] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.3631 | reduced: 0.4287]  time: 0.3405  data: 0.0447  max mem: 5776
[03:12:43.746286] Test: [0] Total time: 0:00:06 (0.3448 s / it)
[03:12:43.746390] ***************************************************************
[03:12:43.746411] ****An extra tail dataset should exist for accracy metrics!****
[03:12:43.746427] ***************************************************************
[03:12:43.746445] **** Length of tail: 8 ****
[03:12:44.054900] ====================
[03:12:44.054979] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:12:44.055000] ====================
[03:12:44.055415] Actual Batchsize/ world_size {'_n': 2.0}
[03:12:44.055477] {'pixel-level F1': tensor(1.7243, device='cuda:0', dtype=torch.float64)}
[03:12:44.068847] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.5459 | reduced: 0.4324]  time: 0.3219  data: 0.1092  max mem: 5776
[03:12:44.068984] Test <remaining>: [0] Total time: 0:00:00 (0.3224 s / it)
[03:12:44.069525] ---syncronized---
[03:12:44.069560] pixel-level F1 reduced_count 928
[03:12:44.069590] pixel-level F1 reduced_sum 413.18852269827073
[03:12:44.069621] ---syncronized done ---
[03:12:44.548743] Averaged stats: pixel-level F1: [local: 0.5459 | reduced: 0.4452]
[03:12:44.549923] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:12:46.352790] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8159 | reduced: 0.8159]  time: 1.7985  data: 1.4886  max mem: 5776
[03:12:47.191989] ====================
[03:12:47.192085] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:12:47.192105] ====================
[03:12:47.207539] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8138 | reduced: 0.7948]  time: 0.6632  data: 0.3722  max mem: 5776
[03:12:47.284004] Test: [0] Total time: 0:00:02 (0.6825 s / it)
[03:12:47.284122] ***************************************************************
[03:12:47.284142] ****An extra tail dataset should exist for accracy metrics!****
[03:12:47.284159] ***************************************************************
[03:12:47.284176] **** Length of tail: 36 ****
[03:12:48.242146] Actual Batchsize/ world_size {'_n': 3.0}
[03:12:48.242276] {'pixel-level F1': tensor(2.8242, device='cuda:0', dtype=torch.float64)}
[03:12:48.263913] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8159 | reduced: 0.8039]  time: 0.9793  data: 0.6638  max mem: 5776
[03:12:49.173851] Actual Batchsize/ world_size {'_n': 3.0}
[03:12:49.173965] {'pixel-level F1': tensor(2.4231, device='cuda:0', dtype=torch.float64)}
[03:12:49.195627] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8138 | reduced: 0.8042]  time: 0.9553  data: 0.6399  max mem: 5776
[03:12:50.058378] Actual Batchsize/ world_size {'_n': 3.0}
[03:12:50.058498] {'pixel-level F1': tensor(2.3158, device='cuda:0', dtype=torch.float64)}
[03:12:50.080144] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8138 | reduced: 0.8024]  time: 0.9316  data: 0.6164  max mem: 5776
[03:12:50.080267] Test <remaining>: [0] Total time: 0:00:02 (0.9320 s / it)
[03:12:50.080772] ---syncronized---
[03:12:50.080814] pixel-level F1 reduced_count 216
[03:12:50.080844] pixel-level F1 reduced_sum 165.0541705201905
[03:12:50.080875] ---syncronized done ---
[03:12:52.421698] Averaged stats: pixel-level F1: [local: 0.8138 | reduced: 0.7641]
[03:12:52.425052] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:13:18.487602] Test: [0]  [ 0/12]  eta: 0:05:12  pixel-level F1: [local: 0.1125 | reduced: 0.1125]  time: 26.0568  data: 25.7459  max mem: 5776
[03:13:29.857449] ====================
[03:13:29.857588] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:13:29.857607] ====================
[03:13:29.873065] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.1249 | reduced: 0.1429]  time: 3.1201  data: 2.8178  max mem: 5776
[03:13:30.013721] Test: [0] Total time: 0:00:37 (3.1319 s / it)
[03:13:30.013825] ***************************************************************
[03:13:30.013845] ****An extra tail dataset should exist for accracy metrics!****
[03:13:30.013862] ***************************************************************
[03:13:30.013879] **** Length of tail: 36 ****
[03:13:51.312280] Actual Batchsize/ world_size {'_n': 3.0}
[03:13:51.312424] {'pixel-level F1': tensor(0.5263, device='cuda:0', dtype=torch.float64)}
[03:13:51.333806] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.1414 | reduced: 0.1436]  time: 21.3194  data: 21.0032  max mem: 5776
[03:14:12.149296] Actual Batchsize/ world_size {'_n': 3.0}
[03:14:12.149420] {'pixel-level F1': tensor(0.2154, device='cuda:0', dtype=torch.float64)}
[03:14:12.170893] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.1249 | reduced: 0.1421]  time: 21.0780  data: 20.7623  max mem: 5776
[03:14:32.983585] Actual Batchsize/ world_size {'_n': 3.0}
[03:14:32.983714] {'pixel-level F1': tensor(0.7819, device='cuda:0', dtype=torch.float64)}
[03:14:33.005286] Test <remaining>: [0]  [2/3]  eta: 0:00:20  pixel-level F1: [local: 0.1414 | reduced: 0.1445]  time: 20.9967  data: 20.6814  max mem: 5776
[03:14:33.005436] Test <remaining>: [0] Total time: 0:01:02 (20.9971 s / it)
[03:14:33.006130] ---syncronized---
[03:14:33.006166] pixel-level F1 reduced_count 600
[03:14:33.006197] pixel-level F1 reduced_sum 107.63238583015551
[03:14:33.006229] ---syncronized done ---
[03:14:35.288946] Averaged stats: pixel-level F1: [local: 0.1414 | reduced: 0.1794]
[03:14:35.292552] 
[ROBUST TEST] RotationWrapper param=12
[03:14:35.293441] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:14:36.585183] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.4473 | reduced: 0.4473]  time: 1.2878  data: 0.9778  max mem: 5776
[03:14:36.845118] ====================
[03:14:36.845191] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:14:36.845212] ====================
[03:14:36.864472] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.4242 | reduced: 0.4362]  time: 0.7833  data: 0.4889  max mem: 5776
[03:14:36.943722] Test: [0] Total time: 0:00:01 (0.8234 s / it)
[03:14:36.943832] ***************************************************************
[03:14:36.943853] ****An extra tail dataset should exist for accracy metrics!****
[03:14:36.943870] ***************************************************************
[03:14:36.943887] **** Length of tail: 43 ****
[03:14:37.488741] Actual Batchsize/ world_size {'_n': 3.0}
[03:14:37.488861] {'pixel-level F1': tensor(1.1037, device='cuda:0', dtype=torch.float64)}
[03:14:37.510539] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4242 | reduced: 0.4284]  time: 0.5662  data: 0.2518  max mem: 5776
[03:14:38.013570] Actual Batchsize/ world_size {'_n': 3.0}
[03:14:38.013688] {'pixel-level F1': tensor(0.9963, device='cuda:0', dtype=torch.float64)}
[03:14:38.035325] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.3679 | reduced: 0.4184]  time: 0.5453  data: 0.2307  max mem: 5776
[03:14:38.567137] Actual Batchsize/ world_size {'_n': 3.0}
[03:14:38.567256] {'pixel-level F1': tensor(0.9678, device='cuda:0', dtype=torch.float64)}
[03:14:38.588952] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.3679 | reduced: 0.4094]  time: 0.5480  data: 0.2334  max mem: 5776
[03:14:38.900389] ====================
[03:14:38.900481] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:14:38.900501] ====================
[03:14:38.900929] Actual Batchsize/ world_size {'_n': 1.75}
[03:14:38.900991] {'pixel-level F1': tensor(0.7687, device='cuda:0', dtype=torch.float64)}
[03:14:38.912586] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.3679 | reduced: 0.4110]  time: 0.4918  data: 0.2074  max mem: 5776
[03:14:38.912713] Test <remaining>: [0] Total time: 0:00:01 (0.4922 s / it)
[03:14:38.913262] ---syncronized---
[03:14:38.913292] pixel-level F1 reduced_count 135
[03:14:38.913324] pixel-level F1 reduced_sum 48.11567071329329
[03:14:38.913353] ---syncronized done ---
[03:14:41.595207] Averaged stats: pixel-level F1: [local: 0.3679 | reduced: 0.3564]
[03:14:41.598844] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:14:42.563621] Test: [0]  [ 0/20]  eta: 0:00:19  pixel-level F1: [local: 0.0979 | reduced: 0.0979]  time: 0.9579  data: 0.6454  max mem: 5776
[03:14:48.169621] ====================
[03:14:48.169708] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:14:48.169730] ====================
[03:14:48.171410] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.3448 | reduced: 0.4023]  time: 0.3283  data: 0.0324  max mem: 5776
[03:14:48.256968] Test: [0] Total time: 0:00:06 (0.3326 s / it)
[03:14:48.257067] ***************************************************************
[03:14:48.257088] ****An extra tail dataset should exist for accracy metrics!****
[03:14:48.257111] ***************************************************************
[03:14:48.257128] **** Length of tail: 8 ****
[03:14:48.569858] ====================
[03:14:48.569938] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:14:48.569959] ====================
[03:14:48.570373] Actual Batchsize/ world_size {'_n': 2.0}
[03:14:48.570437] {'pixel-level F1': tensor(1.6553, device='cuda:0', dtype=torch.float64)}
[03:14:48.582619] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.5462 | reduced: 0.4060]  time: 0.3250  data: 0.1118  max mem: 5776
[03:14:48.582734] Test <remaining>: [0] Total time: 0:00:00 (0.3255 s / it)
[03:14:48.583223] ---syncronized---
[03:14:48.583257] pixel-level F1 reduced_count 928
[03:14:48.583286] pixel-level F1 reduced_sum 391.40483504948963
[03:14:48.583317] ---syncronized done ---
[03:14:49.058807] Averaged stats: pixel-level F1: [local: 0.5462 | reduced: 0.4218]
[03:14:49.059960] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:14:50.844183] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.7953 | reduced: 0.7953]  time: 1.7799  data: 1.4696  max mem: 5776
[03:14:51.683624] ====================
[03:14:51.683730] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:14:51.683749] ====================
[03:14:51.699138] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.7953 | reduced: 0.8076]  time: 0.6586  data: 0.3675  max mem: 5776
[03:14:51.775987] Test: [0] Total time: 0:00:02 (0.6780 s / it)
[03:14:51.776087] ***************************************************************
[03:14:51.776107] ****An extra tail dataset should exist for accracy metrics!****
[03:14:51.776124] ***************************************************************
[03:14:51.776140] **** Length of tail: 36 ****
[03:14:52.734961] Actual Batchsize/ world_size {'_n': 3.0}
[03:14:52.735090] {'pixel-level F1': tensor(2.7256, device='cuda:0', dtype=torch.float64)}
[03:14:52.756638] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8553 | reduced: 0.8139]  time: 0.9800  data: 0.6644  max mem: 5776
[03:14:53.662647] Actual Batchsize/ world_size {'_n': 3.0}
[03:14:53.662779] {'pixel-level F1': tensor(2.3784, device='cuda:0', dtype=torch.float64)}
[03:14:53.684270] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.7953 | reduced: 0.8126]  time: 0.9536  data: 0.6382  max mem: 5776
[03:14:54.539433] Actual Batchsize/ world_size {'_n': 3.0}
[03:14:54.539553] {'pixel-level F1': tensor(2.2873, device='cuda:0', dtype=torch.float64)}
[03:14:54.561252] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.7953 | reduced: 0.8098]  time: 0.9280  data: 0.6127  max mem: 5776
[03:14:54.561376] Test <remaining>: [0] Total time: 0:00:02 (0.9284 s / it)
[03:14:54.561842] ---syncronized---
[03:14:54.561881] pixel-level F1 reduced_count 216
[03:14:54.561913] pixel-level F1 reduced_sum 162.36111618542347
[03:14:54.561944] ---syncronized done ---
[03:14:56.852205] Averaged stats: pixel-level F1: [local: 0.7953 | reduced: 0.7517]
[03:14:56.855504] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:15:22.673862] Test: [0]  [ 0/12]  eta: 0:05:09  pixel-level F1: [local: 0.1845 | reduced: 0.1845]  time: 25.8127  data: 25.5022  max mem: 5776
[03:15:33.981624] ====================
[03:15:33.981724] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:15:33.981742] ====================
[03:15:33.997401] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.1535 | reduced: 0.1703]  time: 3.0946  data: 2.7924  max mem: 5776
[03:15:34.122662] Test: [0] Total time: 0:00:37 (3.1052 s / it)
[03:15:34.122782] ***************************************************************
[03:15:34.122803] ****An extra tail dataset should exist for accracy metrics!****
[03:15:34.122819] ***************************************************************
[03:15:34.122838] **** Length of tail: 36 ****
[03:15:55.111598] Actual Batchsize/ world_size {'_n': 3.0}
[03:15:55.111737] {'pixel-level F1': tensor(0.6149, device='cuda:0', dtype=torch.float64)}
[03:15:55.133259] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.1625 | reduced: 0.1710]  time: 21.0099  data: 20.6949  max mem: 5776
[03:16:16.697203] Actual Batchsize/ world_size {'_n': 3.0}
[03:16:16.697333] {'pixel-level F1': tensor(0.4303, device='cuda:0', dtype=torch.float64)}
[03:16:16.718852] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.1535 | reduced: 0.1705]  time: 21.2976  data: 20.9824  max mem: 5776
[03:16:37.103572] Actual Batchsize/ world_size {'_n': 3.0}
[03:16:37.103701] {'pixel-level F1': tensor(0.9032, device='cuda:0', dtype=torch.float64)}
[03:16:37.125311] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.1625 | reduced: 0.1731]  time: 21.0004  data: 20.6852  max mem: 5776
[03:16:37.125441] Test <remaining>: [0] Total time: 0:01:03 (21.0008 s / it)
[03:16:37.126236] ---syncronized---
[03:16:37.126276] pixel-level F1 reduced_count 600
[03:16:37.126307] pixel-level F1 reduced_sum 114.74277590492632
[03:16:37.126339] ---syncronized done ---
[03:16:39.519487] Averaged stats: pixel-level F1: [local: 0.1625 | reduced: 0.1912]
[03:16:39.523286] 
[ROBUST TEST] GaussianNoiseWrapper param=3
[03:16:39.524080] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:16:41.012079] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.7016 | reduced: 0.7016]  time: 1.4840  data: 1.1737  max mem: 5776
[03:16:41.272398] ====================
[03:16:41.272489] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:16:41.272509] ====================
[03:16:41.291679] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5888 | reduced: 0.6477]  time: 0.8816  data: 0.5869  max mem: 5776
[03:16:41.369323] Test: [0] Total time: 0:00:01 (0.9208 s / it)
[03:16:41.369441] ***************************************************************
[03:16:41.369462] ****An extra tail dataset should exist for accracy metrics!****
[03:16:41.369479] ***************************************************************
[03:16:41.369496] **** Length of tail: 43 ****
[03:16:42.057043] Actual Batchsize/ world_size {'_n': 3.0}
[03:16:42.057168] {'pixel-level F1': tensor(2.0374, device='cuda:0', dtype=torch.float64)}
[03:16:42.078867] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6791 | reduced: 0.6513]  time: 0.7089  data: 0.3943  max mem: 5776
[03:16:42.695581] Actual Batchsize/ world_size {'_n': 3.0}
[03:16:42.695699] {'pixel-level F1': tensor(1.5989, device='cuda:0', dtype=torch.float64)}
[03:16:42.717188] Test <remaining>: [0]  [1/4]  eta: 0:00:02  pixel-level F1: [local: 0.5888 | reduced: 0.6390]  time: 0.6735  data: 0.3587  max mem: 5776
[03:16:43.401058] Actual Batchsize/ world_size {'_n': 3.0}
[03:16:43.401176] {'pixel-level F1': tensor(1.1384, device='cuda:0', dtype=torch.float64)}
[03:16:43.422819] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5888 | reduced: 0.6147]  time: 0.6841  data: 0.3692  max mem: 5776
[03:16:43.814414] ====================
[03:16:43.814508] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:16:43.814529] ====================
[03:16:43.814952] Actual Batchsize/ world_size {'_n': 1.75}
[03:16:43.815013] {'pixel-level F1': tensor(0.7610, device='cuda:0', dtype=torch.float64)}
[03:16:43.826631] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5330 | reduced: 0.6054]  time: 0.6139  data: 0.3292  max mem: 5776
[03:16:43.826766] Test <remaining>: [0] Total time: 0:00:02 (0.6143 s / it)
[03:16:43.827415] ---syncronized---
[03:16:43.827449] pixel-level F1 reduced_count 135
[03:16:43.827480] pixel-level F1 reduced_sum 71.47431077569031
[03:16:43.827511] ---syncronized done ---
[03:16:46.615500] Averaged stats: pixel-level F1: [local: 0.5330 | reduced: 0.5294]
[03:16:46.618807] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:16:47.677222] Test: [0]  [ 0/20]  eta: 0:00:21  pixel-level F1: [local: 0.3861 | reduced: 0.3861]  time: 1.0511  data: 0.7230  max mem: 5776
[03:16:53.354916] ====================
[03:16:53.355006] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:16:53.355027] ====================
[03:16:53.356728] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7864 | reduced: 0.7493]  time: 0.3365  data: 0.0398  max mem: 5776
[03:16:53.447226] Test: [0] Total time: 0:00:06 (0.3411 s / it)
[03:16:53.447342] ***************************************************************
[03:16:53.447364] ****An extra tail dataset should exist for accracy metrics!****
[03:16:53.447380] ***************************************************************
[03:16:53.447398] **** Length of tail: 8 ****
[03:16:53.807236] ====================
[03:16:53.807319] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:16:53.807338] ====================
[03:16:53.807765] Actual Batchsize/ world_size {'_n': 2.0}
[03:16:53.807826] {'pixel-level F1': tensor(1.8945, device='cuda:0', dtype=torch.float64)}
[03:16:53.821171] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.8376 | reduced: 0.7510]  time: 0.3733  data: 0.1605  max mem: 5776
[03:16:53.821288] Test <remaining>: [0] Total time: 0:00:00 (0.3737 s / it)
[03:16:53.821828] ---syncronized---
[03:16:53.821858] pixel-level F1 reduced_count 928
[03:16:53.821888] pixel-level F1 reduced_sum 704.1248967921651
[03:16:53.821919] ---syncronized done ---
[03:16:54.328156] Averaged stats: pixel-level F1: [local: 0.8376 | reduced: 0.7588]
[03:16:54.329299] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:16:56.615441] Test: [0]  [0/4]  eta: 0:00:09  pixel-level F1: [local: 0.8215 | reduced: 0.8215]  time: 2.2818  data: 1.9717  max mem: 5776
[03:16:57.454275] ====================
[03:16:57.454383] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:16:57.454403] ====================
[03:16:57.469762] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8215 | reduced: 0.8508]  time: 0.7839  data: 0.4930  max mem: 5776
[03:16:57.550035] Test: [0] Total time: 0:00:03 (0.8042 s / it)
[03:16:57.550156] ***************************************************************
[03:16:57.550177] ****An extra tail dataset should exist for accracy metrics!****
[03:16:57.550194] ***************************************************************
[03:16:57.550211] **** Length of tail: 36 ****
[03:16:59.003483] Actual Batchsize/ world_size {'_n': 3.0}
[03:16:59.003621] {'pixel-level F1': tensor(2.9562, device='cuda:0', dtype=torch.float64)}
[03:16:59.025141] Test <remaining>: [0]  [0/3]  eta: 0:00:04  pixel-level F1: [local: 0.8573 | reduced: 0.8592]  time: 1.4744  data: 1.1591  max mem: 5776
[03:17:00.417194] Actual Batchsize/ world_size {'_n': 3.0}
[03:17:00.417324] {'pixel-level F1': tensor(2.3031, device='cuda:0', dtype=torch.float64)}
[03:17:00.438850] Test <remaining>: [0]  [1/3]  eta: 0:00:02  pixel-level F1: [local: 0.8215 | reduced: 0.8539]  time: 1.4439  data: 1.1284  max mem: 5776
[03:17:01.740793] Actual Batchsize/ world_size {'_n': 3.0}
[03:17:01.740904] {'pixel-level F1': tensor(2.7260, device='cuda:0', dtype=torch.float64)}
[03:17:01.762644] Test <remaining>: [0]  [2/3]  eta: 0:00:01  pixel-level F1: [local: 0.8573 | reduced: 0.8569]  time: 1.4037  data: 1.0884  max mem: 5776
[03:17:01.762769] Test <remaining>: [0] Total time: 0:00:04 (1.4041 s / it)
[03:17:01.763250] ---syncronized---
[03:17:01.763283] pixel-level F1 reduced_count 216
[03:17:01.763315] pixel-level F1 reduced_sum 183.9843326635126
[03:17:01.763348] ---syncronized done ---
[03:17:04.301568] Averaged stats: pixel-level F1: [local: 0.8573 | reduced: 0.8518]
[03:17:04.304598] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:17:37.665723] Test: [0]  [ 0/12]  eta: 0:06:40  pixel-level F1: [local: 0.4203 | reduced: 0.4203]  time: 33.3553  data: 33.0442  max mem: 5776
[03:17:52.342400] ====================
[03:17:52.342526] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:17:52.342547] ====================
[03:17:52.357956] Test: [0]  [11/12]  eta: 0:00:04  pixel-level F1: [local: 0.3346 | reduced: 0.3344]  time: 4.0039  data: 3.7018  max mem: 5776
[03:17:52.484288] Test: [0] Total time: 0:00:48 (4.0145 s / it)
[03:17:52.484419] ***************************************************************
[03:17:52.484440] ****An extra tail dataset should exist for accracy metrics!****
[03:17:52.484457] ***************************************************************
[03:17:52.484476] **** Length of tail: 36 ****
[03:18:23.079914] Actual Batchsize/ world_size {'_n': 3.0}
[03:18:23.080065] {'pixel-level F1': tensor(1.0895, device='cuda:0', dtype=torch.float64)}
[03:18:23.101217] Test <remaining>: [0]  [0/3]  eta: 0:01:31  pixel-level F1: [local: 0.3406 | reduced: 0.3350]  time: 30.6162  data: 30.3002  max mem: 5776
[03:18:54.059882] Actual Batchsize/ world_size {'_n': 3.0}
[03:18:54.060021] {'pixel-level F1': tensor(0.6900, device='cuda:0', dtype=torch.float64)}
[03:18:54.081533] Test <remaining>: [0]  [1/3]  eta: 0:01:01  pixel-level F1: [local: 0.3346 | reduced: 0.3329]  time: 30.7981  data: 30.4825  max mem: 5776
[03:19:23.454627] Actual Batchsize/ world_size {'_n': 3.0}
[03:19:23.454760] {'pixel-level F1': tensor(1.2913, device='cuda:0', dtype=torch.float64)}
[03:19:23.476282] Test <remaining>: [0]  [2/3]  eta: 0:00:30  pixel-level F1: [local: 0.3406 | reduced: 0.3348]  time: 30.3302  data: 30.0148  max mem: 5776
[03:19:23.476422] Test <remaining>: [0] Total time: 0:01:30 (30.3306 s / it)
[03:19:23.477093] ---syncronized---
[03:19:23.477133] pixel-level F1 reduced_count 600
[03:19:23.477163] pixel-level F1 reduced_sum 221.77901036073695
[03:19:23.477194] ---syncronized done ---
[03:19:25.756405] Averaged stats: pixel-level F1: [local: 0.3406 | reduced: 0.3696]
[03:19:25.759746] 
[ROBUST TEST] GaussianNoiseWrapper param=7
[03:19:25.760706] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:19:27.241102] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6703 | reduced: 0.6703]  time: 1.4763  data: 1.1664  max mem: 5776
[03:19:27.501444] ====================
[03:19:27.501539] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:19:27.501558] ====================
[03:19:27.520734] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.6012 | reduced: 0.6373]  time: 0.8777  data: 0.5833  max mem: 5776
[03:19:27.596627] Test: [0] Total time: 0:00:01 (0.9161 s / it)
[03:19:27.596745] ***************************************************************
[03:19:27.596766] ****An extra tail dataset should exist for accracy metrics!****
[03:19:27.596782] ***************************************************************
[03:19:27.596801] **** Length of tail: 43 ****
[03:19:28.278038] Actual Batchsize/ world_size {'_n': 3.0}
[03:19:28.278159] {'pixel-level F1': tensor(2.0261, device='cuda:0', dtype=torch.float64)}
[03:19:28.299853] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6703 | reduced: 0.6417]  time: 0.7026  data: 0.3880  max mem: 5776
[03:19:28.916874] Actual Batchsize/ world_size {'_n': 3.0}
[03:19:28.916991] {'pixel-level F1': tensor(1.5719, device='cuda:0', dtype=torch.float64)}
[03:19:28.938722] Test <remaining>: [0]  [1/4]  eta: 0:00:02  pixel-level F1: [local: 0.6012 | reduced: 0.6295]  time: 0.6706  data: 0.3559  max mem: 5776
[03:19:29.628186] Actual Batchsize/ world_size {'_n': 3.0}
[03:19:29.628304] {'pixel-level F1': tensor(0.9513, device='cuda:0', dtype=torch.float64)}
[03:19:29.649972] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.6012 | reduced: 0.6002]  time: 0.6840  data: 0.3693  max mem: 5776
[03:19:30.045454] ====================
[03:19:30.045547] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:19:30.045567] ====================
[03:19:30.045983] Actual Batchsize/ world_size {'_n': 1.75}
[03:19:30.046040] {'pixel-level F1': tensor(0.7947, device='cuda:0', dtype=torch.float64)}
[03:19:30.057637] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5240 | reduced: 0.5926]  time: 0.6148  data: 0.3302  max mem: 5776
[03:19:30.057783] Test <remaining>: [0] Total time: 0:00:02 (0.6152 s / it)
[03:19:30.058416] ---syncronized---
[03:19:30.058451] pixel-level F1 reduced_count 135
[03:19:30.058482] pixel-level F1 reduced_sum 67.37789179823088
[03:19:30.058513] ---syncronized done ---
[03:19:32.629498] Averaged stats: pixel-level F1: [local: 0.5240 | reduced: 0.4991]
[03:19:32.632828] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:19:33.675247] Test: [0]  [ 0/20]  eta: 0:00:20  pixel-level F1: [local: 0.4084 | reduced: 0.4084]  time: 1.0355  data: 0.7227  max mem: 5776
[03:19:39.279385] ====================
[03:19:39.279472] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:19:39.279492] ====================
[03:19:39.281215] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7874 | reduced: 0.7449]  time: 0.3320  data: 0.0362  max mem: 5776
[03:19:39.365010] Test: [0] Total time: 0:00:06 (0.3363 s / it)
[03:19:39.365124] ***************************************************************
[03:19:39.365146] ****An extra tail dataset should exist for accracy metrics!****
[03:19:39.365163] ***************************************************************
[03:19:39.365180] **** Length of tail: 8 ****
[03:19:39.723881] ====================
[03:19:39.723970] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:19:39.723989] ====================
[03:19:39.724411] Actual Batchsize/ world_size {'_n': 2.0}
[03:19:39.724473] {'pixel-level F1': tensor(1.8933, device='cuda:0', dtype=torch.float64)}
[03:19:39.737871] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.8359 | reduced: 0.7466]  time: 0.3722  data: 0.1593  max mem: 5776
[03:19:39.737996] Test <remaining>: [0] Total time: 0:00:00 (0.3727 s / it)
[03:19:39.738522] ---syncronized---
[03:19:39.738559] pixel-level F1 reduced_count 928
[03:19:39.738589] pixel-level F1 reduced_sum 701.4421157596337
[03:19:39.738620] ---syncronized done ---
[03:19:40.258003] Averaged stats: pixel-level F1: [local: 0.8359 | reduced: 0.7559]
[03:19:40.259144] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:19:42.540103] Test: [0]  [0/4]  eta: 0:00:09  pixel-level F1: [local: 0.8218 | reduced: 0.8218]  time: 2.2766  data: 1.9663  max mem: 5776
[03:19:43.378908] ====================
[03:19:43.379009] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:19:43.379027] ====================
[03:19:43.394411] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8218 | reduced: 0.8520]  time: 0.7826  data: 0.4917  max mem: 5776
[03:19:43.473252] Test: [0] Total time: 0:00:03 (0.8026 s / it)
[03:19:43.473376] ***************************************************************
[03:19:43.473398] ****An extra tail dataset should exist for accracy metrics!****
[03:19:43.473414] ***************************************************************
[03:19:43.473432] **** Length of tail: 36 ****
[03:19:45.368954] Actual Batchsize/ world_size {'_n': 3.0}
[03:19:45.369081] {'pixel-level F1': tensor(2.9553, device='cuda:0', dtype=torch.float64)}
[03:19:45.390763] Test <remaining>: [0]  [0/3]  eta: 0:00:05  pixel-level F1: [local: 0.8627 | reduced: 0.8603]  time: 1.9169  data: 1.6017  max mem: 5776
[03:19:47.192098] Actual Batchsize/ world_size {'_n': 3.0}
[03:19:47.192224] {'pixel-level F1': tensor(2.2782, device='cuda:0', dtype=torch.float64)}
[03:19:47.213853] Test <remaining>: [0]  [1/3]  eta: 0:00:03  pixel-level F1: [local: 0.8218 | reduced: 0.8544]  time: 1.8698  data: 1.5551  max mem: 5776
[03:19:48.905523] Actual Batchsize/ world_size {'_n': 3.0}
[03:19:48.905638] {'pixel-level F1': tensor(2.7260, device='cuda:0', dtype=torch.float64)}
[03:19:48.927299] Test <remaining>: [0]  [2/3]  eta: 0:00:01  pixel-level F1: [local: 0.8627 | reduced: 0.8574]  time: 1.8175  data: 1.5028  max mem: 5776
[03:19:48.927419] Test <remaining>: [0] Total time: 0:00:05 (1.8179 s / it)
[03:19:48.927959] ---syncronized---
[03:19:48.927996] pixel-level F1 reduced_count 216
[03:19:48.928028] pixel-level F1 reduced_sum 184.49562484858288
[03:19:48.928060] ---syncronized done ---
[03:19:51.059810] Averaged stats: pixel-level F1: [local: 0.8627 | reduced: 0.8541]
[03:19:51.063050] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:20:24.243609] Test: [0]  [ 0/12]  eta: 0:06:38  pixel-level F1: [local: 0.3865 | reduced: 0.3865]  time: 33.1745  data: 32.8628  max mem: 5776
[03:20:39.139174] ====================
[03:20:39.139265] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:20:39.139284] ====================
[03:20:39.155044] Test: [0]  [11/12]  eta: 0:00:04  pixel-level F1: [local: 0.3327 | reduced: 0.3317]  time: 4.0071  data: 3.7047  max mem: 5776
[03:20:39.278433] Test: [0] Total time: 0:00:48 (4.0175 s / it)
[03:20:39.278548] ***************************************************************
[03:20:39.278570] ****An extra tail dataset should exist for accracy metrics!****
[03:20:39.278587] ***************************************************************
[03:20:39.278605] **** Length of tail: 36 ****
[03:21:09.795984] Actual Batchsize/ world_size {'_n': 3.0}
[03:21:09.796113] {'pixel-level F1': tensor(0.9139, device='cuda:0', dtype=torch.float64)}
[03:21:09.817744] Test <remaining>: [0]  [0/3]  eta: 0:01:31  pixel-level F1: [local: 0.3327 | reduced: 0.3311]  time: 30.5387  data: 30.2236  max mem: 5776
[03:21:40.381487] Actual Batchsize/ world_size {'_n': 3.0}
[03:21:40.381646] {'pixel-level F1': tensor(0.7197, device='cuda:0', dtype=torch.float64)}
[03:21:40.402730] Test <remaining>: [0]  [1/3]  eta: 0:01:01  pixel-level F1: [local: 0.3326 | reduced: 0.3292]  time: 30.5616  data: 30.2456  max mem: 5776
[03:22:10.180601] Actual Batchsize/ world_size {'_n': 3.0}
[03:22:10.180730] {'pixel-level F1': tensor(1.2959, device='cuda:0', dtype=torch.float64)}
[03:22:10.202123] Test <remaining>: [0]  [2/3]  eta: 0:00:30  pixel-level F1: [local: 0.3327 | reduced: 0.3313]  time: 30.3074  data: 29.9916  max mem: 5776
[03:22:10.202289] Test <remaining>: [0] Total time: 0:01:30 (30.3078 s / it)
[03:22:10.202924] ---syncronized---
[03:22:10.202961] pixel-level F1 reduced_count 600
[03:22:10.202991] pixel-level F1 reduced_sum 221.15003361272667
[03:22:10.203022] ---syncronized done ---
[03:22:12.277957] Averaged stats: pixel-level F1: [local: 0.3327 | reduced: 0.3686]
[03:22:12.281354] 
[ROBUST TEST] GaussianNoiseWrapper param=11
[03:22:12.282361] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:22:13.742675] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6493 | reduced: 0.6493]  time: 1.4560  data: 1.1458  max mem: 5776
[03:22:14.003235] ====================
[03:22:14.003332] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:22:14.003352] ====================
[03:22:14.022316] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5994 | reduced: 0.6254]  time: 0.8676  data: 0.5729  max mem: 5776
[03:22:14.096240] Test: [0] Total time: 0:00:01 (0.9050 s / it)
[03:22:14.096360] ***************************************************************
[03:22:14.096382] ****An extra tail dataset should exist for accracy metrics!****
[03:22:14.096399] ***************************************************************
[03:22:14.096417] **** Length of tail: 43 ****
[03:22:14.778975] Actual Batchsize/ world_size {'_n': 3.0}
[03:22:14.779094] {'pixel-level F1': tensor(1.8268, device='cuda:0', dtype=torch.float64)}
[03:22:14.800869] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6089 | reduced: 0.6235]  time: 0.7040  data: 0.3897  max mem: 5776
[03:22:15.416356] Actual Batchsize/ world_size {'_n': 3.0}
[03:22:15.416471] {'pixel-level F1': tensor(1.6464, device='cuda:0', dtype=torch.float64)}
[03:22:15.438108] Test <remaining>: [0]  [1/4]  eta: 0:00:02  pixel-level F1: [local: 0.5994 | reduced: 0.6158]  time: 0.6704  data: 0.3560  max mem: 5776
[03:22:16.120612] Actual Batchsize/ world_size {'_n': 3.0}
[03:22:16.120727] {'pixel-level F1': tensor(0.9082, device='cuda:0', dtype=torch.float64)}
[03:22:16.142392] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5994 | reduced: 0.5864]  time: 0.6816  data: 0.3672  max mem: 5776
[03:22:16.545382] ====================
[03:22:16.545472] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:22:16.545492] ====================
[03:22:16.545910] Actual Batchsize/ world_size {'_n': 1.75}
[03:22:16.545970] {'pixel-level F1': tensor(0.7554, device='cuda:0', dtype=torch.float64)}
[03:22:16.557570] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5488 | reduced: 0.5784]  time: 0.6149  data: 0.3306  max mem: 5776
[03:22:16.557698] Test <remaining>: [0] Total time: 0:00:02 (0.6153 s / it)
[03:22:16.558366] ---syncronized---
[03:22:16.558400] pixel-level F1 reduced_count 135
[03:22:16.558430] pixel-level F1 reduced_sum 67.19420850460665
[03:22:16.558460] ---syncronized done ---
[03:22:19.007447] Averaged stats: pixel-level F1: [local: 0.5488 | reduced: 0.4977]
[03:22:19.010784] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:22:20.253268] Test: [0]  [ 0/20]  eta: 0:00:24  pixel-level F1: [local: 0.3951 | reduced: 0.3951]  time: 1.2354  data: 0.9238  max mem: 5776
[03:22:25.859094] ====================
[03:22:25.859183] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:22:25.859203] ====================
[03:22:25.860913] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7691 | reduced: 0.7408]  time: 0.3421  data: 0.0463  max mem: 5776
[03:22:25.945559] Test: [0] Total time: 0:00:06 (0.3464 s / it)
[03:22:25.945668] ***************************************************************
[03:22:25.945688] ****An extra tail dataset should exist for accracy metrics!****
[03:22:25.945704] ***************************************************************
[03:22:25.945721] **** Length of tail: 8 ****
[03:22:26.306732] ====================
[03:22:26.306815] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:22:26.306842] ====================
[03:22:26.307262] Actual Batchsize/ world_size {'_n': 2.0}
[03:22:26.307325] {'pixel-level F1': tensor(1.9026, device='cuda:0', dtype=torch.float64)}
[03:22:26.320636] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7894 | reduced: 0.7426]  time: 0.3744  data: 0.1617  max mem: 5776
[03:22:26.320754] Test <remaining>: [0] Total time: 0:00:00 (0.3749 s / it)
[03:22:26.321374] ---syncronized---
[03:22:26.321410] pixel-level F1 reduced_count 928
[03:22:26.321440] pixel-level F1 reduced_sum 698.9284811537436
[03:22:26.321471] ---syncronized done ---
[03:22:26.837780] Averaged stats: pixel-level F1: [local: 0.7894 | reduced: 0.7532]
[03:22:26.838934] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:22:29.105510] Test: [0]  [0/4]  eta: 0:00:09  pixel-level F1: [local: 0.8220 | reduced: 0.8220]  time: 2.2622  data: 1.9522  max mem: 5776
[03:22:29.944707] ====================
[03:22:29.944819] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:22:29.944838] ====================
[03:22:29.960120] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8220 | reduced: 0.8478]  time: 0.7791  data: 0.4881  max mem: 5776
[03:22:30.046549] Test: [0] Total time: 0:00:03 (0.8009 s / it)
[03:22:30.046671] ***************************************************************
[03:22:30.046691] ****An extra tail dataset should exist for accracy metrics!****
[03:22:30.046708] ***************************************************************
[03:22:30.046726] **** Length of tail: 36 ****
[03:22:31.487258] Actual Batchsize/ world_size {'_n': 3.0}
[03:22:31.487387] {'pixel-level F1': tensor(2.9557, device='cuda:0', dtype=torch.float64)}
[03:22:31.508918] Test <remaining>: [0]  [0/3]  eta: 0:00:04  pixel-level F1: [local: 0.8410 | reduced: 0.8564]  time: 1.4617  data: 1.1463  max mem: 5776
[03:22:32.887066] Actual Batchsize/ world_size {'_n': 3.0}
[03:22:32.887195] {'pixel-level F1': tensor(2.2464, device='cuda:0', dtype=torch.float64)}
[03:22:32.908633] Test <remaining>: [0]  [1/3]  eta: 0:00:02  pixel-level F1: [local: 0.8220 | reduced: 0.8500]  time: 1.4305  data: 1.1152  max mem: 5776
[03:22:34.214572] Actual Batchsize/ world_size {'_n': 3.0}
[03:22:34.214694] {'pixel-level F1': tensor(2.7253, device='cuda:0', dtype=torch.float64)}
[03:22:34.236374] Test <remaining>: [0]  [2/3]  eta: 0:00:01  pixel-level F1: [local: 0.8410 | reduced: 0.8533]  time: 1.3961  data: 1.0811  max mem: 5776
[03:22:34.236496] Test <remaining>: [0] Total time: 0:00:04 (1.3965 s / it)
[03:22:34.236983] ---syncronized---
[03:22:34.237022] pixel-level F1 reduced_count 216
[03:22:34.237054] pixel-level F1 reduced_sum 182.6819339924697
[03:22:34.237084] ---syncronized done ---
[03:22:36.189985] Averaged stats: pixel-level F1: [local: 0.8410 | reduced: 0.8457]
[03:22:36.192951] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:23:09.650024] Test: [0]  [ 0/12]  eta: 0:06:41  pixel-level F1: [local: 0.3827 | reduced: 0.3827]  time: 33.4512  data: 33.1402  max mem: 5776
[03:23:23.996622] ====================
[03:23:23.996767] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:23:23.996787] ====================
[03:23:24.012196] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.3105 | reduced: 0.3145]  time: 3.9844  data: 3.6820  max mem: 5776
[03:23:24.145632] Test: [0] Total time: 0:00:47 (3.9956 s / it)
[03:23:24.145741] ***************************************************************
[03:23:24.145762] ****An extra tail dataset should exist for accracy metrics!****
[03:23:24.145778] ***************************************************************
[03:23:24.145799] **** Length of tail: 36 ****
[03:23:54.958498] Actual Batchsize/ world_size {'_n': 3.0}
[03:23:54.958647] {'pixel-level F1': tensor(1.0305, device='cuda:0', dtype=torch.float64)}
[03:23:54.979798] Test <remaining>: [0]  [0/3]  eta: 0:01:32  pixel-level F1: [local: 0.3357 | reduced: 0.3151]  time: 30.8334  data: 30.5156  max mem: 5776
[03:24:26.021709] Actual Batchsize/ world_size {'_n': 3.0}
[03:24:26.021833] {'pixel-level F1': tensor(0.7221, device='cuda:0', dtype=torch.float64)}
[03:24:26.043257] Test <remaining>: [0]  [1/3]  eta: 0:01:01  pixel-level F1: [local: 0.3105 | reduced: 0.3136]  time: 30.9482  data: 30.6314  max mem: 5776
[03:24:55.849739] Actual Batchsize/ world_size {'_n': 3.0}
[03:24:55.849867] {'pixel-level F1': tensor(1.2665, device='cuda:0', dtype=torch.float64)}
[03:24:55.871453] Test <remaining>: [0]  [2/3]  eta: 0:00:30  pixel-level F1: [local: 0.3357 | reduced: 0.3158]  time: 30.5748  data: 30.2584  max mem: 5776
[03:24:55.871621] Test <remaining>: [0] Total time: 0:01:31 (30.5752 s / it)
[03:24:55.872305] ---syncronized---
[03:24:55.872345] pixel-level F1 reduced_count 600
[03:24:55.872377] pixel-level F1 reduced_sum 218.75329461582953
[03:24:55.872410] ---syncronized done ---
[03:24:57.880130] Averaged stats: pixel-level F1: [local: 0.3357 | reduced: 0.3646]
[03:24:57.883545] 
[ROBUST TEST] GaussianNoiseWrapper param=15
[03:24:57.884588] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:24:59.274102] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6661 | reduced: 0.6661]  time: 1.3854  data: 1.0751  max mem: 5776
[03:24:59.534048] ====================
[03:24:59.534131] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:24:59.534152] ====================
[03:24:59.553466] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5902 | reduced: 0.6298]  time: 0.8321  data: 0.5376  max mem: 5776
[03:24:59.627466] Test: [0] Total time: 0:00:01 (0.8696 s / it)
[03:24:59.627586] ***************************************************************
[03:24:59.627608] ****An extra tail dataset should exist for accracy metrics!****
[03:24:59.627625] ***************************************************************
[03:24:59.627642] **** Length of tail: 43 ****
[03:25:00.306229] Actual Batchsize/ world_size {'_n': 3.0}
[03:25:00.306347] {'pixel-level F1': tensor(2.0455, device='cuda:0', dtype=torch.float64)}
[03:25:00.328004] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6661 | reduced: 0.6358]  time: 0.6999  data: 0.3848  max mem: 5776
[03:25:00.944510] Actual Batchsize/ world_size {'_n': 3.0}
[03:25:00.944635] {'pixel-level F1': tensor(1.4638, device='cuda:0', dtype=torch.float64)}
[03:25:00.966244] Test <remaining>: [0]  [1/4]  eta: 0:00:02  pixel-level F1: [local: 0.5902 | reduced: 0.6205]  time: 0.6689  data: 0.3538  max mem: 5776
[03:25:01.652116] Actual Batchsize/ world_size {'_n': 3.0}
[03:25:01.652235] {'pixel-level F1': tensor(0.8176, device='cuda:0', dtype=torch.float64)}
[03:25:01.673815] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5902 | reduced: 0.5879]  time: 0.6817  data: 0.3667  max mem: 5776
[03:25:02.066981] ====================
[03:25:02.067068] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:25:02.067088] ====================
[03:25:02.067521] Actual Batchsize/ world_size {'_n': 1.75}
[03:25:02.067582] {'pixel-level F1': tensor(0.8295, device='cuda:0', dtype=torch.float64)}
[03:25:02.079126] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.4879 | reduced: 0.5820]  time: 0.6125  data: 0.3277  max mem: 5776
[03:25:02.079253] Test <remaining>: [0] Total time: 0:00:02 (0.6129 s / it)
[03:25:02.079891] ---syncronized---
[03:25:02.079926] pixel-level F1 reduced_count 135
[03:25:02.079957] pixel-level F1 reduced_sum 66.68452644358723
[03:25:02.079988] ---syncronized done ---
[03:25:04.405593] Averaged stats: pixel-level F1: [local: 0.4879 | reduced: 0.4940]
[03:25:04.408925] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:25:05.595975] Test: [0]  [ 0/20]  eta: 0:00:23  pixel-level F1: [local: 0.4159 | reduced: 0.4159]  time: 1.1800  data: 0.8681  max mem: 5776
[03:25:11.203303] ====================
[03:25:11.203429] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:25:11.203449] ====================
[03:25:11.205120] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7744 | reduced: 0.7381]  time: 0.3394  data: 0.0435  max mem: 5776
[03:25:11.288859] Test: [0] Total time: 0:00:06 (0.3437 s / it)
[03:25:11.288957] ***************************************************************
[03:25:11.288978] ****An extra tail dataset should exist for accracy metrics!****
[03:25:11.288995] ***************************************************************
[03:25:11.289013] **** Length of tail: 8 ****
[03:25:11.643459] ====================
[03:25:11.643541] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:25:11.643560] ====================
[03:25:11.643978] Actual Batchsize/ world_size {'_n': 2.0}
[03:25:11.644038] {'pixel-level F1': tensor(1.9052, device='cuda:0', dtype=torch.float64)}
[03:25:11.657343] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7966 | reduced: 0.7400]  time: 0.3679  data: 0.1550  max mem: 5776
[03:25:11.657464] Test <remaining>: [0] Total time: 0:00:00 (0.3683 s / it)
[03:25:11.658091] ---syncronized---
[03:25:11.658123] pixel-level F1 reduced_count 928
[03:25:11.658154] pixel-level F1 reduced_sum 700.4358918546567
[03:25:11.658185] ---syncronized done ---
[03:25:12.161745] Averaged stats: pixel-level F1: [local: 0.7966 | reduced: 0.7548]
[03:25:12.162897] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:25:14.440832] Test: [0]  [0/4]  eta: 0:00:09  pixel-level F1: [local: 0.8216 | reduced: 0.8216]  time: 2.2735  data: 1.9630  max mem: 5776
[03:25:15.280673] ====================
[03:25:15.280776] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:25:15.280795] ====================
[03:25:15.296107] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8216 | reduced: 0.8468]  time: 0.7821  data: 0.4908  max mem: 5776
[03:25:15.374458] Test: [0] Total time: 0:00:03 (0.8019 s / it)
[03:25:15.374584] ***************************************************************
[03:25:15.374605] ****An extra tail dataset should exist for accracy metrics!****
[03:25:15.374621] ***************************************************************
[03:25:15.374639] **** Length of tail: 36 ****
[03:25:16.828372] Actual Batchsize/ world_size {'_n': 3.0}
[03:25:16.828510] {'pixel-level F1': tensor(2.9561, device='cuda:0', dtype=torch.float64)}
[03:25:16.849958] Test <remaining>: [0]  [0/3]  eta: 0:00:04  pixel-level F1: [local: 0.8386 | reduced: 0.8555]  time: 1.4748  data: 1.1592  max mem: 5776
[03:25:18.244127] Actual Batchsize/ world_size {'_n': 3.0}
[03:25:18.244270] {'pixel-level F1': tensor(2.1965, device='cuda:0', dtype=torch.float64)}
[03:25:18.265494] Test <remaining>: [0]  [1/3]  eta: 0:00:02  pixel-level F1: [local: 0.8216 | reduced: 0.8482]  time: 1.4450  data: 1.1289  max mem: 5776
[03:25:19.584412] Actual Batchsize/ world_size {'_n': 3.0}
[03:25:19.584536] {'pixel-level F1': tensor(2.7250, device='cuda:0', dtype=torch.float64)}
[03:25:19.606158] Test <remaining>: [0]  [2/3]  eta: 0:00:01  pixel-level F1: [local: 0.8386 | reduced: 0.8516]  time: 1.4100  data: 1.0942  max mem: 5776
[03:25:19.606292] Test <remaining>: [0] Total time: 0:00:04 (1.4105 s / it)
[03:25:19.606858] ---syncronized---
[03:25:19.606894] pixel-level F1 reduced_count 216
[03:25:19.606924] pixel-level F1 reduced_sum 182.03434603080518
[03:25:19.606960] ---syncronized done ---
[03:25:21.469593] Averaged stats: pixel-level F1: [local: 0.8386 | reduced: 0.8428]
[03:25:21.472687] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:25:54.815221] Test: [0]  [ 0/12]  eta: 0:06:40  pixel-level F1: [local: 0.3822 | reduced: 0.3822]  time: 33.3366  data: 33.0252  max mem: 5776
[03:26:09.405849] ====================
[03:26:09.405945] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:26:09.405964] ====================
[03:26:09.421524] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2804 | reduced: 0.3262]  time: 3.9952  data: 3.6928  max mem: 5776
[03:26:09.544873] Test: [0] Total time: 0:00:48 (4.0056 s / it)
[03:26:09.544974] ***************************************************************
[03:26:09.544996] ****An extra tail dataset should exist for accracy metrics!****
[03:26:09.545013] ***************************************************************
[03:26:09.545032] **** Length of tail: 36 ****
[03:26:40.490404] Actual Batchsize/ world_size {'_n': 3.0}
[03:26:40.490552] {'pixel-level F1': tensor(1.0708, device='cuda:0', dtype=torch.float64)}
[03:26:40.511885] Test <remaining>: [0]  [0/3]  eta: 0:01:32  pixel-level F1: [local: 0.3278 | reduced: 0.3269]  time: 30.9664  data: 30.6508  max mem: 5776
[03:27:11.794024] Actual Batchsize/ world_size {'_n': 3.0}
[03:27:11.794153] {'pixel-level F1': tensor(0.7319, device='cuda:0', dtype=torch.float64)}
[03:27:11.815665] Test <remaining>: [0]  [1/3]  eta: 0:01:02  pixel-level F1: [local: 0.2804 | reduced: 0.3252]  time: 31.1349  data: 30.8191  max mem: 5776
[03:27:41.798458] Actual Batchsize/ world_size {'_n': 3.0}
[03:27:41.798616] {'pixel-level F1': tensor(1.2296, device='cuda:0', dtype=torch.float64)}
[03:27:41.819835] Test <remaining>: [0]  [2/3]  eta: 0:00:30  pixel-level F1: [local: 0.3278 | reduced: 0.3269]  time: 30.7578  data: 30.4418  max mem: 5776
[03:27:41.819993] Test <remaining>: [0] Total time: 0:01:32 (30.7583 s / it)
[03:27:41.820667] ---syncronized---
[03:27:41.820709] pixel-level F1 reduced_count 600
[03:27:41.820740] pixel-level F1 reduced_sum 219.83818741273015
[03:27:41.820771] ---syncronized done ---
[03:27:43.741529] Averaged stats: pixel-level F1: [local: 0.3278 | reduced: 0.3664]
[03:27:43.744889] 
[ROBUST TEST] GaussianNoiseWrapper param=19
[03:27:43.745918] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:27:45.140427] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6183 | reduced: 0.6183]  time: 1.3903  data: 1.0801  max mem: 5776
[03:27:45.400870] ====================
[03:27:45.400949] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:27:45.400969] ====================
[03:27:45.420093] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5505 | reduced: 0.5859]  time: 0.8347  data: 0.5401  max mem: 5776
[03:27:45.492255] Test: [0] Total time: 0:00:01 (0.8713 s / it)
[03:27:45.492371] ***************************************************************
[03:27:45.492392] ****An extra tail dataset should exist for accracy metrics!****
[03:27:45.492409] ***************************************************************
[03:27:45.492426] **** Length of tail: 43 ****
[03:27:46.175361] Actual Batchsize/ world_size {'_n': 3.0}
[03:27:46.175480] {'pixel-level F1': tensor(2.0399, device='cuda:0', dtype=torch.float64)}
[03:27:46.197094] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6183 | reduced: 0.5967]  time: 0.7042  data: 0.3889  max mem: 5776
[03:27:46.813758] Actual Batchsize/ world_size {'_n': 3.0}
[03:27:46.813885] {'pixel-level F1': tensor(1.6197, device='cuda:0', dtype=torch.float64)}
[03:27:46.835496] Test <remaining>: [0]  [1/4]  eta: 0:00:02  pixel-level F1: [local: 0.5505 | reduced: 0.5909]  time: 0.6711  data: 0.3560  max mem: 5776
[03:27:47.523427] Actual Batchsize/ world_size {'_n': 3.0}
[03:27:47.523549] {'pixel-level F1': tensor(0.8404, device='cuda:0', dtype=torch.float64)}
[03:27:47.545201] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5505 | reduced: 0.5617]  time: 0.6839  data: 0.3685  max mem: 5776
[03:27:47.937312] ====================
[03:27:47.937397] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:27:47.937417] ====================
[03:27:47.937835] Actual Batchsize/ world_size {'_n': 1.75}
[03:27:47.937894] {'pixel-level F1': tensor(0.7490, device='cuda:0', dtype=torch.float64)}
[03:27:47.949446] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5399 | reduced: 0.5548]  time: 0.6139  data: 0.3288  max mem: 5776
[03:27:47.949571] Test <remaining>: [0] Total time: 0:00:02 (0.6143 s / it)
[03:27:47.950110] ---syncronized---
[03:27:47.950149] pixel-level F1 reduced_count 135
[03:27:47.950180] pixel-level F1 reduced_sum 64.03617974974418
[03:27:47.950211] ---syncronized done ---
[03:27:50.199029] Averaged stats: pixel-level F1: [local: 0.5399 | reduced: 0.4743]
[03:27:50.202343] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:27:51.332618] Test: [0]  [ 0/20]  eta: 0:00:22  pixel-level F1: [local: 0.3622 | reduced: 0.3622]  time: 1.1233  data: 0.8089  max mem: 5776
[03:27:56.943430] ====================
[03:27:56.943526] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:27:56.943546] ====================
[03:27:56.945243] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7600 | reduced: 0.7304]  time: 0.3368  data: 0.0405  max mem: 5776
[03:27:57.032532] Test: [0] Total time: 0:00:06 (0.3412 s / it)
[03:27:57.032632] ***************************************************************
[03:27:57.032655] ****An extra tail dataset should exist for accracy metrics!****
[03:27:57.032672] ***************************************************************
[03:27:57.032689] **** Length of tail: 8 ****
[03:27:57.388557] ====================
[03:27:57.388639] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:27:57.388658] ====================
[03:27:57.389070] Actual Batchsize/ world_size {'_n': 2.0}
[03:27:57.389129] {'pixel-level F1': tensor(1.9085, device='cuda:0', dtype=torch.float64)}
[03:27:57.402480] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7919 | reduced: 0.7324]  time: 0.3693  data: 0.1566  max mem: 5776
[03:27:57.402599] Test <remaining>: [0] Total time: 0:00:00 (0.3698 s / it)
[03:27:57.403238] ---syncronized---
[03:27:57.403272] pixel-level F1 reduced_count 928
[03:27:57.403303] pixel-level F1 reduced_sum 694.570791087415
[03:27:57.403335] ---syncronized done ---
[03:27:57.898547] Averaged stats: pixel-level F1: [local: 0.7919 | reduced: 0.7485]
[03:27:57.899733] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:28:00.173674] Test: [0]  [0/4]  eta: 0:00:09  pixel-level F1: [local: 0.8215 | reduced: 0.8215]  time: 2.2696  data: 1.9595  max mem: 5776
[03:28:01.012980] ====================
[03:28:01.013082] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:28:01.013102] ====================
[03:28:01.028511] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8215 | reduced: 0.8461]  time: 0.7810  data: 0.4899  max mem: 5776
[03:28:01.106915] Test: [0] Total time: 0:00:03 (0.8008 s / it)
[03:28:01.107019] ***************************************************************
[03:28:01.107041] ****An extra tail dataset should exist for accracy metrics!****
[03:28:01.107063] ***************************************************************
[03:28:01.107081] **** Length of tail: 36 ****
[03:28:02.559217] Actual Batchsize/ world_size {'_n': 3.0}
[03:28:02.559357] {'pixel-level F1': tensor(2.9549, device='cuda:0', dtype=torch.float64)}
[03:28:02.580808] Test <remaining>: [0]  [0/3]  eta: 0:00:04  pixel-level F1: [local: 0.8338 | reduced: 0.8548]  time: 1.4732  data: 1.1577  max mem: 5776
[03:28:03.966755] Actual Batchsize/ world_size {'_n': 3.0}
[03:28:03.966884] {'pixel-level F1': tensor(2.1945, device='cuda:0', dtype=torch.float64)}
[03:28:03.988332] Test <remaining>: [0]  [1/3]  eta: 0:00:02  pixel-level F1: [local: 0.8215 | reduced: 0.8475]  time: 1.4402  data: 1.1249  max mem: 5776
[03:28:05.303384] Actual Batchsize/ world_size {'_n': 3.0}
[03:28:05.303502] {'pixel-level F1': tensor(2.7270, device='cuda:0', dtype=torch.float64)}
[03:28:05.325132] Test <remaining>: [0]  [2/3]  eta: 0:00:01  pixel-level F1: [local: 0.8338 | reduced: 0.8509]  time: 1.4056  data: 1.0905  max mem: 5776
[03:28:05.325279] Test <remaining>: [0] Total time: 0:00:04 (1.4060 s / it)
[03:28:05.325774] ---syncronized---
[03:28:05.325810] pixel-level F1 reduced_count 216
[03:28:05.325842] pixel-level F1 reduced_sum 181.98470821669187
[03:28:05.325875] ---syncronized done ---
[03:28:07.108140] Averaged stats: pixel-level F1: [local: 0.8338 | reduced: 0.8425]
[03:28:07.111262] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:28:40.309572] Test: [0]  [ 0/12]  eta: 0:06:38  pixel-level F1: [local: 0.3833 | reduced: 0.3833]  time: 33.1923  data: 32.8801  max mem: 5776
[03:28:55.075119] ====================
[03:28:55.075221] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:28:55.075241] ====================
[03:28:55.090725] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.3089 | reduced: 0.3250]  time: 3.9977  data: 3.6951  max mem: 5776
[03:28:55.231255] Test: [0] Total time: 0:00:48 (4.0095 s / it)
[03:28:55.231368] ***************************************************************
[03:28:55.231389] ****An extra tail dataset should exist for accracy metrics!****
[03:28:55.231406] ***************************************************************
[03:28:55.231424] **** Length of tail: 36 ****
[03:29:26.262897] Actual Batchsize/ world_size {'_n': 3.0}
[03:29:26.263038] {'pixel-level F1': tensor(1.0087, device='cuda:0', dtype=torch.float64)}
[03:29:26.284633] Test <remaining>: [0]  [0/3]  eta: 0:01:33  pixel-level F1: [local: 0.3310 | reduced: 0.3253]  time: 31.0527  data: 30.7369  max mem: 5776
[03:29:57.432921] Actual Batchsize/ world_size {'_n': 3.0}
[03:29:57.433084] {'pixel-level F1': tensor(0.7805, device='cuda:0', dtype=torch.float64)}
[03:29:57.454285] Test <remaining>: [0]  [1/3]  eta: 0:01:02  pixel-level F1: [local: 0.3089 | reduced: 0.3239]  time: 31.1109  data: 30.7946  max mem: 5776
[03:30:27.127057] Actual Batchsize/ world_size {'_n': 3.0}
[03:30:27.127187] {'pixel-level F1': tensor(1.3029, device='cuda:0', dtype=torch.float64)}
[03:30:27.148588] Test <remaining>: [0]  [2/3]  eta: 0:00:30  pixel-level F1: [local: 0.3310 | reduced: 0.3261]  time: 30.6386  data: 30.3222  max mem: 5776
[03:30:27.148734] Test <remaining>: [0] Total time: 0:01:31 (30.6391 s / it)
[03:30:27.149433] ---syncronized---
[03:30:27.149467] pixel-level F1 reduced_count 600
[03:30:27.149498] pixel-level F1 reduced_sum 217.9113857719277
[03:30:27.149529] ---syncronized done ---
[03:30:28.995742] Averaged stats: pixel-level F1: [local: 0.3310 | reduced: 0.3632]
[03:30:28.999132] 
[ROBUST TEST] GaussianNoiseWrapper param=23
[03:30:29.000307] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:30:30.425800] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6387 | reduced: 0.6387]  time: 1.4213  data: 1.1105  max mem: 5776
[03:30:30.686417] ====================
[03:30:30.686527] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:30:30.686547] ====================
[03:30:30.705602] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5522 | reduced: 0.5973]  time: 0.8503  data: 0.5553  max mem: 5776
[03:30:30.782718] Test: [0] Total time: 0:00:01 (0.8894 s / it)
[03:30:30.782817] ***************************************************************
[03:30:30.782839] ****An extra tail dataset should exist for accracy metrics!****
[03:30:30.782855] ***************************************************************
[03:30:30.782872] **** Length of tail: 43 ****
[03:30:31.462098] Actual Batchsize/ world_size {'_n': 3.0}
[03:30:31.462219] {'pixel-level F1': tensor(2.0401, device='cuda:0', dtype=torch.float64)}
[03:30:31.483991] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6387 | reduced: 0.6069]  time: 0.7007  data: 0.3859  max mem: 5776
[03:30:32.102092] Actual Batchsize/ world_size {'_n': 3.0}
[03:30:32.102205] {'pixel-level F1': tensor(1.5157, device='cuda:0', dtype=torch.float64)}
[03:30:32.123907] Test <remaining>: [0]  [1/4]  eta: 0:00:02  pixel-level F1: [local: 0.5522 | reduced: 0.5964]  time: 0.6701  data: 0.3553  max mem: 5776
[03:30:32.808615] Actual Batchsize/ world_size {'_n': 3.0}
[03:30:32.808731] {'pixel-level F1': tensor(0.7715, device='cuda:0', dtype=torch.float64)}
[03:30:32.830464] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5522 | reduced: 0.5646]  time: 0.6821  data: 0.3673  max mem: 5776
[03:30:33.223467] ====================
[03:30:33.223561] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:30:33.223581] ====================
[03:30:33.224016] Actual Batchsize/ world_size {'_n': 1.75}
[03:30:33.224074] {'pixel-level F1': tensor(0.7261, device='cuda:0', dtype=torch.float64)}
[03:30:33.235640] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5052 | reduced: 0.5568]  time: 0.6128  data: 0.3281  max mem: 5776
[03:30:33.235760] Test <remaining>: [0] Total time: 0:00:02 (0.6132 s / it)
[03:30:33.236307] ---syncronized---
[03:30:33.236342] pixel-level F1 reduced_count 135
[03:30:33.236374] pixel-level F1 reduced_sum 63.91014085056135
[03:30:33.236406] ---syncronized done ---
[03:30:35.477299] Averaged stats: pixel-level F1: [local: 0.5052 | reduced: 0.4734]
[03:30:35.480633] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:30:36.850870] Test: [0]  [ 0/20]  eta: 0:00:27  pixel-level F1: [local: 0.4410 | reduced: 0.4410]  time: 1.3631  data: 1.0510  max mem: 5776
[03:30:42.455600] ====================
[03:30:42.455693] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:30:42.455713] ====================
[03:30:42.457415] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7590 | reduced: 0.7397]  time: 0.3485  data: 0.0526  max mem: 5776
[03:30:42.543169] Test: [0] Total time: 0:00:07 (0.3528 s / it)
[03:30:42.543262] ***************************************************************
[03:30:42.543283] ****An extra tail dataset should exist for accracy metrics!****
[03:30:42.543300] ***************************************************************
[03:30:42.543319] **** Length of tail: 8 ****
[03:30:42.897954] ====================
[03:30:42.898035] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:30:42.898055] ====================
[03:30:42.898469] Actual Batchsize/ world_size {'_n': 2.0}
[03:30:42.898530] {'pixel-level F1': tensor(1.9084, device='cuda:0', dtype=torch.float64)}
[03:30:42.911757] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7621 | reduced: 0.7415]  time: 0.3680  data: 0.1553  max mem: 5776
[03:30:42.911896] Test <remaining>: [0] Total time: 0:00:00 (0.3684 s / it)
[03:30:42.912629] ---syncronized---
[03:30:42.912669] pixel-level F1 reduced_count 928
[03:30:42.912700] pixel-level F1 reduced_sum 693.512396762503
[03:30:42.912732] ---syncronized done ---
[03:30:43.390087] Averaged stats: pixel-level F1: [local: 0.7621 | reduced: 0.7473]
[03:30:43.391214] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:30:45.667756] Test: [0]  [0/4]  eta: 0:00:09  pixel-level F1: [local: 0.8225 | reduced: 0.8225]  time: 2.2721  data: 1.9621  max mem: 5776
[03:30:46.506865] ====================
[03:30:46.506972] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:30:46.506993] ====================
[03:30:46.522380] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8225 | reduced: 0.8486]  time: 0.7815  data: 0.4906  max mem: 5776
[03:30:46.601749] Test: [0] Total time: 0:00:03 (0.8016 s / it)
[03:30:46.601871] ***************************************************************
[03:30:46.601893] ****An extra tail dataset should exist for accracy metrics!****
[03:30:46.601909] ***************************************************************
[03:30:46.601927] **** Length of tail: 36 ****
[03:30:48.044723] Actual Batchsize/ world_size {'_n': 3.0}
[03:30:48.044858] {'pixel-level F1': tensor(2.9522, device='cuda:0', dtype=torch.float64)}
[03:30:48.066325] Test <remaining>: [0]  [0/3]  eta: 0:00:04  pixel-level F1: [local: 0.8435 | reduced: 0.8571]  time: 1.4639  data: 1.1483  max mem: 5776
[03:30:49.451211] Actual Batchsize/ world_size {'_n': 3.0}
[03:30:49.451335] {'pixel-level F1': tensor(2.1675, device='cuda:0', dtype=torch.float64)}
[03:30:49.472832] Test <remaining>: [0]  [1/3]  eta: 0:00:02  pixel-level F1: [local: 0.8225 | reduced: 0.8492]  time: 1.4350  data: 1.1194  max mem: 5776
[03:30:50.774984] Actual Batchsize/ world_size {'_n': 3.0}
[03:30:50.775103] {'pixel-level F1': tensor(2.7269, device='cuda:0', dtype=torch.float64)}
[03:30:50.796796] Test <remaining>: [0]  [2/3]  eta: 0:00:01  pixel-level F1: [local: 0.8435 | reduced: 0.8525]  time: 1.3979  data: 1.0825  max mem: 5776
[03:30:50.796920] Test <remaining>: [0] Total time: 0:00:04 (1.3983 s / it)
[03:30:50.797448] ---syncronized---
[03:30:50.797488] pixel-level F1 reduced_count 216
[03:30:50.797518] pixel-level F1 reduced_sum 182.33866437791363
[03:30:50.797549] ---syncronized done ---
[03:30:52.527563] Averaged stats: pixel-level F1: [local: 0.8435 | reduced: 0.8442]
[03:30:52.530636] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:31:26.230271] Test: [0]  [ 0/12]  eta: 0:06:44  pixel-level F1: [local: 0.3820 | reduced: 0.3820]  time: 33.6935  data: 33.3815  max mem: 5776
[03:31:40.114472] ====================
[03:31:40.114589] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:31:40.114609] ====================
[03:31:40.130295] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2931 | reduced: 0.3191]  time: 3.9661  data: 3.6636  max mem: 5776
[03:31:40.262807] Test: [0] Total time: 0:00:47 (3.9772 s / it)
[03:31:40.262916] ***************************************************************
[03:31:40.262937] ****An extra tail dataset should exist for accracy metrics!****
[03:31:40.262953] ***************************************************************
[03:31:40.262972] **** Length of tail: 36 ****
[03:32:11.376285] Actual Batchsize/ world_size {'_n': 3.0}
[03:32:11.376427] {'pixel-level F1': tensor(0.8814, device='cuda:0', dtype=torch.float64)}
[03:32:11.397725] Test <remaining>: [0]  [0/3]  eta: 0:01:33  pixel-level F1: [local: 0.2938 | reduced: 0.3186]  time: 31.1343  data: 30.8183  max mem: 5776
[03:32:42.539356] Actual Batchsize/ world_size {'_n': 3.0}
[03:32:42.539488] {'pixel-level F1': tensor(0.7604, device='cuda:0', dtype=torch.float64)}
[03:32:42.560945] Test <remaining>: [0]  [1/3]  eta: 0:01:02  pixel-level F1: [local: 0.2931 | reduced: 0.3173]  time: 31.1485  data: 30.8329  max mem: 5776
[03:33:12.347100] Actual Batchsize/ world_size {'_n': 3.0}
[03:33:12.347226] {'pixel-level F1': tensor(1.2969, device='cuda:0', dtype=torch.float64)}
[03:33:12.368818] Test <remaining>: [0]  [2/3]  eta: 0:00:30  pixel-level F1: [local: 0.2938 | reduced: 0.3196]  time: 30.7015  data: 30.3860  max mem: 5776
[03:33:12.368944] Test <remaining>: [0] Total time: 0:01:32 (30.7019 s / it)
[03:33:12.369572] ---syncronized---
[03:33:12.369609] pixel-level F1 reduced_count 600
[03:33:12.369641] pixel-level F1 reduced_sum 218.44589307111644
[03:33:12.369673] ---syncronized done ---
[03:33:14.143960] Averaged stats: pixel-level F1: [local: 0.2938 | reduced: 0.3641]
[03:33:14.148212] 
[ROBUST TEST] JpegCompressionWrapper param=50
[03:33:14.149198] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:33:15.474759] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6069 | reduced: 0.6069]  time: 1.3209  data: 1.0102  max mem: 5776
[03:33:15.735191] ====================
[03:33:15.735290] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:33:15.735310] ====================
[03:33:15.754421] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5797 | reduced: 0.5939]  time: 0.8000  data: 0.5051  max mem: 5776
[03:33:15.827049] Test: [0] Total time: 0:00:01 (0.8368 s / it)
[03:33:15.827166] ***************************************************************
[03:33:15.827187] ****An extra tail dataset should exist for accracy metrics!****
[03:33:15.827206] ***************************************************************
[03:33:15.827223] **** Length of tail: 43 ****
[03:33:16.380563] Actual Batchsize/ world_size {'_n': 3.0}
[03:33:16.380679] {'pixel-level F1': tensor(2.1698, device='cuda:0', dtype=torch.float64)}
[03:33:16.402349] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6069 | reduced: 0.6088]  time: 0.5747  data: 0.2600  max mem: 5776
[03:33:16.902223] Actual Batchsize/ world_size {'_n': 3.0}
[03:33:16.902347] {'pixel-level F1': tensor(1.4846, device='cuda:0', dtype=torch.float64)}
[03:33:16.923995] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5797 | reduced: 0.5970]  time: 0.5480  data: 0.2333  max mem: 5776
[03:33:17.461230] Actual Batchsize/ world_size {'_n': 3.0}
[03:33:17.461360] {'pixel-level F1': tensor(0.9021, device='cuda:0', dtype=torch.float64)}
[03:33:17.482998] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5797 | reduced: 0.5692]  time: 0.5515  data: 0.2369  max mem: 5776
[03:33:17.800500] ====================
[03:33:17.800611] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:33:17.800632] ====================
[03:33:17.801049] Actual Batchsize/ world_size {'_n': 1.75}
[03:33:17.801110] {'pixel-level F1': tensor(0.9515, device='cuda:0', dtype=torch.float64)}
[03:33:17.812713] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5437 | reduced: 0.5679]  time: 0.4960  data: 0.2115  max mem: 5776
[03:33:17.812871] Test <remaining>: [0] Total time: 0:00:01 (0.4964 s / it)
[03:33:17.813404] ---syncronized---
[03:33:17.813439] pixel-level F1 reduced_count 135
[03:33:17.813469] pixel-level F1 reduced_sum 65.15578091880167
[03:33:17.813501] ---syncronized done ---
[03:33:20.204884] Averaged stats: pixel-level F1: [local: 0.5437 | reduced: 0.4826]
[03:33:20.208334] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:33:21.201854] Test: [0]  [ 0/20]  eta: 0:00:19  pixel-level F1: [local: 0.3104 | reduced: 0.3104]  time: 0.9862  data: 0.6737  max mem: 5776
[03:33:26.806817] ====================
[03:33:26.806917] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:33:26.806937] ====================
[03:33:26.808600] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.6167 | reduced: 0.5882]  time: 0.3296  data: 0.0338  max mem: 5776
[03:33:26.895494] Test: [0] Total time: 0:00:06 (0.3340 s / it)
[03:33:26.895588] ***************************************************************
[03:33:26.895609] ****An extra tail dataset should exist for accracy metrics!****
[03:33:26.895627] ***************************************************************
[03:33:26.895645] **** Length of tail: 8 ****
[03:33:27.200931] ====================
[03:33:27.201011] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:33:27.201031] ====================
[03:33:27.201451] Actual Batchsize/ world_size {'_n': 2.0}
[03:33:27.201515] {'pixel-level F1': tensor(1.6914, device='cuda:0', dtype=torch.float64)}
[03:33:27.214860] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.6465 | reduced: 0.5904]  time: 0.3188  data: 0.1059  max mem: 5776
[03:33:27.214972] Test <remaining>: [0] Total time: 0:00:00 (0.3192 s / it)
[03:33:27.215496] ---syncronized---
[03:33:27.215528] pixel-level F1 reduced_count 928
[03:33:27.215560] pixel-level F1 reduced_sum 561.6997316024087
[03:33:27.215591] ---syncronized done ---
[03:33:27.630300] Averaged stats: pixel-level F1: [local: 0.6465 | reduced: 0.6053]
[03:33:27.631459] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:33:29.410564] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8226 | reduced: 0.8226]  time: 1.7747  data: 1.4646  max mem: 5776
[03:33:30.249014] ====================
[03:33:30.249119] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:33:30.249139] ====================
[03:33:30.264458] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8226 | reduced: 0.8477]  time: 0.6570  data: 0.3662  max mem: 5776
[03:33:30.339542] Test: [0] Total time: 0:00:02 (0.6760 s / it)
[03:33:30.339663] ***************************************************************
[03:33:30.339685] ****An extra tail dataset should exist for accracy metrics!****
[03:33:30.339701] ***************************************************************
[03:33:30.339718] **** Length of tail: 36 ****
[03:33:31.332668] Actual Batchsize/ world_size {'_n': 3.0}
[03:33:31.332794] {'pixel-level F1': tensor(2.9541, device='cuda:0', dtype=torch.float64)}
[03:33:31.354360] Test <remaining>: [0]  [0/3]  eta: 0:00:03  pixel-level F1: [local: 0.8449 | reduced: 0.8563]  time: 1.0142  data: 0.6986  max mem: 5776
[03:33:32.289418] Actual Batchsize/ world_size {'_n': 3.0}
[03:33:32.289546] {'pixel-level F1': tensor(2.3381, device='cuda:0', dtype=torch.float64)}
[03:33:32.311081] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8226 | reduced: 0.8518]  time: 0.9852  data: 0.6697  max mem: 5776
[03:33:33.187886] Actual Batchsize/ world_size {'_n': 3.0}
[03:33:33.188003] {'pixel-level F1': tensor(2.7246, device='cuda:0', dtype=torch.float64)}
[03:33:33.209694] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8449 | reduced: 0.8549]  time: 0.9562  data: 0.6409  max mem: 5776
[03:33:33.209821] Test <remaining>: [0] Total time: 0:00:02 (0.9567 s / it)
[03:33:33.210307] ---syncronized---
[03:33:33.210339] pixel-level F1 reduced_count 216
[03:33:33.210369] pixel-level F1 reduced_sum 179.86919124624694
[03:33:33.210400] ---syncronized done ---
[03:33:34.830326] Averaged stats: pixel-level F1: [local: 0.8449 | reduced: 0.8327]
[03:33:34.833523] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:34:00.478201] Test: [0]  [ 0/12]  eta: 0:05:07  pixel-level F1: [local: 0.4403 | reduced: 0.4403]  time: 25.6389  data: 25.3281  max mem: 5776
[03:34:11.447843] ====================
[03:34:11.447946] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:34:11.447965] ====================
[03:34:11.463590] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2755 | reduced: 0.3123]  time: 3.0520  data: 2.7498  max mem: 5776
[03:34:11.584495] Test: [0] Total time: 0:00:36 (3.0621 s / it)
[03:34:11.584622] ***************************************************************
[03:34:11.584643] ****An extra tail dataset should exist for accracy metrics!****
[03:34:11.584659] ***************************************************************
[03:34:11.584678] **** Length of tail: 36 ****
[03:34:33.518736] Actual Batchsize/ world_size {'_n': 3.0}
[03:34:33.518874] {'pixel-level F1': tensor(1.1359, device='cuda:0', dtype=torch.float64)}
[03:34:33.540342] Test <remaining>: [0]  [0/3]  eta: 0:01:05  pixel-level F1: [local: 0.3114 | reduced: 0.3136]  time: 21.9552  data: 21.6398  max mem: 5776
[03:34:55.543730] Actual Batchsize/ world_size {'_n': 3.0}
[03:34:55.543860] {'pixel-level F1': tensor(0.6486, device='cuda:0', dtype=torch.float64)}
[03:34:55.565389] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.2755 | reduced: 0.3116]  time: 21.9899  data: 21.6745  max mem: 5776
[03:35:16.727268] Actual Batchsize/ world_size {'_n': 3.0}
[03:35:16.727412] {'pixel-level F1': tensor(1.2373, device='cuda:0', dtype=torch.float64)}
[03:35:16.748948] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.3114 | reduced: 0.3137]  time: 21.7210  data: 21.4054  max mem: 5776
[03:35:16.749087] Test <remaining>: [0] Total time: 0:01:05 (21.7214 s / it)
[03:35:16.749666] ---syncronized---
[03:35:16.749703] pixel-level F1 reduced_count 600
[03:35:16.749735] pixel-level F1 reduced_sum 212.5722019727972
[03:35:16.749767] ---syncronized done ---
[03:35:18.829927] Averaged stats: pixel-level F1: [local: 0.3114 | reduced: 0.3543]
[03:35:18.833326] 
[ROBUST TEST] JpegCompressionWrapper param=60
[03:35:18.834299] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:35:20.083755] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.5852 | reduced: 0.5852]  time: 1.2453  data: 0.9350  max mem: 5776
[03:35:20.343674] ====================
[03:35:20.343750] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:35:20.343769] ====================
[03:35:20.363073] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5313 | reduced: 0.5594]  time: 0.7621  data: 0.4676  max mem: 5776
[03:35:20.440212] Test: [0] Total time: 0:00:01 (0.8011 s / it)
[03:35:20.440325] ***************************************************************
[03:35:20.440346] ****An extra tail dataset should exist for accracy metrics!****
[03:35:20.440363] ***************************************************************
[03:35:20.440381] **** Length of tail: 43 ****
[03:35:20.983461] Actual Batchsize/ world_size {'_n': 3.0}
[03:35:20.983605] {'pixel-level F1': tensor(2.2810, device='cuda:0', dtype=torch.float64)}
[03:35:21.005178] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.5852 | reduced: 0.5826]  time: 0.5643  data: 0.2498  max mem: 5776
[03:35:21.503517] Actual Batchsize/ world_size {'_n': 3.0}
[03:35:21.503636] {'pixel-level F1': tensor(1.4525, device='cuda:0', dtype=torch.float64)}
[03:35:21.525327] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5313 | reduced: 0.5724]  time: 0.5421  data: 0.2274  max mem: 5776
[03:35:22.070851] Actual Batchsize/ world_size {'_n': 3.0}
[03:35:22.070969] {'pixel-level F1': tensor(0.8118, device='cuda:0', dtype=torch.float64)}
[03:35:22.092615] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5313 | reduced: 0.5441]  time: 0.5503  data: 0.2357  max mem: 5776
[03:35:22.410019] ====================
[03:35:22.410114] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:35:22.410133] ====================
[03:35:22.410550] Actual Batchsize/ world_size {'_n': 1.75}
[03:35:22.410608] {'pixel-level F1': tensor(0.9391, device='cuda:0', dtype=torch.float64)}
[03:35:22.422214] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5313 | reduced: 0.5437]  time: 0.4951  data: 0.2106  max mem: 5776
[03:35:22.422335] Test <remaining>: [0] Total time: 0:00:01 (0.4955 s / it)
[03:35:22.422939] ---syncronized---
[03:35:22.422974] pixel-level F1 reduced_count 135
[03:35:22.423003] pixel-level F1 reduced_sum 66.56139060786454
[03:35:22.423034] ---syncronized done ---
[03:35:24.857522] Averaged stats: pixel-level F1: [local: 0.5313 | reduced: 0.4930]
[03:35:24.860891] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:35:26.014158] Test: [0]  [ 0/20]  eta: 0:00:22  pixel-level F1: [local: 0.3332 | reduced: 0.3332]  time: 1.1462  data: 0.8335  max mem: 5776
[03:35:31.618680] ====================
[03:35:31.618770] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:35:31.618790] ====================
[03:35:31.620469] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.6520 | reduced: 0.6406]  time: 0.3376  data: 0.0417  max mem: 5776
[03:35:31.707711] Test: [0] Total time: 0:00:06 (0.3420 s / it)
[03:35:31.707831] ***************************************************************
[03:35:31.707853] ****An extra tail dataset should exist for accracy metrics!****
[03:35:31.707869] ***************************************************************
[03:35:31.707886] **** Length of tail: 8 ****
[03:35:32.016474] ====================
[03:35:32.016560] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:35:32.016580] ====================
[03:35:32.016996] Actual Batchsize/ world_size {'_n': 2.0}
[03:35:32.017061] {'pixel-level F1': tensor(1.9019, device='cuda:0', dtype=torch.float64)}
[03:35:32.030385] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.6552 | reduced: 0.6433]  time: 0.3220  data: 0.1097  max mem: 5776
[03:35:32.030503] Test <remaining>: [0] Total time: 0:00:00 (0.3225 s / it)
[03:35:32.031097] ---syncronized---
[03:35:32.031134] pixel-level F1 reduced_count 928
[03:35:32.031163] pixel-level F1 reduced_sum 615.8352562661014
[03:35:32.031195] ---syncronized done ---
[03:35:32.433848] Averaged stats: pixel-level F1: [local: 0.6552 | reduced: 0.6636]
[03:35:32.435007] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:35:34.191696] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8237 | reduced: 0.8237]  time: 1.7524  data: 1.4424  max mem: 5776
[03:35:35.030166] ====================
[03:35:35.030277] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:35:35.030296] ====================
[03:35:35.045688] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8237 | reduced: 0.8435]  time: 0.6515  data: 0.3607  max mem: 5776
[03:35:35.120498] Test: [0] Total time: 0:00:02 (0.6704 s / it)
[03:35:35.120623] ***************************************************************
[03:35:35.120645] ****An extra tail dataset should exist for accracy metrics!****
[03:35:35.120663] ***************************************************************
[03:35:35.120681] **** Length of tail: 36 ****
[03:35:36.090916] Actual Batchsize/ world_size {'_n': 3.0}
[03:35:36.091060] {'pixel-level F1': tensor(2.9539, device='cuda:0', dtype=torch.float64)}
[03:35:36.112597] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8324 | reduced: 0.8523]  time: 0.9914  data: 0.6761  max mem: 5776
[03:35:37.049610] Actual Batchsize/ world_size {'_n': 3.0}
[03:35:37.049742] {'pixel-level F1': tensor(2.1855, device='cuda:0', dtype=torch.float64)}
[03:35:37.071300] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8237 | reduced: 0.8451]  time: 0.9749  data: 0.6596  max mem: 5776
[03:35:37.944256] Actual Batchsize/ world_size {'_n': 3.0}
[03:35:37.944379] {'pixel-level F1': tensor(2.7224, device='cuda:0', dtype=torch.float64)}
[03:35:37.966077] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8324 | reduced: 0.8485]  time: 0.9480  data: 0.6330  max mem: 5776
[03:35:37.966202] Test <remaining>: [0] Total time: 0:00:02 (0.9485 s / it)
[03:35:37.966714] ---syncronized---
[03:35:37.966749] pixel-level F1 reduced_count 216
[03:35:37.966780] pixel-level F1 reduced_sum 178.57449888023052
[03:35:37.966812] ---syncronized done ---
[03:35:39.759849] Averaged stats: pixel-level F1: [local: 0.8324 | reduced: 0.8267]
[03:35:39.763000] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:36:04.954501] Test: [0]  [ 0/12]  eta: 0:05:02  pixel-level F1: [local: 0.4374 | reduced: 0.4374]  time: 25.1857  data: 24.8749  max mem: 5776
[03:36:15.690101] ====================
[03:36:15.690226] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:36:15.690246] ====================
[03:36:15.705884] Test: [0]  [11/12]  eta: 0:00:02  pixel-level F1: [local: 0.2956 | reduced: 0.3076]  time: 2.9947  data: 2.6924  max mem: 5776
[03:36:15.837723] Test: [0] Total time: 0:00:36 (3.0058 s / it)
[03:36:15.837858] ***************************************************************
[03:36:15.837880] ****An extra tail dataset should exist for accracy metrics!****
[03:36:15.837897] ***************************************************************
[03:36:15.837915] **** Length of tail: 36 ****
[03:36:37.641756] Actual Batchsize/ world_size {'_n': 3.0}
[03:36:37.641914] {'pixel-level F1': tensor(0.9921, device='cuda:0', dtype=torch.float64)}
[03:36:37.663183] Test <remaining>: [0]  [0/3]  eta: 0:01:05  pixel-level F1: [local: 0.3123 | reduced: 0.3081]  time: 21.8246  data: 21.5073  max mem: 5776
[03:36:59.737764] Actual Batchsize/ world_size {'_n': 3.0}
[03:36:59.737890] {'pixel-level F1': tensor(0.6453, device='cuda:0', dtype=torch.float64)}
[03:36:59.759460] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.2956 | reduced: 0.3062]  time: 21.9603  data: 21.6437  max mem: 5776
[03:37:20.606041] Actual Batchsize/ world_size {'_n': 3.0}
[03:37:20.606171] {'pixel-level F1': tensor(1.2533, device='cuda:0', dtype=torch.float64)}
[03:37:20.627809] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.3123 | reduced: 0.3084]  time: 21.5962  data: 21.2801  max mem: 5776
[03:37:20.627939] Test <remaining>: [0] Total time: 0:01:04 (21.5966 s / it)
[03:37:20.628653] ---syncronized---
[03:37:20.628693] pixel-level F1 reduced_count 600
[03:37:20.628724] pixel-level F1 reduced_sum 210.29277380121454
[03:37:20.628755] ---syncronized done ---
[03:37:22.780395] Averaged stats: pixel-level F1: [local: 0.3123 | reduced: 0.3505]
[03:37:22.783714] 
[ROBUST TEST] JpegCompressionWrapper param=70
[03:37:22.784701] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:37:24.041262] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.5680 | reduced: 0.5680]  time: 1.2525  data: 0.9424  max mem: 5776
[03:37:24.301219] ====================
[03:37:24.301291] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:37:24.301311] ====================
[03:37:24.320611] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5347 | reduced: 0.5521]  time: 0.7657  data: 0.4713  max mem: 5776
[03:37:24.396041] Test: [0] Total time: 0:00:01 (0.8039 s / it)
[03:37:24.396151] ***************************************************************
[03:37:24.396172] ****An extra tail dataset should exist for accracy metrics!****
[03:37:24.396188] ***************************************************************
[03:37:24.396204] **** Length of tail: 43 ****
[03:37:24.948979] Actual Batchsize/ world_size {'_n': 3.0}
[03:37:24.949110] {'pixel-level F1': tensor(2.2015, device='cuda:0', dtype=torch.float64)}
[03:37:24.970714] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.5680 | reduced: 0.5731]  time: 0.5741  data: 0.2589  max mem: 5776
[03:37:25.471231] Actual Batchsize/ world_size {'_n': 3.0}
[03:37:25.471352] {'pixel-level F1': tensor(1.2918, device='cuda:0', dtype=torch.float64)}
[03:37:25.492992] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5347 | reduced: 0.5583]  time: 0.5480  data: 0.2327  max mem: 5776
[03:37:26.034715] Actual Batchsize/ world_size {'_n': 3.0}
[03:37:26.034841] {'pixel-level F1': tensor(0.8015, device='cuda:0', dtype=torch.float64)}
[03:37:26.056269] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5347 | reduced: 0.5310]  time: 0.5530  data: 0.2378  max mem: 5776
[03:37:26.376264] ====================
[03:37:26.376359] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:37:26.376379] ====================
[03:37:26.376832] Actual Batchsize/ world_size {'_n': 1.75}
[03:37:26.376899] {'pixel-level F1': tensor(0.9380, device='cuda:0', dtype=torch.float64)}
[03:37:26.388451] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5347 | reduced: 0.5313]  time: 0.4977  data: 0.2126  max mem: 5776
[03:37:26.388588] Test <remaining>: [0] Total time: 0:00:01 (0.4981 s / it)
[03:37:26.389146] ---syncronized---
[03:37:26.389184] pixel-level F1 reduced_count 135
[03:37:26.389214] pixel-level F1 reduced_sum 62.81702678960946
[03:37:26.389244] ---syncronized done ---
[03:37:28.883204] Averaged stats: pixel-level F1: [local: 0.5347 | reduced: 0.4653]
[03:37:28.886576] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:37:30.189575] Test: [0]  [ 0/20]  eta: 0:00:25  pixel-level F1: [local: 0.4280 | reduced: 0.4280]  time: 1.2960  data: 0.9852  max mem: 5776
[03:37:35.794131] ====================
[03:37:35.794230] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:37:35.794251] ====================
[03:37:35.795909] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.6911 | reduced: 0.6934]  time: 0.3451  data: 0.0493  max mem: 5776
[03:37:35.885646] Test: [0] Total time: 0:00:06 (0.3496 s / it)
[03:37:35.885747] ***************************************************************
[03:37:35.885770] ****An extra tail dataset should exist for accracy metrics!****
[03:37:35.885786] ***************************************************************
[03:37:35.885804] **** Length of tail: 8 ****
[03:37:36.190404] ====================
[03:37:36.190489] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:37:36.190510] ====================
[03:37:36.190926] Actual Batchsize/ world_size {'_n': 2.0}
[03:37:36.190987] {'pixel-level F1': tensor(1.9091, device='cuda:0', dtype=torch.float64)}
[03:37:36.204329] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.6936 | reduced: 0.6957]  time: 0.3181  data: 0.1054  max mem: 5776
[03:37:36.204443] Test <remaining>: [0] Total time: 0:00:00 (0.3185 s / it)
[03:37:36.204955] ---syncronized---
[03:37:36.204995] pixel-level F1 reduced_count 928
[03:37:36.205025] pixel-level F1 reduced_sum 645.8228970832043
[03:37:36.205055] ---syncronized done ---
[03:37:36.598462] Averaged stats: pixel-level F1: [local: 0.6936 | reduced: 0.6959]
[03:37:36.599605] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:37:38.369511] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8279 | reduced: 0.8279]  time: 1.7655  data: 1.4555  max mem: 5776
[03:37:39.208594] ====================
[03:37:39.208703] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:37:39.208724] ====================
[03:37:39.224023] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8158 | reduced: 0.8427]  time: 0.6549  data: 0.3639  max mem: 5776
[03:37:39.302066] Test: [0] Total time: 0:00:02 (0.6746 s / it)
[03:37:39.302185] ***************************************************************
[03:37:39.302208] ****An extra tail dataset should exist for accracy metrics!****
[03:37:39.302225] ***************************************************************
[03:37:39.302243] **** Length of tail: 36 ****
[03:37:40.287682] Actual Batchsize/ world_size {'_n': 3.0}
[03:37:40.287825] {'pixel-level F1': tensor(2.9411, device='cuda:0', dtype=torch.float64)}
[03:37:40.309065] Test <remaining>: [0]  [0/3]  eta: 0:00:03  pixel-level F1: [local: 0.8279 | reduced: 0.8513]  time: 1.0063  data: 0.6874  max mem: 5776
[03:37:41.247221] Actual Batchsize/ world_size {'_n': 3.0}
[03:37:41.247351] {'pixel-level F1': tensor(2.1637, device='cuda:0', dtype=torch.float64)}
[03:37:41.268749] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8158 | reduced: 0.8436]  time: 0.9828  data: 0.6656  max mem: 5776
[03:37:42.169854] Actual Batchsize/ world_size {'_n': 3.0}
[03:37:42.169975] {'pixel-level F1': tensor(2.7215, device='cuda:0', dtype=torch.float64)}
[03:37:42.191542] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8279 | reduced: 0.8471]  time: 0.9627  data: 0.6461  max mem: 5776
[03:37:42.191686] Test <remaining>: [0] Total time: 0:00:02 (0.9631 s / it)
[03:37:42.192161] ---syncronized---
[03:37:42.192193] pixel-level F1 reduced_count 216
[03:37:42.192222] pixel-level F1 reduced_sum 178.19769364922067
[03:37:42.192252] ---syncronized done ---
[03:37:44.198134] Averaged stats: pixel-level F1: [local: 0.8279 | reduced: 0.8250]
[03:37:44.201204] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:38:09.115997] Test: [0]  [ 0/12]  eta: 0:04:58  pixel-level F1: [local: 0.4375 | reduced: 0.4375]  time: 24.9089  data: 24.5981  max mem: 5776
[03:38:20.557732] ====================
[03:38:20.557838] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:38:20.557858] ====================
[03:38:20.573481] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2811 | reduced: 0.3067]  time: 3.0305  data: 2.7283  max mem: 5776
[03:38:20.681293] Test: [0] Total time: 0:00:36 (3.0396 s / it)
[03:38:20.681393] ***************************************************************
[03:38:20.681415] ****An extra tail dataset should exist for accracy metrics!****
[03:38:20.681431] ***************************************************************
[03:38:20.681448] **** Length of tail: 36 ****
[03:38:42.660284] Actual Batchsize/ world_size {'_n': 3.0}
[03:38:42.660423] {'pixel-level F1': tensor(1.0381, device='cuda:0', dtype=torch.float64)}
[03:38:42.682045] Test <remaining>: [0]  [0/3]  eta: 0:01:06  pixel-level F1: [local: 0.3091 | reduced: 0.3075]  time: 22.0001  data: 21.6847  max mem: 5776
[03:39:04.839970] Actual Batchsize/ world_size {'_n': 3.0}
[03:39:04.840120] {'pixel-level F1': tensor(0.6681, device='cuda:0', dtype=torch.float64)}
[03:39:04.861610] Test <remaining>: [0]  [1/3]  eta: 0:00:44  pixel-level F1: [local: 0.2811 | reduced: 0.3058]  time: 22.0896  data: 21.7739  max mem: 5776
[03:39:25.885147] Actual Batchsize/ world_size {'_n': 3.0}
[03:39:25.885282] {'pixel-level F1': tensor(1.3120, device='cuda:0', dtype=torch.float64)}
[03:39:25.906759] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.3091 | reduced: 0.3084]  time: 21.7413  data: 21.4256  max mem: 5776
[03:39:25.906904] Test <remaining>: [0] Total time: 0:01:05 (21.7418 s / it)
[03:39:25.907540] ---syncronized---
[03:39:25.907575] pixel-level F1 reduced_count 600
[03:39:25.907607] pixel-level F1 reduced_sum 212.03188321196518
[03:39:25.907638] ---syncronized done ---
[03:39:28.047429] Averaged stats: pixel-level F1: [local: 0.3091 | reduced: 0.3534]
[03:39:28.050815] 
[ROBUST TEST] JpegCompressionWrapper param=80
[03:39:28.051897] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:39:29.320733] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6183 | reduced: 0.6183]  time: 1.2646  data: 0.9544  max mem: 5776
[03:39:29.580926] ====================
[03:39:29.581012] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:39:29.581031] ====================
[03:39:29.600229] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5379 | reduced: 0.5799]  time: 0.7718  data: 0.4772  max mem: 5776
[03:39:29.679738] Test: [0] Total time: 0:00:01 (0.8120 s / it)
[03:39:29.679855] ***************************************************************
[03:39:29.679876] ****An extra tail dataset should exist for accracy metrics!****
[03:39:29.679893] ***************************************************************
[03:39:29.679910] **** Length of tail: 43 ****
[03:39:30.229900] Actual Batchsize/ world_size {'_n': 3.0}
[03:39:30.230019] {'pixel-level F1': tensor(2.1343, device='cuda:0', dtype=torch.float64)}
[03:39:30.251485] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6183 | reduced: 0.5950]  time: 0.5711  data: 0.2559  max mem: 5776
[03:39:30.755466] Actual Batchsize/ world_size {'_n': 3.0}
[03:39:30.755590] {'pixel-level F1': tensor(1.3509, device='cuda:0', dtype=torch.float64)}
[03:39:30.777237] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5379 | reduced: 0.5801]  time: 0.5483  data: 0.2331  max mem: 5776
[03:39:31.326750] Actual Batchsize/ world_size {'_n': 3.0}
[03:39:31.326867] {'pixel-level F1': tensor(0.8575, device='cuda:0', dtype=torch.float64)}
[03:39:31.348377] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5379 | reduced: 0.5525]  time: 0.5558  data: 0.2405  max mem: 5776
[03:39:31.670701] ====================
[03:39:31.670796] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:39:31.670817] ====================
[03:39:31.671228] Actual Batchsize/ world_size {'_n': 1.75}
[03:39:31.671288] {'pixel-level F1': tensor(0.8907, device='cuda:0', dtype=torch.float64)}
[03:39:31.682904] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5090 | reduced: 0.5502]  time: 0.5004  data: 0.2153  max mem: 5776
[03:39:31.683034] Test <remaining>: [0] Total time: 0:00:02 (0.5008 s / it)
[03:39:31.683627] ---syncronized---
[03:39:31.683662] pixel-level F1 reduced_count 135
[03:39:31.683691] pixel-level F1 reduced_sum 65.01902543327054
[03:39:31.683721] ---syncronized done ---
[03:39:34.183214] Averaged stats: pixel-level F1: [local: 0.5090 | reduced: 0.4816]
[03:39:34.186548] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:39:35.473974] Test: [0]  [ 0/20]  eta: 0:00:25  pixel-level F1: [local: 0.3912 | reduced: 0.3912]  time: 1.2804  data: 0.9692  max mem: 5776
[03:39:41.079642] ====================
[03:39:41.079734] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:39:41.079755] ====================
[03:39:41.081472] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7542 | reduced: 0.6874]  time: 0.3444  data: 0.0485  max mem: 5776
[03:39:41.166501] Test: [0] Total time: 0:00:06 (0.3487 s / it)
[03:39:41.166601] ***************************************************************
[03:39:41.166623] ****An extra tail dataset should exist for accracy metrics!****
[03:39:41.166639] ***************************************************************
[03:39:41.166656] **** Length of tail: 8 ****
[03:39:41.472894] ====================
[03:39:41.472980] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:39:41.473001] ====================
[03:39:41.473427] Actual Batchsize/ world_size {'_n': 2.0}
[03:39:41.473491] {'pixel-level F1': tensor(1.7741, device='cuda:0', dtype=torch.float64)}
[03:39:41.486834] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7569 | reduced: 0.6891]  time: 0.3197  data: 0.1070  max mem: 5776
[03:39:41.486952] Test <remaining>: [0] Total time: 0:00:00 (0.3201 s / it)
[03:39:41.487493] ---syncronized---
[03:39:41.487526] pixel-level F1 reduced_count 928
[03:39:41.487558] pixel-level F1 reduced_sum 643.0817599482089
[03:39:41.487590] ---syncronized done ---
[03:39:41.892672] Averaged stats: pixel-level F1: [local: 0.7569 | reduced: 0.6930]
[03:39:41.893814] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:39:43.674698] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8222 | reduced: 0.8222]  time: 1.7765  data: 1.4659  max mem: 5776
[03:39:44.514024] ====================
[03:39:44.514138] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:39:44.514159] ====================
[03:39:44.529479] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8222 | reduced: 0.8452]  time: 0.6577  data: 0.3666  max mem: 5776
[03:39:44.607130] Test: [0] Total time: 0:00:02 (0.6773 s / it)
[03:39:44.607233] ***************************************************************
[03:39:44.607255] ****An extra tail dataset should exist for accracy metrics!****
[03:39:44.607272] ***************************************************************
[03:39:44.607290] **** Length of tail: 36 ****
[03:39:45.596242] Actual Batchsize/ world_size {'_n': 3.0}
[03:39:45.596379] {'pixel-level F1': tensor(2.9543, device='cuda:0', dtype=torch.float64)}
[03:39:45.617880] Test <remaining>: [0]  [0/3]  eta: 0:00:03  pixel-level F1: [local: 0.8313 | reduced: 0.8539]  time: 1.0101  data: 0.6939  max mem: 5776
[03:39:46.564205] Actual Batchsize/ world_size {'_n': 3.0}
[03:39:46.564341] {'pixel-level F1': tensor(2.2261, device='cuda:0', dtype=torch.float64)}
[03:39:46.585882] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8222 | reduced: 0.8473]  time: 0.9889  data: 0.6730  max mem: 5776
[03:39:47.476350] Actual Batchsize/ world_size {'_n': 3.0}
[03:39:47.476474] {'pixel-level F1': tensor(2.7257, device='cuda:0', dtype=torch.float64)}
[03:39:47.498101] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8313 | reduced: 0.8507]  time: 0.9632  data: 0.6475  max mem: 5776
[03:39:47.498235] Test <remaining>: [0] Total time: 0:00:02 (0.9636 s / it)
[03:39:47.498761] ---syncronized---
[03:39:47.498796] pixel-level F1 reduced_count 216
[03:39:47.498829] pixel-level F1 reduced_sum 180.4490880589039
[03:39:47.498863] ---syncronized done ---
[03:39:49.753009] Averaged stats: pixel-level F1: [local: 0.8313 | reduced: 0.8354]
[03:39:49.756009] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:40:15.535265] Test: [0]  [ 0/12]  eta: 0:05:09  pixel-level F1: [local: 0.4361 | reduced: 0.4361]  time: 25.7735  data: 25.4628  max mem: 5776
[03:40:26.425179] ====================
[03:40:26.425294] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:40:26.425313] ====================
[03:40:26.440610] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.3232 | reduced: 0.3131]  time: 3.0565  data: 2.7542  max mem: 5776
[03:40:26.565166] Test: [0] Total time: 0:00:36 (3.0670 s / it)
[03:40:26.565281] ***************************************************************
[03:40:26.565302] ****An extra tail dataset should exist for accracy metrics!****
[03:40:26.565319] ***************************************************************
[03:40:26.565336] **** Length of tail: 36 ****
[03:40:48.135793] Actual Batchsize/ world_size {'_n': 3.0}
[03:40:48.135937] {'pixel-level F1': tensor(0.9312, device='cuda:0', dtype=torch.float64)}
[03:40:48.157436] Test <remaining>: [0]  [0/3]  eta: 0:01:04  pixel-level F1: [local: 0.3232 | reduced: 0.3130]  time: 21.5916  data: 21.2762  max mem: 5776
[03:41:10.087209] Actual Batchsize/ world_size {'_n': 3.0}
[03:41:10.087338] {'pixel-level F1': tensor(0.6635, device='cuda:0', dtype=torch.float64)}
[03:41:10.108880] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.3104 | reduced: 0.3112]  time: 21.7713  data: 21.4560  max mem: 5776
[03:41:31.092364] Actual Batchsize/ world_size {'_n': 3.0}
[03:41:31.092489] {'pixel-level F1': tensor(1.2635, device='cuda:0', dtype=torch.float64)}
[03:41:31.114054] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.3232 | reduced: 0.3134]  time: 21.5158  data: 21.2008  max mem: 5776
[03:41:31.114187] Test <remaining>: [0] Total time: 0:01:04 (21.5162 s / it)
[03:41:31.114760] ---syncronized---
[03:41:31.114793] pixel-level F1 reduced_count 600
[03:41:31.114823] pixel-level F1 reduced_sum 210.4455723728416
[03:41:31.114853] ---syncronized done ---
[03:41:33.317232] Averaged stats: pixel-level F1: [local: 0.3232 | reduced: 0.3507]
[03:41:33.320505] 
[ROBUST TEST] JpegCompressionWrapper param=90
[03:41:33.321387] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:41:34.564870] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6040 | reduced: 0.6040]  time: 1.2394  data: 0.9292  max mem: 5776
[03:41:34.824883] ====================
[03:41:34.824959] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:41:34.824980] ====================
[03:41:34.844269] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.6040 | reduced: 0.6108]  time: 0.7592  data: 0.4646  max mem: 5776
[03:41:34.924309] Test: [0] Total time: 0:00:01 (0.7997 s / it)
[03:41:34.924425] ***************************************************************
[03:41:34.924445] ****An extra tail dataset should exist for accracy metrics!****
[03:41:34.924461] ***************************************************************
[03:41:34.924478] **** Length of tail: 43 ****
[03:41:35.474127] Actual Batchsize/ world_size {'_n': 3.0}
[03:41:35.474246] {'pixel-level F1': tensor(2.1943, device='cuda:0', dtype=torch.float64)}
[03:41:35.495933] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6183 | reduced: 0.6247]  time: 0.5710  data: 0.2564  max mem: 5776
[03:41:36.002660] Actual Batchsize/ world_size {'_n': 3.0}
[03:41:36.002775] {'pixel-level F1': tensor(1.4986, device='cuda:0', dtype=torch.float64)}
[03:41:36.024518] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.6040 | reduced: 0.6118]  time: 0.5496  data: 0.2350  max mem: 5776
[03:41:36.574280] Actual Batchsize/ world_size {'_n': 3.0}
[03:41:36.574394] {'pixel-level F1': tensor(1.1708, device='cuda:0', dtype=torch.float64)}
[03:41:36.596064] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.6040 | reduced: 0.5910]  time: 0.5568  data: 0.2422  max mem: 5776
[03:41:36.916493] ====================
[03:41:36.916593] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:41:36.916613] ====================
[03:41:36.917038] Actual Batchsize/ world_size {'_n': 1.75}
[03:41:36.917095] {'pixel-level F1': tensor(0.8456, device='cuda:0', dtype=torch.float64)}
[03:41:36.928624] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.4995 | reduced: 0.5854]  time: 0.5007  data: 0.2163  max mem: 5776
[03:41:36.928756] Test <remaining>: [0] Total time: 0:00:02 (0.5010 s / it)
[03:41:36.929392] ---syncronized---
[03:41:36.929425] pixel-level F1 reduced_count 135
[03:41:36.929455] pixel-level F1 reduced_sum 69.66446639968761
[03:41:36.929486] ---syncronized done ---
[03:41:39.432462] Averaged stats: pixel-level F1: [local: 0.4995 | reduced: 0.5160]
[03:41:39.435794] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:41:40.482257] Test: [0]  [ 0/20]  eta: 0:00:20  pixel-level F1: [local: 0.4554 | reduced: 0.4554]  time: 1.0394  data: 0.7255  max mem: 5776
[03:41:46.099287] ====================
[03:41:46.099375] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:41:46.099396] ====================
[03:41:46.101059] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7797 | reduced: 0.7436]  time: 0.3329  data: 0.0364  max mem: 5776
[03:41:46.187681] Test: [0] Total time: 0:00:06 (0.3373 s / it)
[03:41:46.187779] ***************************************************************
[03:41:46.187800] ****An extra tail dataset should exist for accracy metrics!****
[03:41:46.187817] ***************************************************************
[03:41:46.187834] **** Length of tail: 8 ****
[03:41:46.497231] ====================
[03:41:46.497311] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:41:46.497330] ====================
[03:41:46.497747] Actual Batchsize/ world_size {'_n': 2.0}
[03:41:46.497809] {'pixel-level F1': tensor(1.8883, device='cuda:0', dtype=torch.float64)}
[03:41:46.511118] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7913 | reduced: 0.7454]  time: 0.3228  data: 0.1087  max mem: 5776
[03:41:46.511241] Test <remaining>: [0] Total time: 0:00:00 (0.3233 s / it)
[03:41:46.511757] ---syncronized---
[03:41:46.511791] pixel-level F1 reduced_count 928
[03:41:46.511823] pixel-level F1 reduced_sum 679.6244698119585
[03:41:46.511855] ---syncronized done ---
[03:41:46.977204] Averaged stats: pixel-level F1: [local: 0.7913 | reduced: 0.7324]
[03:41:46.978489] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:41:48.778724] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8224 | reduced: 0.8224]  time: 1.7932  data: 1.4829  max mem: 5776
[03:41:49.618793] ====================
[03:41:49.618900] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:41:49.618919] ====================
[03:41:49.634271] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8224 | reduced: 0.8462]  time: 0.6621  data: 0.3708  max mem: 5776
[03:41:49.711264] Test: [0] Total time: 0:00:02 (0.6815 s / it)
[03:41:49.711378] ***************************************************************
[03:41:49.711400] ****An extra tail dataset should exist for accracy metrics!****
[03:41:49.711417] ***************************************************************
[03:41:49.711435] **** Length of tail: 36 ****
[03:41:50.697815] Actual Batchsize/ world_size {'_n': 3.0}
[03:41:50.697951] {'pixel-level F1': tensor(2.9568, device='cuda:0', dtype=torch.float64)}
[03:41:50.719523] Test <remaining>: [0]  [0/3]  eta: 0:00:03  pixel-level F1: [local: 0.8328 | reduced: 0.8549]  time: 1.0076  data: 0.6920  max mem: 5776
[03:41:51.660124] Actual Batchsize/ world_size {'_n': 3.0}
[03:41:51.660252] {'pixel-level F1': tensor(2.3714, device='cuda:0', dtype=torch.float64)}
[03:41:51.681778] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8224 | reduced: 0.8511]  time: 0.9847  data: 0.6694  max mem: 5776
[03:41:52.570770] Actual Batchsize/ world_size {'_n': 3.0}
[03:41:52.570887] {'pixel-level F1': tensor(2.7256, device='cuda:0', dtype=torch.float64)}
[03:41:52.592557] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8328 | reduced: 0.8543]  time: 0.9600  data: 0.6448  max mem: 5776
[03:41:52.592684] Test <remaining>: [0] Total time: 0:00:02 (0.9604 s / it)
[03:41:52.593169] ---syncronized---
[03:41:52.593201] pixel-level F1 reduced_count 216
[03:41:52.593231] pixel-level F1 reduced_sum 184.2294949859387
[03:41:52.593261] ---syncronized done ---
[03:41:54.902401] Averaged stats: pixel-level F1: [local: 0.8328 | reduced: 0.8529]
[03:41:54.905414] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:42:20.605521] Test: [0]  [ 0/12]  eta: 0:05:08  pixel-level F1: [local: 0.4487 | reduced: 0.4487]  time: 25.6943  data: 25.3802  max mem: 5776
[03:42:31.910261] ====================
[03:42:31.910417] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:42:31.910437] ====================
[03:42:31.925798] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.3110 | reduced: 0.3152]  time: 3.0845  data: 2.7821  max mem: 5776
[03:42:32.057721] Test: [0] Total time: 0:00:37 (3.0956 s / it)
[03:42:32.057834] ***************************************************************
[03:42:32.057855] ****An extra tail dataset should exist for accracy metrics!****
[03:42:32.057871] ***************************************************************
[03:42:32.057890] **** Length of tail: 36 ****
[03:42:53.989573] Actual Batchsize/ world_size {'_n': 3.0}
[03:42:53.989720] {'pixel-level F1': tensor(0.9810, device='cuda:0', dtype=torch.float64)}
[03:42:54.011100] Test <remaining>: [0]  [0/3]  eta: 0:01:05  pixel-level F1: [local: 0.3216 | reduced: 0.3155]  time: 21.9527  data: 21.6367  max mem: 5776
[03:43:15.951366] Actual Batchsize/ world_size {'_n': 3.0}
[03:43:15.951491] {'pixel-level F1': tensor(0.6689, device='cuda:0', dtype=torch.float64)}
[03:43:15.973159] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.3110 | reduced: 0.3136]  time: 21.9572  data: 21.6416  max mem: 5776
[03:43:37.132078] Actual Batchsize/ world_size {'_n': 3.0}
[03:43:37.132204] {'pixel-level F1': tensor(1.2578, device='cuda:0', dtype=torch.float64)}
[03:43:37.153745] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.3216 | reduced: 0.3157]  time: 21.6982  data: 21.3828  max mem: 5776
[03:43:37.153875] Test <remaining>: [0] Total time: 0:01:05 (21.6986 s / it)
[03:43:37.154483] ---syncronized---
[03:43:37.154516] pixel-level F1 reduced_count 600
[03:43:37.154550] pixel-level F1 reduced_sum 211.63288685418473
[03:43:37.154582] ---syncronized done ---
[03:43:39.368721] Averaged stats: pixel-level F1: [local: 0.3216 | reduced: 0.3527]
[03:43:39.372046] 
[ROBUST TEST] JpegCompressionWrapper param=100
[03:43:39.373010] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:43:40.645227] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6629 | reduced: 0.6629]  time: 1.2681  data: 0.9576  max mem: 5776
[03:43:40.905903] ====================
[03:43:40.905977] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:43:40.905997] ====================
[03:43:40.925177] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5727 | reduced: 0.6198]  time: 0.7738  data: 0.4789  max mem: 5776
[03:43:41.007121] Test: [0] Total time: 0:00:01 (0.8153 s / it)
[03:43:41.007220] ***************************************************************
[03:43:41.007240] ****An extra tail dataset should exist for accracy metrics!****
[03:43:41.007257] ***************************************************************
[03:43:41.007275] **** Length of tail: 43 ****
[03:43:41.573925] Actual Batchsize/ world_size {'_n': 3.0}
[03:43:41.574049] {'pixel-level F1': tensor(1.8707, device='cuda:0', dtype=torch.float64)}
[03:43:41.595638] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6236 | reduced: 0.6202]  time: 0.5879  data: 0.2725  max mem: 5776
[03:43:42.119872] Actual Batchsize/ world_size {'_n': 3.0}
[03:43:42.119984] {'pixel-level F1': tensor(1.5587, device='cuda:0', dtype=torch.float64)}
[03:43:42.141611] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5727 | reduced: 0.6098]  time: 0.5668  data: 0.2514  max mem: 5776
[03:43:42.706609] Actual Batchsize/ world_size {'_n': 3.0}
[03:43:42.706732] {'pixel-level F1': tensor(0.8499, device='cuda:0', dtype=torch.float64)}
[03:43:42.728153] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5727 | reduced: 0.5792]  time: 0.5732  data: 0.2578  max mem: 5776
[03:43:43.059150] ====================
[03:43:43.059247] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:43:43.059268] ====================
[03:43:43.059688] Actual Batchsize/ world_size {'_n': 1.75}
[03:43:43.059747] {'pixel-level F1': tensor(0.8376, device='cuda:0', dtype=torch.float64)}
[03:43:43.071231] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5196 | reduced: 0.5740]  time: 0.5156  data: 0.2304  max mem: 5776
[03:43:43.071354] Test <remaining>: [0] Total time: 0:00:02 (0.5160 s / it)
[03:43:43.071988] ---syncronized---
[03:43:43.072021] pixel-level F1 reduced_count 135
[03:43:43.072051] pixel-level F1 reduced_sum 65.25145657607365
[03:43:43.072081] ---syncronized done ---
[03:43:45.748010] Averaged stats: pixel-level F1: [local: 0.5196 | reduced: 0.4833]
[03:43:45.751335] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:43:47.054750] Test: [0]  [ 0/20]  eta: 0:00:25  pixel-level F1: [local: 0.4620 | reduced: 0.4620]  time: 1.2963  data: 0.9850  max mem: 5776
[03:43:52.660088] ====================
[03:43:52.660176] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:43:52.660196] ====================
[03:43:52.661909] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7693 | reduced: 0.7367]  time: 0.3451  data: 0.0493  max mem: 5776
[03:43:52.749131] Test: [0] Total time: 0:00:06 (0.3496 s / it)
[03:43:52.749223] ***************************************************************
[03:43:52.749242] ****An extra tail dataset should exist for accracy metrics!****
[03:43:52.749259] ***************************************************************
[03:43:52.749276] **** Length of tail: 8 ****
[03:43:53.060641] ====================
[03:43:53.060726] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:43:53.060746] ====================
[03:43:53.061161] Actual Batchsize/ world_size {'_n': 2.0}
[03:43:53.061219] {'pixel-level F1': tensor(1.8882, device='cuda:0', dtype=torch.float64)}
[03:43:53.074537] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7923 | reduced: 0.7385]  time: 0.3248  data: 0.1123  max mem: 5776
[03:43:53.074648] Test <remaining>: [0] Total time: 0:00:00 (0.3252 s / it)
[03:43:53.075170] ---syncronized---
[03:43:53.075205] pixel-level F1 reduced_count 928
[03:43:53.075235] pixel-level F1 reduced_sum 687.8684939885866
[03:43:53.075266] ---syncronized done ---
[03:43:53.486418] Averaged stats: pixel-level F1: [local: 0.7923 | reduced: 0.7412]
[03:43:53.487565] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:43:55.326427] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8217 | reduced: 0.8217]  time: 1.8345  data: 1.5244  max mem: 5776
[03:43:56.164958] ====================
[03:43:56.165077] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:43:56.165097] ====================
[03:43:56.180382] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8217 | reduced: 0.8463]  time: 0.6720  data: 0.3812  max mem: 5776
[03:43:56.260605] Test: [0] Total time: 0:00:02 (0.6923 s / it)
[03:43:56.260724] ***************************************************************
[03:43:56.260745] ****An extra tail dataset should exist for accracy metrics!****
[03:43:56.260762] ***************************************************************
[03:43:56.260780] **** Length of tail: 36 ****
[03:43:57.286516] Actual Batchsize/ world_size {'_n': 3.0}
[03:43:57.286654] {'pixel-level F1': tensor(2.9564, device='cuda:0', dtype=torch.float64)}
[03:43:57.308201] Test <remaining>: [0]  [0/3]  eta: 0:00:03  pixel-level F1: [local: 0.8361 | reduced: 0.8550]  time: 1.0469  data: 0.7314  max mem: 5776
[03:43:58.317999] Actual Batchsize/ world_size {'_n': 3.0}
[03:43:58.318132] {'pixel-level F1': tensor(2.3539, device='cuda:0', dtype=torch.float64)}
[03:43:58.339732] Test <remaining>: [0]  [1/3]  eta: 0:00:02  pixel-level F1: [local: 0.8217 | reduced: 0.8508]  time: 1.0390  data: 0.7235  max mem: 5776
[03:43:59.272744] Actual Batchsize/ world_size {'_n': 3.0}
[03:43:59.272863] {'pixel-level F1': tensor(2.7250, device='cuda:0', dtype=torch.float64)}
[03:43:59.294551] Test <remaining>: [0]  [2/3]  eta: 0:00:01  pixel-level F1: [local: 0.8361 | reduced: 0.8540]  time: 1.0108  data: 0.6955  max mem: 5776
[03:43:59.294674] Test <remaining>: [0] Total time: 0:00:03 (1.0113 s / it)
[03:43:59.295170] ---syncronized---
[03:43:59.295203] pixel-level F1 reduced_count 216
[03:43:59.295233] pixel-level F1 reduced_sum 184.3685975131849
[03:43:59.295264] ---syncronized done ---
[03:44:01.661310] Averaged stats: pixel-level F1: [local: 0.8361 | reduced: 0.8536]
[03:44:01.664344] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:44:27.586801] Test: [0]  [ 0/12]  eta: 0:05:10  pixel-level F1: [local: 0.4499 | reduced: 0.4499]  time: 25.9165  data: 25.6051  max mem: 5776
[03:44:38.935290] ====================
[03:44:38.935418] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:44:38.935438] ====================
[03:44:38.951026] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.3182 | reduced: 0.3165]  time: 3.1067  data: 2.8037  max mem: 5776
[03:44:39.084437] Test: [0] Total time: 0:00:37 (3.1179 s / it)
[03:44:39.084574] ***************************************************************
[03:44:39.084600] ****An extra tail dataset should exist for accracy metrics!****
[03:44:39.084617] ***************************************************************
[03:44:39.084635] **** Length of tail: 36 ****
[03:45:01.948969] Actual Batchsize/ world_size {'_n': 3.0}
[03:45:01.949107] {'pixel-level F1': tensor(1.0094, device='cuda:0', dtype=torch.float64)}
[03:45:01.970671] Test <remaining>: [0]  [0/3]  eta: 0:01:08  pixel-level F1: [local: 0.3221 | reduced: 0.3170]  time: 22.8854  data: 22.5701  max mem: 5776
[03:45:23.668934] Actual Batchsize/ world_size {'_n': 3.0}
[03:45:23.669057] {'pixel-level F1': tensor(0.6829, device='cuda:0', dtype=torch.float64)}
[03:45:23.690581] Test <remaining>: [0]  [1/3]  eta: 0:00:44  pixel-level F1: [local: 0.3182 | reduced: 0.3151]  time: 22.3025  data: 21.9872  max mem: 5776
[03:45:45.045051] Actual Batchsize/ world_size {'_n': 3.0}
[03:45:45.045285] {'pixel-level F1': tensor(1.2642, device='cuda:0', dtype=torch.float64)}
[03:45:45.066037] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.3221 | reduced: 0.3173]  time: 21.9932  data: 21.6777  max mem: 5776
[03:45:45.066177] Test <remaining>: [0] Total time: 0:01:05 (21.9938 s / it)
[03:45:45.066999] ---syncronized---
[03:45:45.067034] pixel-level F1 reduced_count 600
[03:45:45.067065] pixel-level F1 reduced_sum 212.69519905657222
[03:45:45.067101] ---syncronized done ---
[03:45:47.237563] Averaged stats: pixel-level F1: [local: 0.3221 | reduced: 0.3545]
[03:45:47.241255] 
[ROBUST TEST] ResolutionChangeWrapper param=0.75
[03:45:47.242338] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:45:48.446550] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.4679 | reduced: 0.4679]  time: 1.2000  data: 0.8900  max mem: 5776
[03:45:48.705154] ====================
[03:45:48.705225] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:45:48.705244] ====================
[03:45:48.724448] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.4484 | reduced: 0.4586]  time: 0.7388  data: 0.4450  max mem: 5776
[03:45:48.798898] Test: [0] Total time: 0:00:01 (0.7764 s / it)
[03:45:48.799006] ***************************************************************
[03:45:48.799026] ****An extra tail dataset should exist for accracy metrics!****
[03:45:48.799042] ***************************************************************
[03:45:48.799060] **** Length of tail: 43 ****
[03:45:49.323047] Actual Batchsize/ world_size {'_n': 3.0}
[03:45:49.323177] {'pixel-level F1': tensor(1.6499, device='cuda:0', dtype=torch.float64)}
[03:45:49.344636] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4679 | reduced: 0.4691]  time: 0.5452  data: 0.2325  max mem: 5776
[03:45:49.827488] Actual Batchsize/ world_size {'_n': 3.0}
[03:45:49.827603] {'pixel-level F1': tensor(1.4153, device='cuda:0', dtype=torch.float64)}
[03:45:49.849079] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.4679 | reduced: 0.4694]  time: 0.5246  data: 0.2120  max mem: 5776
[03:45:50.372356] Actual Batchsize/ world_size {'_n': 3.0}
[03:45:50.372476] {'pixel-level F1': tensor(1.1192, device='cuda:0', dtype=torch.float64)}
[03:45:50.393962] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.4679 | reduced: 0.4604]  time: 0.5313  data: 0.2187  max mem: 5776
[03:45:50.703058] ====================
[03:45:50.703149] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:45:50.703170] ====================
[03:45:50.703579] Actual Batchsize/ world_size {'_n': 1.75}
[03:45:50.703640] {'pixel-level F1': tensor(0.6970, device='cuda:0', dtype=torch.float64)}
[03:45:50.715148] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.4484 | reduced: 0.4572]  time: 0.4787  data: 0.1960  max mem: 5776
[03:45:50.715264] Test <remaining>: [0] Total time: 0:00:01 (0.4790 s / it)
[03:45:50.715839] ---syncronized---
[03:45:50.715872] pixel-level F1 reduced_count 135
[03:45:50.715902] pixel-level F1 reduced_sum 59.4900540054293
[03:45:50.715933] ---syncronized done ---
[03:45:53.318999] Averaged stats: pixel-level F1: [local: 0.4484 | reduced: 0.4407]
[03:45:53.322313] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:45:54.515651] Test: [0]  [ 0/20]  eta: 0:00:23  pixel-level F1: [local: 0.3731 | reduced: 0.3731]  time: 1.1864  data: 0.8752  max mem: 5776
[03:46:00.121828] ====================
[03:46:00.121917] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:46:00.121938] ====================
[03:46:00.123649] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.6869 | reduced: 0.6986]  time: 0.3397  data: 0.0438  max mem: 5776
[03:46:00.210640] Test: [0] Total time: 0:00:06 (0.3441 s / it)
[03:46:00.210761] ***************************************************************
[03:46:00.210783] ****An extra tail dataset should exist for accracy metrics!****
[03:46:00.210800] ***************************************************************
[03:46:00.210817] **** Length of tail: 8 ****
[03:46:00.509791] ====================
[03:46:00.509879] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:46:00.509900] ====================
[03:46:00.510326] Actual Batchsize/ world_size {'_n': 2.0}
[03:46:00.510390] {'pixel-level F1': tensor(1.8783, device='cuda:0', dtype=torch.float64)}
[03:46:00.523719] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7862 | reduced: 0.7007]  time: 0.3124  data: 0.0998  max mem: 5776
[03:46:00.523845] Test <remaining>: [0] Total time: 0:00:00 (0.3129 s / it)
[03:46:00.524447] ---syncronized---
[03:46:00.524485] pixel-level F1 reduced_count 928
[03:46:00.524517] pixel-level F1 reduced_sum 643.1485329743846
[03:46:00.524560] ---syncronized done ---
[03:46:00.879152] Averaged stats: pixel-level F1: [local: 0.7862 | reduced: 0.6930]
[03:46:00.880340] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:46:02.619850] Test: [0]  [0/4]  eta: 0:00:06  pixel-level F1: [local: 0.8223 | reduced: 0.8223]  time: 1.7352  data: 1.4253  max mem: 5776
[03:46:03.458544] ====================
[03:46:03.458654] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:46:03.458674] ====================
[03:46:03.473997] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8223 | reduced: 0.8507]  time: 0.6472  data: 0.3564  max mem: 5776
[03:46:03.549467] Test: [0] Total time: 0:00:02 (0.6663 s / it)
[03:46:03.549580] ***************************************************************
[03:46:03.549600] ****An extra tail dataset should exist for accracy metrics!****
[03:46:03.549616] ***************************************************************
[03:46:03.549633] **** Length of tail: 36 ****
[03:46:04.466966] Actual Batchsize/ world_size {'_n': 3.0}
[03:46:04.467103] {'pixel-level F1': tensor(2.9444, device='cuda:0', dtype=torch.float64)}
[03:46:04.488610] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8466 | reduced: 0.8589]  time: 0.9385  data: 0.6232  max mem: 5776
[03:46:05.370940] Actual Batchsize/ world_size {'_n': 3.0}
[03:46:05.371072] {'pixel-level F1': tensor(2.3025, device='cuda:0', dtype=torch.float64)}
[03:46:05.392589] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8223 | reduced: 0.8535]  time: 0.9210  data: 0.6057  max mem: 5776
[03:46:06.244960] Actual Batchsize/ world_size {'_n': 3.0}
[03:46:06.245076] {'pixel-level F1': tensor(2.7196, device='cuda:0', dtype=torch.float64)}
[03:46:06.266729] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8466 | reduced: 0.8564]  time: 0.9053  data: 0.5901  max mem: 5776
[03:46:06.266849] Test <remaining>: [0] Total time: 0:00:02 (0.9057 s / it)
[03:46:06.267341] ---syncronized---
[03:46:06.267377] pixel-level F1 reduced_count 216
[03:46:06.267410] pixel-level F1 reduced_sum 185.2724207675259
[03:46:06.267442] ---syncronized done ---
[03:46:08.553026] Averaged stats: pixel-level F1: [local: 0.8466 | reduced: 0.8577]
[03:46:08.556033] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:46:32.707101] Test: [0]  [ 0/12]  eta: 0:04:49  pixel-level F1: [local: 0.4142 | reduced: 0.4142]  time: 24.1452  data: 23.8344  max mem: 5776
[03:46:44.758473] ====================
[03:46:44.758581] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:46:44.758600] ====================
[03:46:44.774040] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.3260 | reduced: 0.3203]  time: 3.0176  data: 2.7154  max mem: 5776
[03:46:44.894389] Test: [0] Total time: 0:00:36 (3.0277 s / it)
[03:46:44.894511] ***************************************************************
[03:46:44.894532] ****An extra tail dataset should exist for accracy metrics!****
[03:46:44.894554] ***************************************************************
[03:46:44.894573] **** Length of tail: 36 ****
[03:47:06.348260] Actual Batchsize/ world_size {'_n': 3.0}
[03:47:06.348395] {'pixel-level F1': tensor(1.0205, device='cuda:0', dtype=torch.float64)}
[03:47:06.369771] Test <remaining>: [0]  [0/3]  eta: 0:01:04  pixel-level F1: [local: 0.3386 | reduced: 0.3208]  time: 21.4747  data: 21.1588  max mem: 5776
[03:47:27.449550] Actual Batchsize/ world_size {'_n': 3.0}
[03:47:27.449720] {'pixel-level F1': tensor(0.6813, device='cuda:0', dtype=torch.float64)}
[03:47:27.470509] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.3260 | reduced: 0.3189]  time: 21.2874  data: 20.9708  max mem: 5776
[03:47:47.841284] Actual Batchsize/ world_size {'_n': 3.0}
[03:47:47.841404] {'pixel-level F1': tensor(1.3766, device='cuda:0', dtype=torch.float64)}
[03:47:47.863026] Test <remaining>: [0]  [2/3]  eta: 0:00:20  pixel-level F1: [local: 0.3386 | reduced: 0.3217]  time: 20.9890  data: 20.6731  max mem: 5776
[03:47:47.863185] Test <remaining>: [0] Total time: 0:01:02 (20.9895 s / it)
[03:47:47.863896] ---syncronized---
[03:47:47.863930] pixel-level F1 reduced_count 600
[03:47:47.863961] pixel-level F1 reduced_sum 215.52620022948201
[03:47:47.863991] ---syncronized done ---
[03:47:50.007261] Averaged stats: pixel-level F1: [local: 0.3386 | reduced: 0.3592]
[03:47:50.010661] 
[ROBUST TEST] ResolutionChangeWrapper param=0.65
[03:47:50.011761] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:47:51.212155] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.4903 | reduced: 0.4903]  time: 1.1962  data: 0.8860  max mem: 5776
[03:47:51.472478] ====================
[03:47:51.472572] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:47:51.472593] ====================
[03:47:51.491719] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.4106 | reduced: 0.4522]  time: 0.7376  data: 0.4431  max mem: 5776
[03:47:51.568213] Test: [0] Total time: 0:00:01 (0.7763 s / it)
[03:47:51.568311] ***************************************************************
[03:47:51.568332] ****An extra tail dataset should exist for accracy metrics!****
[03:47:51.568349] ***************************************************************
[03:47:51.568367] **** Length of tail: 43 ****
[03:47:52.095611] Actual Batchsize/ world_size {'_n': 3.0}
[03:47:52.095737] {'pixel-level F1': tensor(1.6301, device='cuda:0', dtype=torch.float64)}
[03:47:52.117406] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4903 | reduced: 0.4627]  time: 0.5486  data: 0.2340  max mem: 5776
[03:47:52.607983] Actual Batchsize/ world_size {'_n': 3.0}
[03:47:52.608099] {'pixel-level F1': tensor(1.2646, device='cuda:0', dtype=torch.float64)}
[03:47:52.629759] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.4215 | reduced: 0.4584]  time: 0.5303  data: 0.2156  max mem: 5776
[03:47:53.155161] Actual Batchsize/ world_size {'_n': 3.0}
[03:47:53.155287] {'pixel-level F1': tensor(1.0589, device='cuda:0', dtype=torch.float64)}
[03:47:53.176857] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.4215 | reduced: 0.4485]  time: 0.5358  data: 0.2211  max mem: 5776
[03:47:53.487193] ====================
[03:47:53.487283] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:47:53.487302] ====================
[03:47:53.487731] Actual Batchsize/ world_size {'_n': 1.75}
[03:47:53.487791] {'pixel-level F1': tensor(0.5412, device='cuda:0', dtype=torch.float64)}
[03:47:53.499368] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.4106 | reduced: 0.4413]  time: 0.4824  data: 0.1978  max mem: 5776
[03:47:53.499494] Test <remaining>: [0] Total time: 0:00:01 (0.4828 s / it)
[03:47:53.500025] ---syncronized---
[03:47:53.500063] pixel-level F1 reduced_count 135
[03:47:53.500094] pixel-level F1 reduced_sum 56.23066899984326
[03:47:53.500126] ---syncronized done ---
[03:47:56.058714] Averaged stats: pixel-level F1: [local: 0.4106 | reduced: 0.4165]
[03:47:56.062054] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:47:57.126657] Test: [0]  [ 0/20]  eta: 0:00:21  pixel-level F1: [local: 0.3207 | reduced: 0.3207]  time: 1.0576  data: 0.7455  max mem: 5776
[03:48:02.732700] ====================
[03:48:02.732791] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:48:02.732810] ====================
[03:48:02.734484] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.6873 | reduced: 0.6382]  time: 0.3332  data: 0.0373  max mem: 5776
[03:48:02.821632] Test: [0] Total time: 0:00:06 (0.3377 s / it)
[03:48:02.821727] ***************************************************************
[03:48:02.821747] ****An extra tail dataset should exist for accracy metrics!****
[03:48:02.821764] ***************************************************************
[03:48:02.821781] **** Length of tail: 8 ****
[03:48:03.122985] ====================
[03:48:03.123063] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:48:03.123083] ====================
[03:48:03.123499] Actual Batchsize/ world_size {'_n': 2.0}
[03:48:03.123561] {'pixel-level F1': tensor(1.8694, device='cuda:0', dtype=torch.float64)}
[03:48:03.136862] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7330 | reduced: 0.6408]  time: 0.3146  data: 0.1019  max mem: 5776
[03:48:03.136978] Test <remaining>: [0] Total time: 0:00:00 (0.3150 s / it)
[03:48:03.137576] ---syncronized---
[03:48:03.137609] pixel-level F1 reduced_count 928
[03:48:03.137638] pixel-level F1 reduced_sum 604.3684813693988
[03:48:03.137669] ---syncronized done ---
[03:48:03.475891] Averaged stats: pixel-level F1: [local: 0.7330 | reduced: 0.6513]
[03:48:03.477067] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:48:05.208044] Test: [0]  [0/4]  eta: 0:00:06  pixel-level F1: [local: 0.8623 | reduced: 0.8623]  time: 1.7266  data: 1.4167  max mem: 5776
[03:48:06.046977] ====================
[03:48:06.047086] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:48:06.047106] ====================
[03:48:06.062396] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8456 | reduced: 0.8577]  time: 0.6451  data: 0.3542  max mem: 5776
[03:48:06.136575] Test: [0] Total time: 0:00:02 (0.6639 s / it)
[03:48:06.136698] ***************************************************************
[03:48:06.136720] ****An extra tail dataset should exist for accracy metrics!****
[03:48:06.136736] ***************************************************************
[03:48:06.136753] **** Length of tail: 36 ****
[03:48:07.063543] Actual Batchsize/ world_size {'_n': 3.0}
[03:48:07.063672] {'pixel-level F1': tensor(2.9370, device='cuda:0', dtype=torch.float64)}
[03:48:07.085247] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8623 | reduced: 0.8653]  time: 0.9480  data: 0.6325  max mem: 5776
[03:48:07.987701] Actual Batchsize/ world_size {'_n': 3.0}
[03:48:07.987827] {'pixel-level F1': tensor(2.2843, device='cuda:0', dtype=torch.float64)}
[03:48:08.009413] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8456 | reduced: 0.8592]  time: 0.9359  data: 0.6202  max mem: 5776
[03:48:08.853289] Actual Batchsize/ world_size {'_n': 3.0}
[03:48:08.853406] {'pixel-level F1': tensor(2.7155, device='cuda:0', dtype=torch.float64)}
[03:48:08.875085] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8623 | reduced: 0.8618]  time: 0.9124  data: 0.5969  max mem: 5776
[03:48:08.875216] Test <remaining>: [0] Total time: 0:00:02 (0.9128 s / it)
[03:48:08.875703] ---syncronized---
[03:48:08.875735] pixel-level F1 reduced_count 216
[03:48:08.875766] pixel-level F1 reduced_sum 183.80095929641004
[03:48:08.875797] ---syncronized done ---
[03:48:11.160669] Averaged stats: pixel-level F1: [local: 0.8623 | reduced: 0.8509]
[03:48:11.163738] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:48:35.943999] Test: [0]  [ 0/12]  eta: 0:04:57  pixel-level F1: [local: 0.4237 | reduced: 0.4237]  time: 24.7743  data: 24.4634  max mem: 5776
[03:48:46.920988] ====================
[03:48:46.921099] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:48:46.921120] ====================
[03:48:46.936531] Test: [0]  [11/12]  eta: 0:00:02  pixel-level F1: [local: 0.3190 | reduced: 0.3265]  time: 2.9805  data: 2.6783  max mem: 5776
[03:48:47.065684] Test: [0] Total time: 0:00:35 (2.9914 s / it)
[03:48:47.065790] ***************************************************************
[03:48:47.065809] ****An extra tail dataset should exist for accracy metrics!****
[03:48:47.065826] ***************************************************************
[03:48:47.065845] **** Length of tail: 36 ****
[03:49:08.315732] Actual Batchsize/ world_size {'_n': 3.0}
[03:49:08.315878] {'pixel-level F1': tensor(0.9913, device='cuda:0', dtype=torch.float64)}
[03:49:08.337180] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.3304 | reduced: 0.3266]  time: 21.2708  data: 20.9548  max mem: 5776
[03:49:29.510653] Actual Batchsize/ world_size {'_n': 3.0}
[03:49:29.510782] {'pixel-level F1': tensor(0.7190, device='cuda:0', dtype=torch.float64)}
[03:49:29.532323] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.3190 | reduced: 0.3248]  time: 21.2328  data: 20.9172  max mem: 5776
[03:49:49.948289] Actual Batchsize/ world_size {'_n': 3.0}
[03:49:49.948417] {'pixel-level F1': tensor(1.4603, device='cuda:0', dtype=torch.float64)}
[03:49:49.970015] Test <remaining>: [0]  [2/3]  eta: 0:00:20  pixel-level F1: [local: 0.3304 | reduced: 0.3280]  time: 20.9676  data: 20.6522  max mem: 5776
[03:49:49.970175] Test <remaining>: [0] Total time: 0:01:02 (20.9681 s / it)
[03:49:49.970794] ---syncronized---
[03:49:49.970829] pixel-level F1 reduced_count 600
[03:49:49.970860] pixel-level F1 reduced_sum 214.80297236835966
[03:49:49.970892] ---syncronized done ---
[03:49:52.128625] Averaged stats: pixel-level F1: [local: 0.3304 | reduced: 0.3580]
[03:49:52.131917] 
[ROBUST TEST] ResolutionChangeWrapper param=0.55
[03:49:52.132838] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:49:53.357835] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.3987 | reduced: 0.3987]  time: 1.2210  data: 0.9109  max mem: 5776
[03:49:53.617578] ====================
[03:49:53.617648] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:49:53.617668] ====================
[03:49:53.636993] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.3987 | reduced: 0.4172]  time: 0.7499  data: 0.4555  max mem: 5776
[03:49:53.710122] Test: [0] Total time: 0:00:01 (0.7869 s / it)
[03:49:53.710217] ***************************************************************
[03:49:53.710237] ****An extra tail dataset should exist for accracy metrics!****
[03:49:53.710253] ***************************************************************
[03:49:53.710270] **** Length of tail: 43 ****
[03:49:54.237121] Actual Batchsize/ world_size {'_n': 3.0}
[03:49:54.237239] {'pixel-level F1': tensor(1.5178, device='cuda:0', dtype=torch.float64)}
[03:49:54.258913] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4374 | reduced: 0.4274]  time: 0.5482  data: 0.2338  max mem: 5776
[03:49:54.744879] Actual Batchsize/ world_size {'_n': 3.0}
[03:49:54.745001] {'pixel-level F1': tensor(1.2493, device='cuda:0', dtype=torch.float64)}
[03:49:54.766723] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.4164 | reduced: 0.4263]  time: 0.5278  data: 0.2134  max mem: 5776
[03:49:55.292720] Actual Batchsize/ world_size {'_n': 3.0}
[03:49:55.292838] {'pixel-level F1': tensor(1.0978, device='cuda:0', dtype=torch.float64)}
[03:49:55.314420] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.4164 | reduced: 0.4206]  time: 0.5343  data: 0.2199  max mem: 5776
[03:49:55.622979] ====================
[03:49:55.623074] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:49:55.623095] ====================
[03:49:55.623528] Actual Batchsize/ world_size {'_n': 1.75}
[03:49:55.623585] {'pixel-level F1': tensor(0.6741, device='cuda:0', dtype=torch.float64)}
[03:49:55.635125] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.3987 | reduced: 0.4188]  time: 0.4808  data: 0.1965  max mem: 5776
[03:49:55.635246] Test <remaining>: [0] Total time: 0:00:01 (0.4812 s / it)
[03:49:55.635844] ---syncronized---
[03:49:55.635878] pixel-level F1 reduced_count 135
[03:49:55.635908] pixel-level F1 reduced_sum 54.47426321170809
[03:49:55.635938] ---syncronized done ---
[03:49:58.210521] Averaged stats: pixel-level F1: [local: 0.3987 | reduced: 0.4035]
[03:49:58.214051] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:49:59.452854] Test: [0]  [ 0/20]  eta: 0:00:24  pixel-level F1: [local: 0.2901 | reduced: 0.2901]  time: 1.2319  data: 0.9198  max mem: 5776
[03:50:05.057670] ====================
[03:50:05.057760] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:50:05.057779] ====================
[03:50:05.059482] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7053 | reduced: 0.6212]  time: 0.3419  data: 0.0461  max mem: 5776
[03:50:05.142758] Test: [0] Total time: 0:00:06 (0.3461 s / it)
[03:50:05.142870] ***************************************************************
[03:50:05.142891] ****An extra tail dataset should exist for accracy metrics!****
[03:50:05.142908] ***************************************************************
[03:50:05.142925] **** Length of tail: 8 ****
[03:50:05.441352] ====================
[03:50:05.441432] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:50:05.441451] ====================
[03:50:05.441871] Actual Batchsize/ world_size {'_n': 2.0}
[03:50:05.441931] {'pixel-level F1': tensor(1.8527, device='cuda:0', dtype=torch.float64)}
[03:50:05.455324] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7145 | reduced: 0.6239]  time: 0.3119  data: 0.0994  max mem: 5776
[03:50:05.455442] Test <remaining>: [0] Total time: 0:00:00 (0.3124 s / it)
[03:50:05.455973] ---syncronized---
[03:50:05.456005] pixel-level F1 reduced_count 928
[03:50:05.456035] pixel-level F1 reduced_sum 580.1222542751684
[03:50:05.456067] ---syncronized done ---
[03:50:05.777482] Averaged stats: pixel-level F1: [local: 0.7145 | reduced: 0.6251]
[03:50:05.778656] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:50:07.492805] Test: [0]  [0/4]  eta: 0:00:06  pixel-level F1: [local: 0.8537 | reduced: 0.8537]  time: 1.7098  data: 1.3998  max mem: 5776
[03:50:08.331464] ====================
[03:50:08.331570] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:50:08.331590] ====================
[03:50:08.346965] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8537 | reduced: 0.8683]  time: 0.6409  data: 0.3500  max mem: 5776
[03:50:08.420502] Test: [0] Total time: 0:00:02 (0.6595 s / it)
[03:50:08.420641] ***************************************************************
[03:50:08.420663] ****An extra tail dataset should exist for accracy metrics!****
[03:50:08.420681] ***************************************************************
[03:50:08.420699] **** Length of tail: 36 ****
[03:50:09.339698] Actual Batchsize/ world_size {'_n': 3.0}
[03:50:09.339830] {'pixel-level F1': tensor(2.9429, device='cuda:0', dtype=torch.float64)}
[03:50:09.361343] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8841 | reduced: 0.8753]  time: 0.9402  data: 0.6248  max mem: 5776
[03:50:10.250637] Actual Batchsize/ world_size {'_n': 3.0}
[03:50:10.250766] {'pixel-level F1': tensor(2.4292, device='cuda:0', dtype=torch.float64)}
[03:50:10.272251] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8537 | reduced: 0.8714]  time: 0.9253  data: 0.6100  max mem: 5776
[03:50:11.113141] Actual Batchsize/ world_size {'_n': 3.0}
[03:50:11.113259] {'pixel-level F1': tensor(2.7191, device='cuda:0', dtype=torch.float64)}
[03:50:11.134877] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8841 | reduced: 0.8734]  time: 0.9043  data: 0.5891  max mem: 5776
[03:50:11.135007] Test <remaining>: [0] Total time: 0:00:02 (0.9047 s / it)
[03:50:11.135519] ---syncronized---
[03:50:11.135550] pixel-level F1 reduced_count 216
[03:50:11.135581] pixel-level F1 reduced_sum 185.5878280924722
[03:50:11.135613] ---syncronized done ---
[03:50:13.385140] Averaged stats: pixel-level F1: [local: 0.8841 | reduced: 0.8592]
[03:50:13.388147] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:50:38.521856] Test: [0]  [ 0/12]  eta: 0:05:01  pixel-level F1: [local: 0.3863 | reduced: 0.3863]  time: 25.1277  data: 24.8166  max mem: 5776
[03:50:49.340489] ====================
[03:50:49.340622] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:50:49.340642] ====================
[03:50:49.356089] Test: [0]  [11/12]  eta: 0:00:02  pixel-level F1: [local: 0.2946 | reduced: 0.3101]  time: 2.9968  data: 2.6944  max mem: 5776
[03:50:49.503505] Test: [0] Total time: 0:00:36 (3.0092 s / it)
[03:50:49.503635] ***************************************************************
[03:50:49.503658] ****An extra tail dataset should exist for accracy metrics!****
[03:50:49.503677] ***************************************************************
[03:50:49.503696] **** Length of tail: 36 ****
[03:51:10.723470] Actual Batchsize/ world_size {'_n': 3.0}
[03:51:10.723607] {'pixel-level F1': tensor(0.9722, device='cuda:0', dtype=torch.float64)}
[03:51:10.745036] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.3155 | reduced: 0.3104]  time: 21.2408  data: 20.9251  max mem: 5776
[03:51:32.218143] Actual Batchsize/ world_size {'_n': 3.0}
[03:51:32.218266] {'pixel-level F1': tensor(0.6722, device='cuda:0', dtype=torch.float64)}
[03:51:32.239902] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.2946 | reduced: 0.3086]  time: 21.3677  data: 21.0523  max mem: 5776
[03:51:52.730078] Actual Batchsize/ world_size {'_n': 3.0}
[03:51:52.730203] {'pixel-level F1': tensor(1.4518, device='cuda:0', dtype=torch.float64)}
[03:51:52.751552] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.3155 | reduced: 0.3121]  time: 21.0822  data: 20.7668  max mem: 5776
[03:51:52.751692] Test <remaining>: [0] Total time: 0:01:03 (21.0826 s / it)
[03:51:52.752366] ---syncronized---
[03:51:52.752405] pixel-level F1 reduced_count 600
[03:51:52.752437] pixel-level F1 reduced_sum 213.59160241034334
[03:51:52.752469] ---syncronized done ---
[03:51:54.952838] Averaged stats: pixel-level F1: [local: 0.3155 | reduced: 0.3560]
[03:51:54.956166] 
[ROBUST TEST] ResolutionChangeWrapper param=0.45
[03:51:54.957105] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:51:56.232079] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.4459 | reduced: 0.4459]  time: 1.2709  data: 0.9609  max mem: 5776
[03:51:56.491960] ====================
[03:51:56.492032] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:51:56.492052] ====================
[03:51:56.511397] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.3929 | reduced: 0.4206]  time: 0.7749  data: 0.4805  max mem: 5776
[03:51:56.589735] Test: [0] Total time: 0:00:01 (0.8145 s / it)
[03:51:56.589838] ***************************************************************
[03:51:56.589857] ****An extra tail dataset should exist for accracy metrics!****
[03:51:56.589873] ***************************************************************
[03:51:56.589890] **** Length of tail: 43 ****
[03:51:57.116545] Actual Batchsize/ world_size {'_n': 3.0}
[03:51:57.116670] {'pixel-level F1': tensor(1.8460, device='cuda:0', dtype=torch.float64)}
[03:51:57.138314] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4459 | reduced: 0.4430]  time: 0.5480  data: 0.2336  max mem: 5776
[03:51:57.625339] Actual Batchsize/ world_size {'_n': 3.0}
[03:51:57.625456] {'pixel-level F1': tensor(0.7971, device='cuda:0', dtype=torch.float64)}
[03:51:57.647158] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.3929 | reduced: 0.4247]  time: 0.5282  data: 0.2139  max mem: 5776
[03:51:58.175084] Actual Batchsize/ world_size {'_n': 3.0}
[03:51:58.175210] {'pixel-level F1': tensor(1.0364, device='cuda:0', dtype=torch.float64)}
[03:51:58.196786] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.3929 | reduced: 0.4173]  time: 0.5352  data: 0.2207  max mem: 5776
[03:51:58.505975] ====================
[03:51:58.506061] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:51:58.506082] ====================
[03:51:58.506498] Actual Batchsize/ world_size {'_n': 1.75}
[03:51:58.506559] {'pixel-level F1': tensor(0.8187, device='cuda:0', dtype=torch.float64)}
[03:51:58.518138] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.3929 | reduced: 0.4199]  time: 0.4817  data: 0.1972  max mem: 5776
[03:51:58.518281] Test <remaining>: [0] Total time: 0:00:01 (0.4821 s / it)
[03:51:58.518890] ---syncronized---
[03:51:58.518923] pixel-level F1 reduced_count 135
[03:51:58.518953] pixel-level F1 reduced_sum 54.8879051177664
[03:51:58.518984] ---syncronized done ---
[03:52:00.846687] Averaged stats: pixel-level F1: [local: 0.3929 | reduced: 0.4066]
[03:52:00.850182] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:52:01.971382] Test: [0]  [ 0/20]  eta: 0:00:22  pixel-level F1: [local: 0.2331 | reduced: 0.2331]  time: 1.1139  data: 0.8028  max mem: 5776
[03:52:07.577652] ====================
[03:52:07.577739] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:52:07.577758] ====================
[03:52:07.579436] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.5956 | reduced: 0.5861]  time: 0.3361  data: 0.0402  max mem: 5776
[03:52:07.665135] Test: [0] Total time: 0:00:06 (0.3404 s / it)
[03:52:07.665234] ***************************************************************
[03:52:07.665255] ****An extra tail dataset should exist for accracy metrics!****
[03:52:07.665272] ***************************************************************
[03:52:07.665289] **** Length of tail: 8 ****
[03:52:07.964889] ====================
[03:52:07.964968] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:52:07.964988] ====================
[03:52:07.965404] Actual Batchsize/ world_size {'_n': 2.0}
[03:52:07.965468] {'pixel-level F1': tensor(1.8468, device='cuda:0', dtype=torch.float64)}
[03:52:07.978882] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7192 | reduced: 0.5890]  time: 0.3131  data: 0.1005  max mem: 5776
[03:52:07.979001] Test <remaining>: [0] Total time: 0:00:00 (0.3136 s / it)
[03:52:07.979561] ---syncronized---
[03:52:07.979595] pixel-level F1 reduced_count 928
[03:52:07.979626] pixel-level F1 reduced_sum 555.504544418258
[03:52:07.979658] ---syncronized done ---
[03:52:08.257679] Averaged stats: pixel-level F1: [local: 0.7192 | reduced: 0.5986]
[03:52:08.258806] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:52:09.983065] Test: [0]  [0/4]  eta: 0:00:06  pixel-level F1: [local: 0.8346 | reduced: 0.8346]  time: 1.7199  data: 1.4101  max mem: 5776
[03:52:10.821395] ====================
[03:52:10.821494] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:52:10.821514] ====================
[03:52:10.836911] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8346 | reduced: 0.8537]  time: 0.6433  data: 0.3526  max mem: 5776
[03:52:10.911482] Test: [0] Total time: 0:00:02 (0.6622 s / it)
[03:52:10.911587] ***************************************************************
[03:52:10.911609] ****An extra tail dataset should exist for accracy metrics!****
[03:52:10.911625] ***************************************************************
[03:52:10.911645] **** Length of tail: 36 ****
[03:52:11.837781] Actual Batchsize/ world_size {'_n': 3.0}
[03:52:11.837918] {'pixel-level F1': tensor(2.9432, device='cuda:0', dtype=torch.float64)}
[03:52:11.859378] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8588 | reduced: 0.8617]  time: 0.9473  data: 0.6318  max mem: 5776
[03:52:12.748702] Actual Batchsize/ world_size {'_n': 3.0}
[03:52:12.748833] {'pixel-level F1': tensor(2.3607, device='cuda:0', dtype=torch.float64)}
[03:52:12.770169] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8346 | reduced: 0.8573]  time: 0.9288  data: 0.6134  max mem: 5776
[03:52:13.615980] Actual Batchsize/ world_size {'_n': 3.0}
[03:52:13.616108] {'pixel-level F1': tensor(2.7150, device='cuda:0', dtype=torch.float64)}
[03:52:13.637755] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8588 | reduced: 0.8599]  time: 0.9083  data: 0.5931  max mem: 5776
[03:52:13.637874] Test <remaining>: [0] Total time: 0:00:02 (0.9087 s / it)
[03:52:13.638352] ---syncronized---
[03:52:13.638385] pixel-level F1 reduced_count 216
[03:52:13.638415] pixel-level F1 reduced_sum 183.07197471065695
[03:52:13.638447] ---syncronized done ---
[03:52:15.774388] Averaged stats: pixel-level F1: [local: 0.8588 | reduced: 0.8476]
[03:52:15.777389] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:52:40.324858] Test: [0]  [ 0/12]  eta: 0:04:54  pixel-level F1: [local: 0.3696 | reduced: 0.3696]  time: 24.5418  data: 24.2310  max mem: 5776
[03:52:51.066116] ====================
[03:52:51.066214] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:52:51.066234] ====================
[03:52:51.081754] Test: [0]  [11/12]  eta: 0:00:02  pixel-level F1: [local: 0.3057 | reduced: 0.3147]  time: 2.9415  data: 2.6392  max mem: 5776
[03:52:51.237119] Test: [0] Total time: 0:00:35 (2.9545 s / it)
[03:52:51.237227] ***************************************************************
[03:52:51.237248] ****An extra tail dataset should exist for accracy metrics!****
[03:52:51.237264] ***************************************************************
[03:52:51.237283] **** Length of tail: 36 ****
[03:53:12.366488] Actual Batchsize/ world_size {'_n': 3.0}
[03:53:12.366616] {'pixel-level F1': tensor(1.0081, device='cuda:0', dtype=torch.float64)}
[03:53:12.388237] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.3124 | reduced: 0.3151]  time: 21.1505  data: 20.8354  max mem: 5776
[03:53:33.467173] Actual Batchsize/ world_size {'_n': 3.0}
[03:53:33.467310] {'pixel-level F1': tensor(0.6716, device='cuda:0', dtype=torch.float64)}
[03:53:33.488903] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.3057 | reduced: 0.3133]  time: 21.1254  data: 20.8103  max mem: 5776
[03:53:53.790527] Actual Batchsize/ world_size {'_n': 3.0}
[03:53:53.790695] {'pixel-level F1': tensor(1.4260, device='cuda:0', dtype=torch.float64)}
[03:53:53.812035] Test <remaining>: [0]  [2/3]  eta: 0:00:20  pixel-level F1: [local: 0.3124 | reduced: 0.3165]  time: 20.8578  data: 20.5426  max mem: 5776
[03:53:53.812175] Test <remaining>: [0] Total time: 0:01:02 (20.8582 s / it)
[03:53:53.812883] ---syncronized---
[03:53:53.812925] pixel-level F1 reduced_count 600
[03:53:53.812955] pixel-level F1 reduced_sum 213.24567908251112
[03:53:53.812985] ---syncronized done ---
[03:53:55.940904] Averaged stats: pixel-level F1: [local: 0.3124 | reduced: 0.3554]
[03:53:55.944291] 
[ROBUST TEST] ResolutionChangeWrapper param=0.35
[03:53:55.945416] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:53:57.201019] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.3894 | reduced: 0.3894]  time: 1.2514  data: 0.9411  max mem: 5776
[03:53:57.461051] ====================
[03:53:57.461123] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:53:57.461143] ====================
[03:53:57.480387] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.3894 | reduced: 0.4234]  time: 0.7652  data: 0.4706  max mem: 5776
[03:53:57.554897] Test: [0] Total time: 0:00:01 (0.8029 s / it)
[03:53:57.554990] ***************************************************************
[03:53:57.555011] ****An extra tail dataset should exist for accracy metrics!****
[03:53:57.555028] ***************************************************************
[03:53:57.555046] **** Length of tail: 43 ****
[03:53:58.084783] Actual Batchsize/ world_size {'_n': 3.0}
[03:53:58.084906] {'pixel-level F1': tensor(1.3298, device='cuda:0', dtype=torch.float64)}
[03:53:58.106535] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4433 | reduced: 0.4257]  time: 0.5511  data: 0.2367  max mem: 5776
[03:53:58.593543] Actual Batchsize/ world_size {'_n': 3.0}
[03:53:58.593656] {'pixel-level F1': tensor(1.1321, device='cuda:0', dtype=torch.float64)}
[03:53:58.615336] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.3894 | reduced: 0.4207]  time: 0.5298  data: 0.2152  max mem: 5776
[03:53:59.141355] Actual Batchsize/ world_size {'_n': 3.0}
[03:53:59.141468] {'pixel-level F1': tensor(0.8106, device='cuda:0', dtype=torch.float64)}
[03:53:59.163139] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.3894 | reduced: 0.4066]  time: 0.5357  data: 0.2211  max mem: 5776
[03:53:59.473289] ====================
[03:53:59.473377] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:53:59.473397] ====================
[03:53:59.473822] Actual Batchsize/ world_size {'_n': 1.75}
[03:53:59.473881] {'pixel-level F1': tensor(0.8313, device='cuda:0', dtype=torch.float64)}
[03:53:59.485496] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.3894 | reduced: 0.4101]  time: 0.4822  data: 0.1978  max mem: 5776
[03:53:59.485627] Test <remaining>: [0] Total time: 0:00:01 (0.4826 s / it)
[03:53:59.486221] ---syncronized---
[03:53:59.486253] pixel-level F1 reduced_count 135
[03:53:59.486283] pixel-level F1 reduced_sum 52.72513627429526
[03:53:59.486314] ---syncronized done ---
[03:54:01.426326] Averaged stats: pixel-level F1: [local: 0.3894 | reduced: 0.3906]
[03:54:01.429745] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:54:02.559429] Test: [0]  [ 0/20]  eta: 0:00:22  pixel-level F1: [local: 0.1030 | reduced: 0.1030]  time: 1.1224  data: 0.8113  max mem: 5776
[03:54:08.165760] ====================
[03:54:08.165850] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:54:08.165870] ====================
[03:54:08.167585] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.4654 | reduced: 0.4992]  time: 0.3365  data: 0.0406  max mem: 5776
[03:54:08.249173] Test: [0] Total time: 0:00:06 (0.3406 s / it)
[03:54:08.249279] ***************************************************************
[03:54:08.249299] ****An extra tail dataset should exist for accracy metrics!****
[03:54:08.249315] ***************************************************************
[03:54:08.249332] **** Length of tail: 8 ****
[03:54:08.552876] ====================
[03:54:08.552958] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:54:08.552979] ====================
[03:54:08.553403] Actual Batchsize/ world_size {'_n': 2.0}
[03:54:08.553466] {'pixel-level F1': tensor(1.7892, device='cuda:0', dtype=torch.float64)}
[03:54:08.566854] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.6018 | reduced: 0.5026]  time: 0.3170  data: 0.1045  max mem: 5776
[03:54:08.566969] Test <remaining>: [0] Total time: 0:00:00 (0.3175 s / it)
[03:54:08.567454] ---syncronized---
[03:54:08.567485] pixel-level F1 reduced_count 928
[03:54:08.567515] pixel-level F1 reduced_sum 498.14099251823166
[03:54:08.567546] ---syncronized done ---
[03:54:08.783429] Averaged stats: pixel-level F1: [local: 0.6018 | reduced: 0.5368]
[03:54:08.784560] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:54:10.517202] Test: [0]  [0/4]  eta: 0:00:06  pixel-level F1: [local: 0.8244 | reduced: 0.8244]  time: 1.7283  data: 1.4185  max mem: 5776
[03:54:11.355221] ====================
[03:54:11.355334] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:54:11.355364] ====================
[03:54:11.370743] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8244 | reduced: 0.8476]  time: 0.6453  data: 0.3547  max mem: 5776
[03:54:11.447677] Test: [0] Total time: 0:00:02 (0.6648 s / it)
[03:54:11.447798] ***************************************************************
[03:54:11.447818] ****An extra tail dataset should exist for accracy metrics!****
[03:54:11.447834] ***************************************************************
[03:54:11.447852] **** Length of tail: 36 ****
[03:54:12.384816] Actual Batchsize/ world_size {'_n': 3.0}
[03:54:12.384954] {'pixel-level F1': tensor(2.9361, device='cuda:0', dtype=torch.float64)}
[03:54:12.406590] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8521 | reduced: 0.8558]  time: 0.9582  data: 0.6426  max mem: 5776
[03:54:13.302852] Actual Batchsize/ world_size {'_n': 3.0}
[03:54:13.302988] {'pixel-level F1': tensor(2.3795, device='cuda:0', dtype=torch.float64)}
[03:54:13.324419] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8244 | reduced: 0.8521]  time: 0.9378  data: 0.6221  max mem: 5776
[03:54:14.176618] Actual Batchsize/ world_size {'_n': 3.0}
[03:54:14.176735] {'pixel-level F1': tensor(2.7017, device='cuda:0', dtype=torch.float64)}
[03:54:14.198444] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8521 | reduced: 0.8548]  time: 0.9164  data: 0.6009  max mem: 5776
[03:54:14.198571] Test <remaining>: [0] Total time: 0:00:02 (0.9169 s / it)
[03:54:14.199055] ---syncronized---
[03:54:14.199086] pixel-level F1 reduced_count 216
[03:54:14.199116] pixel-level F1 reduced_sum 179.93421926870124
[03:54:14.199146] ---syncronized done ---
[03:54:16.055577] Averaged stats: pixel-level F1: [local: 0.8521 | reduced: 0.8330]
[03:54:16.058592] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:54:41.032978] Test: [0]  [ 0/12]  eta: 0:04:59  pixel-level F1: [local: 0.3406 | reduced: 0.3406]  time: 24.9686  data: 24.6581  max mem: 5776
[03:54:51.415604] ====================
[03:54:51.415701] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:54:51.415721] ====================
[03:54:51.431298] Test: [0]  [11/12]  eta: 0:00:02  pixel-level F1: [local: 0.3055 | reduced: 0.3069]  time: 2.9472  data: 2.6451  max mem: 5776
[03:54:51.559798] Test: [0] Total time: 0:00:35 (2.9580 s / it)
[03:54:51.559891] ***************************************************************
[03:54:51.559911] ****An extra tail dataset should exist for accracy metrics!****
[03:54:51.559927] ***************************************************************
[03:54:51.559944] **** Length of tail: 36 ****
[03:55:12.422934] Actual Batchsize/ world_size {'_n': 3.0}
[03:55:12.423078] {'pixel-level F1': tensor(1.1061, device='cuda:0', dtype=torch.float64)}
[03:55:12.444654] Test <remaining>: [0]  [0/3]  eta: 0:01:02  pixel-level F1: [local: 0.3122 | reduced: 0.3082]  time: 20.8842  data: 20.5690  max mem: 5776
[03:55:33.508127] Actual Batchsize/ world_size {'_n': 3.0}
[03:55:33.508258] {'pixel-level F1': tensor(0.6563, device='cuda:0', dtype=torch.float64)}
[03:55:33.529971] Test <remaining>: [0]  [1/3]  eta: 0:00:41  pixel-level F1: [local: 0.3055 | reduced: 0.3064]  time: 20.9846  data: 20.6699  max mem: 5776
[03:55:53.803056] Actual Batchsize/ world_size {'_n': 3.0}
[03:55:53.803178] {'pixel-level F1': tensor(1.4766, device='cuda:0', dtype=torch.float64)}
[03:55:53.824899] Test <remaining>: [0]  [2/3]  eta: 0:00:20  pixel-level F1: [local: 0.3122 | reduced: 0.3101]  time: 20.7546  data: 20.4399  max mem: 5776
[03:55:53.825031] Test <remaining>: [0] Total time: 0:01:02 (20.7550 s / it)
[03:55:53.825533] ---syncronized---
[03:55:53.825567] pixel-level F1 reduced_count 600
[03:55:53.825598] pixel-level F1 reduced_sum 212.5767609114675
[03:55:53.825628] ---syncronized done ---
[03:55:55.846093] Averaged stats: pixel-level F1: [local: 0.3122 | reduced: 0.3543]
[03:55:55.849365] 
[ROBUST TEST] ResolutionChangeWrapper param=0.25
[03:55:55.850289] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:55:57.104500] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.4084 | reduced: 0.4084]  time: 1.2502  data: 0.9399  max mem: 5776
[03:55:57.364722] ====================
[03:55:57.364793] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:55:57.364813] ====================
[03:55:57.384039] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.4084 | reduced: 0.4165]  time: 0.7646  data: 0.4700  max mem: 5776
[03:55:57.460194] Test: [0] Total time: 0:00:01 (0.8032 s / it)
[03:55:57.460303] ***************************************************************
[03:55:57.460322] ****An extra tail dataset should exist for accracy metrics!****
[03:55:57.460338] ***************************************************************
[03:55:57.460356] **** Length of tail: 43 ****
[03:55:57.986054] Actual Batchsize/ world_size {'_n': 3.0}
[03:55:57.986178] {'pixel-level F1': tensor(1.0161, device='cuda:0', dtype=torch.float64)}
[03:55:58.007924] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.4084 | reduced: 0.4076]  time: 0.5472  data: 0.2327  max mem: 5776
[03:55:58.496554] Actual Batchsize/ world_size {'_n': 3.0}
[03:55:58.496677] {'pixel-level F1': tensor(0.9320, device='cuda:0', dtype=torch.float64)}
[03:55:58.518325] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.3387 | reduced: 0.3975]  time: 0.5286  data: 0.2142  max mem: 5776
[03:55:59.042510] Actual Batchsize/ world_size {'_n': 3.0}
[03:55:59.042628] {'pixel-level F1': tensor(0.5512, device='cuda:0', dtype=torch.float64)}
[03:55:59.064378] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.3387 | reduced: 0.3775]  time: 0.5343  data: 0.2199  max mem: 5776
[03:55:59.372389] ====================
[03:55:59.372478] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:55:59.372498] ====================
[03:55:59.372924] Actual Batchsize/ world_size {'_n': 1.75}
[03:55:59.372986] {'pixel-level F1': tensor(0.5679, device='cuda:0', dtype=torch.float64)}
[03:55:59.384586] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.3245 | reduced: 0.3747]  time: 0.4807  data: 0.1964  max mem: 5776
[03:55:59.384709] Test <remaining>: [0] Total time: 0:00:01 (0.4811 s / it)
[03:55:59.385355] ---syncronized---
[03:55:59.385389] pixel-level F1 reduced_count 135
[03:55:59.385420] pixel-level F1 reduced_sum 39.059026500472285
[03:55:59.385452] ---syncronized done ---
[03:56:00.974168] Averaged stats: pixel-level F1: [local: 0.3245 | reduced: 0.2893]
[03:56:00.977497] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:56:02.070688] Test: [0]  [ 0/20]  eta: 0:00:21  pixel-level F1: [local: 0.0636 | reduced: 0.0636]  time: 1.0862  data: 0.7749  max mem: 5776
[03:56:07.677406] ====================
[03:56:07.677496] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:56:07.677515] ====================
[03:56:07.679223] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.2639 | reduced: 0.3822]  time: 0.3347  data: 0.0388  max mem: 5776
[03:56:07.766712] Test: [0] Total time: 0:00:06 (0.3391 s / it)
[03:56:07.766826] ***************************************************************
[03:56:07.766847] ****An extra tail dataset should exist for accracy metrics!****
[03:56:07.766865] ***************************************************************
[03:56:07.766883] **** Length of tail: 8 ****
[03:56:08.063904] ====================
[03:56:08.063986] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:56:08.064006] ====================
[03:56:08.064414] Actual Batchsize/ world_size {'_n': 2.0}
[03:56:08.064473] {'pixel-level F1': tensor(1.6669, device='cuda:0', dtype=torch.float64)}
[03:56:08.077876] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.5094 | reduced: 0.3861]  time: 0.3105  data: 0.0982  max mem: 5776
[03:56:08.077998] Test <remaining>: [0] Total time: 0:00:00 (0.3110 s / it)
[03:56:08.078636] ---syncronized---
[03:56:08.078672] pixel-level F1 reduced_count 928
[03:56:08.078702] pixel-level F1 reduced_sum 381.581582244812
[03:56:08.078734] ---syncronized done ---
[03:56:08.267612] Averaged stats: pixel-level F1: [local: 0.5094 | reduced: 0.4112]
[03:56:08.268714] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:56:09.990253] Test: [0]  [0/4]  eta: 0:00:06  pixel-level F1: [local: 0.8637 | reduced: 0.8637]  time: 1.7173  data: 1.4072  max mem: 5776
[03:56:10.828228] ====================
[03:56:10.828334] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:56:10.828354] ====================
[03:56:10.843695] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8451 | reduced: 0.8673]  time: 0.6425  data: 0.3519  max mem: 5776
[03:56:10.920041] Test: [0] Total time: 0:00:02 (0.6619 s / it)
[03:56:10.920162] ***************************************************************
[03:56:10.920185] ****An extra tail dataset should exist for accracy metrics!****
[03:56:10.920202] ***************************************************************
[03:56:10.920220] **** Length of tail: 36 ****
[03:56:11.844341] Actual Batchsize/ world_size {'_n': 3.0}
[03:56:11.844474] {'pixel-level F1': tensor(2.9317, device='cuda:0', dtype=torch.float64)}
[03:56:11.866108] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8637 | reduced: 0.8741]  time: 0.9454  data: 0.6300  max mem: 5776
[03:56:12.753331] Actual Batchsize/ world_size {'_n': 3.0}
[03:56:12.753453] {'pixel-level F1': tensor(2.3131, device='cuda:0', dtype=torch.float64)}
[03:56:12.774977] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8451 | reduced: 0.8681]  time: 0.9269  data: 0.6116  max mem: 5776
[03:56:13.619112] Actual Batchsize/ world_size {'_n': 3.0}
[03:56:13.619232] {'pixel-level F1': tensor(2.6794, device='cuda:0', dtype=torch.float64)}
[03:56:13.640971] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8637 | reduced: 0.8695]  time: 0.9065  data: 0.5913  max mem: 5776
[03:56:13.641087] Test <remaining>: [0] Total time: 0:00:02 (0.9069 s / it)
[03:56:13.641569] ---syncronized---
[03:56:13.641599] pixel-level F1 reduced_count 216
[03:56:13.641629] pixel-level F1 reduced_sum 180.69314644824448
[03:56:13.641660] ---syncronized done ---
[03:56:15.169729] Averaged stats: pixel-level F1: [local: 0.8637 | reduced: 0.8365]
[03:56:15.172767] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:56:39.551501] Test: [0]  [ 0/12]  eta: 0:04:52  pixel-level F1: [local: 0.4219 | reduced: 0.4219]  time: 24.3730  data: 24.0616  max mem: 5776
[03:56:50.585579] ====================
[03:56:50.585714] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:56:50.585734] ====================
[03:56:50.601256] Test: [0]  [11/12]  eta: 0:00:02  pixel-level F1: [local: 0.3254 | reduced: 0.3387]  time: 2.9518  data: 2.6493  max mem: 5776
[03:56:50.733989] Test: [0] Total time: 0:00:35 (2.9630 s / it)
[03:56:50.734108] ***************************************************************
[03:56:50.734129] ****An extra tail dataset should exist for accracy metrics!****
[03:56:50.734145] ***************************************************************
[03:56:50.734164] **** Length of tail: 36 ****
[03:57:11.948376] Actual Batchsize/ world_size {'_n': 3.0}
[03:57:11.948530] {'pixel-level F1': tensor(1.0530, device='cuda:0', dtype=torch.float64)}
[03:57:11.969804] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.3448 | reduced: 0.3390]  time: 21.2351  data: 20.9192  max mem: 5776
[03:57:33.460195] Actual Batchsize/ world_size {'_n': 3.0}
[03:57:33.460328] {'pixel-level F1': tensor(0.6712, device='cuda:0', dtype=torch.float64)}
[03:57:33.481830] Test <remaining>: [0]  [1/3]  eta: 0:00:42  pixel-level F1: [local: 0.3254 | reduced: 0.3366]  time: 21.3734  data: 21.0577  max mem: 5776
[03:57:53.893940] Actual Batchsize/ world_size {'_n': 3.0}
[03:57:53.894064] {'pixel-level F1': tensor(1.4414, device='cuda:0', dtype=torch.float64)}
[03:57:53.915703] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.3448 | reduced: 0.3395]  time: 21.0601  data: 20.7446  max mem: 5776
[03:57:53.915834] Test <remaining>: [0] Total time: 0:01:03 (21.0605 s / it)
[03:57:53.916365] ---syncronized---
[03:57:53.916401] pixel-level F1 reduced_count 600
[03:57:53.916431] pixel-level F1 reduced_sum 221.93964464470162
[03:57:53.916463] ---syncronized done ---
[03:57:55.747921] Averaged stats: pixel-level F1: [local: 0.3448 | reduced: 0.3699]
[03:57:55.751475] 
[ROBUST TEST] ColorJitterWrapper param=0.1
[03:57:55.752502] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[03:57:57.090042] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6784 | reduced: 0.6784]  time: 1.3334  data: 1.0231  max mem: 5776
[03:57:57.350314] ====================
[03:57:57.350413] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[03:57:57.350445] ====================
[03:57:57.369661] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.6057 | reduced: 0.6436]  time: 0.8063  data: 0.5116  max mem: 5776
[03:57:57.445147] Test: [0] Total time: 0:00:01 (0.8445 s / it)
[03:57:57.445262] ***************************************************************
[03:57:57.445284] ****An extra tail dataset should exist for accracy metrics!****
[03:57:57.445301] ***************************************************************
[03:57:57.445319] **** Length of tail: 43 ****
[03:57:57.982058] Actual Batchsize/ world_size {'_n': 3.0}
[03:57:57.982181] {'pixel-level F1': tensor(1.8713, device='cuda:0', dtype=torch.float64)}
[03:57:58.003933] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6238 | reduced: 0.6413]  time: 0.5582  data: 0.2436  max mem: 5776
[03:57:58.503007] Actual Batchsize/ world_size {'_n': 3.0}
[03:57:58.503128] {'pixel-level F1': tensor(1.5513, device='cuda:0', dtype=torch.float64)}
[03:57:58.524882] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.6057 | reduced: 0.6285]  time: 0.5394  data: 0.2249  max mem: 5776
[03:57:59.063606] Actual Batchsize/ world_size {'_n': 3.0}
[03:57:59.063725] {'pixel-level F1': tensor(0.9874, device='cuda:0', dtype=torch.float64)}
[03:57:59.085388] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.6057 | reduced: 0.6004]  time: 0.5463  data: 0.2317  max mem: 5776
[03:57:59.400142] ====================
[03:57:59.400233] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[03:57:59.400253] ====================
[03:57:59.400677] Actual Batchsize/ world_size {'_n': 1.75}
[03:57:59.400741] {'pixel-level F1': tensor(0.6739, device='cuda:0', dtype=torch.float64)}
[03:57:59.412456] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5171 | reduced: 0.5893]  time: 0.4914  data: 0.2069  max mem: 5776
[03:57:59.412606] Test <remaining>: [0] Total time: 0:00:01 (0.4918 s / it)
[03:57:59.413110] ---syncronized---
[03:57:59.413143] pixel-level F1 reduced_count 135
[03:57:59.413175] pixel-level F1 reduced_sum 66.6644967356081
[03:57:59.413206] ---syncronized done ---
[03:58:02.060148] Averaged stats: pixel-level F1: [local: 0.5171 | reduced: 0.4938]
[03:58:02.063573] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[03:58:03.177621] Test: [0]  [ 0/20]  eta: 0:00:22  pixel-level F1: [local: 0.4482 | reduced: 0.4482]  time: 1.1069  data: 0.7956  max mem: 5776
[03:58:08.783118] ====================
[03:58:08.783207] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[03:58:08.783226] ====================
[03:58:08.784971] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7992 | reduced: 0.7626]  time: 0.3357  data: 0.0398  max mem: 5776
[03:58:08.874284] Test: [0] Total time: 0:00:06 (0.3402 s / it)
[03:58:08.874402] ***************************************************************
[03:58:08.874424] ****An extra tail dataset should exist for accracy metrics!****
[03:58:08.874440] ***************************************************************
[03:58:08.874457] **** Length of tail: 8 ****
[03:58:09.184106] ====================
[03:58:09.184192] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[03:58:09.184212] ====================
[03:58:09.184636] Actual Batchsize/ world_size {'_n': 2.0}
[03:58:09.184706] {'pixel-level F1': tensor(1.8990, device='cuda:0', dtype=torch.float64)}
[03:58:09.198204] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.8431 | reduced: 0.7642]  time: 0.3233  data: 0.1104  max mem: 5776
[03:58:09.198322] Test <remaining>: [0] Total time: 0:00:00 (0.3237 s / it)
[03:58:09.198858] ---syncronized---
[03:58:09.198891] pixel-level F1 reduced_count 928
[03:58:09.198928] pixel-level F1 reduced_sum 715.2771588314887
[03:58:09.198960] ---syncronized done ---
[03:58:09.627736] Averaged stats: pixel-level F1: [local: 0.8431 | reduced: 0.7708]
[03:58:09.629693] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[03:58:11.414210] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8215 | reduced: 0.8215]  time: 1.7802  data: 1.4703  max mem: 5776
[03:58:12.252561] ====================
[03:58:12.252675] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:58:12.252694] ====================
[03:58:12.268074] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8215 | reduced: 0.8481]  time: 0.6584  data: 0.3677  max mem: 5776
[03:58:12.345381] Test: [0] Total time: 0:00:02 (0.6780 s / it)
[03:58:12.345471] ***************************************************************
[03:58:12.345492] ****An extra tail dataset should exist for accracy metrics!****
[03:58:12.345509] ***************************************************************
[03:58:12.345526] **** Length of tail: 36 ****
[03:58:13.295650] Actual Batchsize/ world_size {'_n': 3.0}
[03:58:13.295782] {'pixel-level F1': tensor(2.9557, device='cuda:0', dtype=torch.float64)}
[03:58:13.317493] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8444 | reduced: 0.8567]  time: 0.9715  data: 0.6563  max mem: 5776
[03:58:14.230177] Actual Batchsize/ world_size {'_n': 3.0}
[03:58:14.230310] {'pixel-level F1': tensor(2.3441, device='cuda:0', dtype=torch.float64)}
[03:58:14.251936] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8215 | reduced: 0.8523]  time: 0.9528  data: 0.6377  max mem: 5776
[03:58:15.111760] Actual Batchsize/ world_size {'_n': 3.0}
[03:58:15.111883] {'pixel-level F1': tensor(2.7248, device='cuda:0', dtype=torch.float64)}
[03:58:15.133696] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8444 | reduced: 0.8554]  time: 0.9290  data: 0.6140  max mem: 5776
[03:58:15.133818] Test <remaining>: [0] Total time: 0:00:02 (0.9294 s / it)
[03:58:15.134312] ---syncronized---
[03:58:15.134345] pixel-level F1 reduced_count 216
[03:58:15.134376] pixel-level F1 reduced_sum 184.56633693225848
[03:58:15.134407] ---syncronized done ---
[03:58:17.519510] Averaged stats: pixel-level F1: [local: 0.8444 | reduced: 0.8545]
[03:58:17.522532] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[03:58:42.989030] Test: [0]  [ 0/12]  eta: 0:05:05  pixel-level F1: [local: 0.4514 | reduced: 0.4514]  time: 25.4603  data: 25.1370  max mem: 5776
[03:58:54.422199] ====================
[03:58:54.422325] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[03:58:54.422345] ====================
[03:58:54.437654] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.3281 | reduced: 0.3073]  time: 3.0757  data: 2.7717  max mem: 5776
[03:58:54.569898] Test: [0] Total time: 0:00:37 (3.0868 s / it)
[03:58:54.570018] ***************************************************************
[03:58:54.570040] ****An extra tail dataset should exist for accracy metrics!****
[03:58:54.570056] ***************************************************************
[03:58:54.570074] **** Length of tail: 36 ****
[03:59:16.408737] Actual Batchsize/ world_size {'_n': 3.0}
[03:59:16.408867] {'pixel-level F1': tensor(0.8932, device='cuda:0', dtype=torch.float64)}
[03:59:16.430403] Test <remaining>: [0]  [0/3]  eta: 0:01:05  pixel-level F1: [local: 0.3281 | reduced: 0.3071]  time: 21.8598  data: 21.5445  max mem: 5776
[03:59:37.723823] Actual Batchsize/ world_size {'_n': 3.0}
[03:59:37.723957] {'pixel-level F1': tensor(0.6876, device='cuda:0', dtype=torch.float64)}
[03:59:37.745434] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.2977 | reduced: 0.3056]  time: 21.5872  data: 21.2719  max mem: 5776
[03:59:58.641314] Actual Batchsize/ world_size {'_n': 3.0}
[03:59:58.641440] {'pixel-level F1': tensor(1.2632, device='cuda:0', dtype=torch.float64)}
[03:59:58.663052] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.3281 | reduced: 0.3079]  time: 21.3639  data: 21.0486  max mem: 5776
[03:59:58.663187] Test <remaining>: [0] Total time: 0:01:04 (21.3643 s / it)
[03:59:58.663880] ---syncronized---
[03:59:58.663914] pixel-level F1 reduced_count 600
[03:59:58.663945] pixel-level F1 reduced_sum 214.21164115348608
[03:59:58.663978] ---syncronized done ---
[04:00:00.858486] Averaged stats: pixel-level F1: [local: 0.3281 | reduced: 0.3570]
[04:00:00.861793] 
[ROBUST TEST] ColorJitterWrapper param=0.2
[04:00:00.862753] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[04:00:02.105176] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.7131 | reduced: 0.7131]  time: 1.2384  data: 0.9280  max mem: 5776
[04:00:02.365188] ====================
[04:00:02.365262] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[04:00:02.365282] ====================
[04:00:02.384509] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5921 | reduced: 0.6552]  time: 0.7586  data: 0.4640  max mem: 5776
[04:00:02.458292] Test: [0] Total time: 0:00:01 (0.7960 s / it)
[04:00:02.458402] ***************************************************************
[04:00:02.458424] ****An extra tail dataset should exist for accracy metrics!****
[04:00:02.458440] ***************************************************************
[04:00:02.458458] **** Length of tail: 43 ****
[04:00:03.000690] Actual Batchsize/ world_size {'_n': 3.0}
[04:00:03.000815] {'pixel-level F1': tensor(1.9623, device='cuda:0', dtype=torch.float64)}
[04:00:03.022420] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6541 | reduced: 0.6551]  time: 0.5635  data: 0.2488  max mem: 5776
[04:00:03.520884] Actual Batchsize/ world_size {'_n': 3.0}
[04:00:03.521005] {'pixel-level F1': tensor(1.5513, device='cuda:0', dtype=torch.float64)}
[04:00:03.542657] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5921 | reduced: 0.6408]  time: 0.5417  data: 0.2270  max mem: 5776
[04:00:04.082555] Actual Batchsize/ world_size {'_n': 3.0}
[04:00:04.082679] {'pixel-level F1': tensor(0.9414, device='cuda:0', dtype=torch.float64)}
[04:00:04.104325] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5921 | reduced: 0.6102]  time: 0.5482  data: 0.2335  max mem: 5776
[04:00:04.421361] ====================
[04:00:04.421457] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[04:00:04.421477] ====================
[04:00:04.421898] Actual Batchsize/ world_size {'_n': 1.75}
[04:00:04.421958] {'pixel-level F1': tensor(0.6229, device='cuda:0', dtype=torch.float64)}
[04:00:04.433558] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5171 | reduced: 0.5970]  time: 0.4934  data: 0.2088  max mem: 5776
[04:00:04.433674] Test <remaining>: [0] Total time: 0:00:01 (0.4938 s / it)
[04:00:04.434184] ---syncronized---
[04:00:04.434216] pixel-level F1 reduced_count 135
[04:00:04.434246] pixel-level F1 reduced_sum 66.51859426926802
[04:00:04.434277] ---syncronized done ---
[04:00:07.038464] Averaged stats: pixel-level F1: [local: 0.5171 | reduced: 0.4927]
[04:00:07.041853] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[04:00:08.122541] Test: [0]  [ 0/20]  eta: 0:00:21  pixel-level F1: [local: 0.4548 | reduced: 0.4548]  time: 1.0736  data: 0.7613  max mem: 5776
[04:00:13.728317] ====================
[04:00:13.728407] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[04:00:13.728431] ====================
[04:00:13.730138] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7781 | reduced: 0.7632]  time: 0.3340  data: 0.0381  max mem: 5776
[04:00:13.815106] Test: [0] Total time: 0:00:06 (0.3383 s / it)
[04:00:13.815224] ***************************************************************
[04:00:13.815245] ****An extra tail dataset should exist for accracy metrics!****
[04:00:13.815262] ***************************************************************
[04:00:13.815280] **** Length of tail: 8 ****
[04:00:14.119967] ====================
[04:00:14.120053] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[04:00:14.120073] ====================
[04:00:14.120500] Actual Batchsize/ world_size {'_n': 2.0}
[04:00:14.120572] {'pixel-level F1': tensor(1.8983, device='cuda:0', dtype=torch.float64)}
[04:00:14.133899] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.8290 | reduced: 0.7648]  time: 0.3182  data: 0.1054  max mem: 5776
[04:00:14.134015] Test <remaining>: [0] Total time: 0:00:00 (0.3186 s / it)
[04:00:14.134529] ---syncronized---
[04:00:14.134561] pixel-level F1 reduced_count 928
[04:00:14.134590] pixel-level F1 reduced_sum 712.5516623572098
[04:00:14.134621] ---syncronized done ---
[04:00:14.537608] Averaged stats: pixel-level F1: [local: 0.8290 | reduced: 0.7678]
[04:00:14.538742] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[04:00:16.342372] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8216 | reduced: 0.8216]  time: 1.7992  data: 1.4893  max mem: 5776
[04:00:17.180891] ====================
[04:00:17.181000] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:00:17.181020] ====================
[04:00:17.196405] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8216 | reduced: 0.8462]  time: 0.6632  data: 0.3724  max mem: 5776
[04:00:17.274620] Test: [0] Total time: 0:00:02 (0.6830 s / it)
[04:00:17.274737] ***************************************************************
[04:00:17.274759] ****An extra tail dataset should exist for accracy metrics!****
[04:00:17.274776] ***************************************************************
[04:00:17.274794] **** Length of tail: 36 ****
[04:00:18.221967] Actual Batchsize/ world_size {'_n': 3.0}
[04:00:18.222101] {'pixel-level F1': tensor(2.9566, device='cuda:0', dtype=torch.float64)}
[04:00:18.243817] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8409 | reduced: 0.8550]  time: 0.9685  data: 0.6531  max mem: 5776
[04:00:19.154309] Actual Batchsize/ world_size {'_n': 3.0}
[04:00:19.154437] {'pixel-level F1': tensor(2.4162, device='cuda:0', dtype=torch.float64)}
[04:00:19.176144] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8216 | reduced: 0.8520]  time: 0.9502  data: 0.6350  max mem: 5776
[04:00:20.036980] Actual Batchsize/ world_size {'_n': 3.0}
[04:00:20.037101] {'pixel-level F1': tensor(2.7253, device='cuda:0', dtype=torch.float64)}
[04:00:20.058781] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8409 | reduced: 0.8552]  time: 0.9276  data: 0.6124  max mem: 5776
[04:00:20.058903] Test <remaining>: [0] Total time: 0:00:02 (0.9280 s / it)
[04:00:20.059388] ---syncronized---
[04:00:20.059419] pixel-level F1 reduced_count 216
[04:00:20.059449] pixel-level F1 reduced_sum 184.71147223935372
[04:00:20.059480] ---syncronized done ---
[04:00:22.543602] Averaged stats: pixel-level F1: [local: 0.8409 | reduced: 0.8551]
[04:00:22.546974] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[04:00:48.074676] Test: [0]  [ 0/12]  eta: 0:05:06  pixel-level F1: [local: 0.4537 | reduced: 0.4537]  time: 25.5169  data: 25.2059  max mem: 5776
[04:00:59.090486] ====================
[04:00:59.090614] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:00:59.090634] ====================
[04:00:59.106147] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2989 | reduced: 0.3073]  time: 3.0456  data: 2.7423  max mem: 5776
[04:00:59.241614] Test: [0] Total time: 0:00:36 (3.0570 s / it)
[04:00:59.241728] ***************************************************************
[04:00:59.241750] ****An extra tail dataset should exist for accracy metrics!****
[04:00:59.241766] ***************************************************************
[04:00:59.241784] **** Length of tail: 36 ****
[04:01:20.792358] Actual Batchsize/ world_size {'_n': 3.0}
[04:01:20.792515] {'pixel-level F1': tensor(0.8981, device='cuda:0', dtype=torch.float64)}
[04:01:20.811996] Test <remaining>: [0]  [0/3]  eta: 0:01:04  pixel-level F1: [local: 0.2994 | reduced: 0.3071]  time: 21.5697  data: 21.2527  max mem: 5776
[04:01:42.659874] Actual Batchsize/ world_size {'_n': 3.0}
[04:01:42.660011] {'pixel-level F1': tensor(0.7058, device='cuda:0', dtype=torch.float64)}
[04:01:42.681526] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.2989 | reduced: 0.3057]  time: 21.7194  data: 21.4033  max mem: 5776
[04:02:03.464477] Actual Batchsize/ world_size {'_n': 3.0}
[04:02:03.464608] {'pixel-level F1': tensor(1.2281, device='cuda:0', dtype=torch.float64)}
[04:02:03.486253] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.2994 | reduced: 0.3077]  time: 21.4144  data: 21.0985  max mem: 5776
[04:02:03.486387] Test <remaining>: [0] Total time: 0:01:04 (21.4148 s / it)
[04:02:03.487081] ---syncronized---
[04:02:03.487118] pixel-level F1 reduced_count 600
[04:02:03.487148] pixel-level F1 reduced_sum 210.7096727861853
[04:02:03.487178] ---syncronized done ---
[04:02:05.557765] Averaged stats: pixel-level F1: [local: 0.2994 | reduced: 0.3512]
[04:02:05.561076] 
[ROBUST TEST] ColorJitterWrapper param=0.3
[04:02:05.562045] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[04:02:06.809282] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6665 | reduced: 0.6665]  time: 1.2432  data: 0.9312  max mem: 5776
[04:02:07.069303] ====================
[04:02:07.069379] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[04:02:07.069399] ====================
[04:02:07.088680] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5553 | reduced: 0.6133]  time: 0.7611  data: 0.4657  max mem: 5776
[04:02:07.162807] Test: [0] Total time: 0:00:01 (0.7986 s / it)
[04:02:07.162915] ***************************************************************
[04:02:07.162935] ****An extra tail dataset should exist for accracy metrics!****
[04:02:07.162952] ***************************************************************
[04:02:07.162970] **** Length of tail: 43 ****
[04:02:07.696417] Actual Batchsize/ world_size {'_n': 3.0}
[04:02:07.696547] {'pixel-level F1': tensor(1.7407, device='cuda:0', dtype=torch.float64)}
[04:02:07.718331] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.5802 | reduced: 0.6095]  time: 0.5549  data: 0.2406  max mem: 5776
[04:02:08.214077] Actual Batchsize/ world_size {'_n': 3.0}
[04:02:08.214195] {'pixel-level F1': tensor(1.3512, device='cuda:0', dtype=torch.float64)}
[04:02:08.235899] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5553 | reduced: 0.5930]  time: 0.5361  data: 0.2217  max mem: 5776
[04:02:08.776895] Actual Batchsize/ world_size {'_n': 3.0}
[04:02:08.777012] {'pixel-level F1': tensor(0.8402, device='cuda:0', dtype=torch.float64)}
[04:02:08.798697] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5553 | reduced: 0.5637]  time: 0.5449  data: 0.2304  max mem: 5776
[04:02:09.111830] ====================
[04:02:09.111924] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[04:02:09.111949] ====================
[04:02:09.112361] Actual Batchsize/ world_size {'_n': 1.75}
[04:02:09.112417] {'pixel-level F1': tensor(0.6898, device='cuda:0', dtype=torch.float64)}
[04:02:09.124068] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.4504 | reduced: 0.5549]  time: 0.4899  data: 0.2056  max mem: 5776
[04:02:09.124189] Test <remaining>: [0] Total time: 0:00:01 (0.4903 s / it)
[04:02:09.124740] ---syncronized---
[04:02:09.124779] pixel-level F1 reduced_count 135
[04:02:09.124808] pixel-level F1 reduced_sum 67.00503977286958
[04:02:09.124839] ---syncronized done ---
[04:02:11.747087] Averaged stats: pixel-level F1: [local: 0.4504 | reduced: 0.4963]
[04:02:11.750524] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[04:02:13.003162] Test: [0]  [ 0/20]  eta: 0:00:24  pixel-level F1: [local: 0.4531 | reduced: 0.4531]  time: 1.2454  data: 0.9332  max mem: 5776
[04:02:18.607546] ====================
[04:02:18.607640] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[04:02:18.607658] ====================
[04:02:18.609361] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7627 | reduced: 0.7520]  time: 0.3426  data: 0.0467  max mem: 5776
[04:02:18.696778] Test: [0] Total time: 0:00:06 (0.3470 s / it)
[04:02:18.696907] ***************************************************************
[04:02:18.696928] ****An extra tail dataset should exist for accracy metrics!****
[04:02:18.696944] ***************************************************************
[04:02:18.696962] **** Length of tail: 8 ****
[04:02:19.008888] ====================
[04:02:19.008971] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[04:02:19.008991] ====================
[04:02:19.009410] Actual Batchsize/ world_size {'_n': 2.0}
[04:02:19.009474] {'pixel-level F1': tensor(1.8596, device='cuda:0', dtype=torch.float64)}
[04:02:19.022864] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.8010 | reduced: 0.7535]  time: 0.3254  data: 0.1124  max mem: 5776
[04:02:19.022985] Test <remaining>: [0] Total time: 0:00:00 (0.3259 s / it)
[04:02:19.023515] ---syncronized---
[04:02:19.023547] pixel-level F1 reduced_count 928
[04:02:19.023577] pixel-level F1 reduced_sum 708.4114175642198
[04:02:19.023608] ---syncronized done ---
[04:02:19.433368] Averaged stats: pixel-level F1: [local: 0.8010 | reduced: 0.7634]
[04:02:19.434525] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[04:02:21.216075] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8233 | reduced: 0.8233]  time: 1.7772  data: 1.4674  max mem: 5776
[04:02:22.054562] ====================
[04:02:22.054668] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:02:22.054687] ====================
[04:02:22.070076] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8233 | reduced: 0.8545]  time: 0.6577  data: 0.3669  max mem: 5776
[04:02:22.146080] Test: [0] Total time: 0:00:02 (0.6769 s / it)
[04:02:22.146199] ***************************************************************
[04:02:22.146219] ****An extra tail dataset should exist for accracy metrics!****
[04:02:22.146235] ***************************************************************
[04:02:22.146253] **** Length of tail: 36 ****
[04:02:23.090779] Actual Batchsize/ world_size {'_n': 3.0}
[04:02:23.090912] {'pixel-level F1': tensor(2.9545, device='cuda:0', dtype=torch.float64)}
[04:02:23.112549] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8818 | reduced: 0.8627]  time: 0.9658  data: 0.6502  max mem: 5776
[04:02:24.028269] Actual Batchsize/ world_size {'_n': 3.0}
[04:02:24.028404] {'pixel-level F1': tensor(2.3644, device='cuda:0', dtype=torch.float64)}
[04:02:24.050007] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8233 | reduced: 0.8583]  time: 0.9514  data: 0.6361  max mem: 5776
[04:02:24.916998] Actual Batchsize/ world_size {'_n': 3.0}
[04:02:24.917123] {'pixel-level F1': tensor(2.6407, device='cuda:0', dtype=torch.float64)}
[04:02:24.938777] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8802 | reduced: 0.8595]  time: 0.9304  data: 0.6151  max mem: 5776
[04:02:24.938927] Test <remaining>: [0] Total time: 0:00:02 (0.9308 s / it)
[04:02:24.939433] ---syncronized---
[04:02:24.939468] pixel-level F1 reduced_count 216
[04:02:24.939498] pixel-level F1 reduced_sum 182.89794826105864
[04:02:24.939529] ---syncronized done ---
[04:02:27.224783] Averaged stats: pixel-level F1: [local: 0.8802 | reduced: 0.8467]
[04:02:27.227794] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[04:02:53.094401] Test: [0]  [ 0/12]  eta: 0:05:10  pixel-level F1: [local: 0.3897 | reduced: 0.3897]  time: 25.8608  data: 25.5501  max mem: 5776
[04:03:04.893569] ====================
[04:03:04.893682] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:03:04.893701] ====================
[04:03:04.909086] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2498 | reduced: 0.2871]  time: 3.1396  data: 2.8372  max mem: 5776
[04:03:05.044816] Test: [0] Total time: 0:00:37 (3.1510 s / it)
[04:03:05.044921] ***************************************************************
[04:03:05.044941] ****An extra tail dataset should exist for accracy metrics!****
[04:03:05.044957] ***************************************************************
[04:03:05.044975] **** Length of tail: 36 ****
[04:03:26.285672] Actual Batchsize/ world_size {'_n': 3.0}
[04:03:26.285836] {'pixel-level F1': tensor(0.8428, device='cuda:0', dtype=torch.float64)}
[04:03:26.307103] Test <remaining>: [0]  [0/3]  eta: 0:01:03  pixel-level F1: [local: 0.2528 | reduced: 0.2870]  time: 21.2615  data: 20.9454  max mem: 5776
[04:03:48.246163] Actual Batchsize/ world_size {'_n': 3.0}
[04:03:48.246296] {'pixel-level F1': tensor(0.7185, device='cuda:0', dtype=torch.float64)}
[04:03:48.267884] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.2498 | reduced: 0.2860]  time: 21.6110  data: 21.2952  max mem: 5776
[04:04:08.835503] Actual Batchsize/ world_size {'_n': 3.0}
[04:04:08.835634] {'pixel-level F1': tensor(1.1757, device='cuda:0', dtype=torch.float64)}
[04:04:08.857297] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.2528 | reduced: 0.2881]  time: 21.2703  data: 20.9548  max mem: 5776
[04:04:08.857434] Test <remaining>: [0] Total time: 0:01:03 (21.2708 s / it)
[04:04:08.858146] ---syncronized---
[04:04:08.858179] pixel-level F1 reduced_count 600
[04:04:08.858210] pixel-level F1 reduced_sum 207.23086390780057
[04:04:08.858242] ---syncronized done ---
[04:04:10.996222] Averaged stats: pixel-level F1: [local: 0.2528 | reduced: 0.3454]
[04:04:10.999582] 
[ROBUST TEST] ColorJitterWrapper param=0.4
[04:04:11.000659] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[04:04:12.276822] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6336 | reduced: 0.6336]  time: 1.2720  data: 0.9614  max mem: 5776
[04:04:12.536842] ====================
[04:04:12.536918] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[04:04:12.536939] ====================
[04:04:12.556144] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5583 | reduced: 0.5976]  time: 0.7754  data: 0.4807  max mem: 5776
[04:04:12.633103] Test: [0] Total time: 0:00:01 (0.8144 s / it)
[04:04:12.633207] ***************************************************************
[04:04:12.633232] ****An extra tail dataset should exist for accracy metrics!****
[04:04:12.633248] ***************************************************************
[04:04:12.633265] **** Length of tail: 43 ****
[04:04:13.164104] Actual Batchsize/ world_size {'_n': 3.0}
[04:04:13.164228] {'pixel-level F1': tensor(2.0034, device='cuda:0', dtype=torch.float64)}
[04:04:13.185886] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6336 | reduced: 0.6057]  time: 0.5522  data: 0.2378  max mem: 5776
[04:04:13.681430] Actual Batchsize/ world_size {'_n': 3.0}
[04:04:13.681545] {'pixel-level F1': tensor(1.6584, device='cuda:0', dtype=torch.float64)}
[04:04:13.703312] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5583 | reduced: 0.6002]  time: 0.5346  data: 0.2201  max mem: 5776
[04:04:14.241764] Actual Batchsize/ world_size {'_n': 3.0}
[04:04:14.241884] {'pixel-level F1': tensor(1.0525, device='cuda:0', dtype=torch.float64)}
[04:04:14.263573] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5583 | reduced: 0.5768]  time: 0.5430  data: 0.2285  max mem: 5776
[04:04:14.577745] ====================
[04:04:14.577839] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[04:04:14.577860] ====================
[04:04:14.578270] Actual Batchsize/ world_size {'_n': 1.75}
[04:04:14.578326] {'pixel-level F1': tensor(0.7948, device='cuda:0', dtype=torch.float64)}
[04:04:14.589958] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5528 | reduced: 0.5705]  time: 0.4888  data: 0.2044  max mem: 5776
[04:04:14.590081] Test <remaining>: [0] Total time: 0:00:01 (0.4892 s / it)
[04:04:14.590599] ---syncronized---
[04:04:14.590631] pixel-level F1 reduced_count 135
[04:04:14.590660] pixel-level F1 reduced_sum 65.62294618360457
[04:04:14.590690] ---syncronized done ---
[04:04:17.190335] Averaged stats: pixel-level F1: [local: 0.5528 | reduced: 0.4861]
[04:04:17.193642] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[04:04:18.288180] Test: [0]  [ 0/20]  eta: 0:00:21  pixel-level F1: [local: 0.4174 | reduced: 0.4174]  time: 1.0875  data: 0.7761  max mem: 5776
[04:04:23.893911] ====================
[04:04:23.894002] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[04:04:23.894021] ====================
[04:04:23.895714] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7597 | reduced: 0.7546]  time: 0.3347  data: 0.0389  max mem: 5776
[04:04:23.983917] Test: [0] Total time: 0:00:06 (0.3392 s / it)
[04:04:23.984028] ***************************************************************
[04:04:23.984047] ****An extra tail dataset should exist for accracy metrics!****
[04:04:23.984063] ***************************************************************
[04:04:23.984080] **** Length of tail: 8 ****
[04:04:24.289708] ====================
[04:04:24.289793] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[04:04:24.289812] ====================
[04:04:24.290236] Actual Batchsize/ world_size {'_n': 2.0}
[04:04:24.290298] {'pixel-level F1': tensor(1.8980, device='cuda:0', dtype=torch.float64)}
[04:04:24.303649] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7885 | reduced: 0.7563]  time: 0.3191  data: 0.1067  max mem: 5776
[04:04:24.303765] Test <remaining>: [0] Total time: 0:00:00 (0.3195 s / it)
[04:04:24.304260] ---syncronized---
[04:04:24.304289] pixel-level F1 reduced_count 928
[04:04:24.304319] pixel-level F1 reduced_sum 697.5096139438152
[04:04:24.304350] ---syncronized done ---
[04:04:24.701630] Averaged stats: pixel-level F1: [local: 0.7885 | reduced: 0.7516]
[04:04:24.702754] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[04:04:26.484094] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8214 | reduced: 0.8214]  time: 1.7769  data: 1.4669  max mem: 5776
[04:04:27.324423] ====================
[04:04:27.324535] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:04:27.324560] ====================
[04:04:27.339917] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8214 | reduced: 0.8546]  time: 0.6581  data: 0.3668  max mem: 5776
[04:04:27.417274] Test: [0] Total time: 0:00:02 (0.6776 s / it)
[04:04:27.417384] ***************************************************************
[04:04:27.417405] ****An extra tail dataset should exist for accracy metrics!****
[04:04:27.417422] ***************************************************************
[04:04:27.417439] **** Length of tail: 36 ****
[04:04:28.361507] Actual Batchsize/ world_size {'_n': 3.0}
[04:04:28.361640] {'pixel-level F1': tensor(2.9562, device='cuda:0', dtype=torch.float64)}
[04:04:28.383285] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8804 | reduced: 0.8628]  time: 0.9654  data: 0.6500  max mem: 5776
[04:04:29.307349] Actual Batchsize/ world_size {'_n': 3.0}
[04:04:29.307478] {'pixel-level F1': tensor(2.2813, device='cuda:0', dtype=torch.float64)}
[04:04:29.328979] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8214 | reduced: 0.8568]  time: 0.9553  data: 0.6394  max mem: 5776
[04:04:30.192969] Actual Batchsize/ world_size {'_n': 3.0}
[04:04:30.193088] {'pixel-level F1': tensor(2.7172, device='cuda:0', dtype=torch.float64)}
[04:04:30.214801] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8804 | reduced: 0.8595]  time: 0.9320  data: 0.6164  max mem: 5776
[04:04:30.214949] Test <remaining>: [0] Total time: 0:00:02 (0.9325 s / it)
[04:04:30.215454] ---syncronized---
[04:04:30.215489] pixel-level F1 reduced_count 216
[04:04:30.215520] pixel-level F1 reduced_sum 184.1622373939709
[04:04:30.215552] ---syncronized done ---
[04:04:32.566654] Averaged stats: pixel-level F1: [local: 0.8804 | reduced: 0.8526]
[04:04:32.569754] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[04:04:58.213421] Test: [0]  [ 0/12]  eta: 0:05:07  pixel-level F1: [local: 0.3822 | reduced: 0.3822]  time: 25.6376  data: 25.3267  max mem: 5776
[04:05:09.582029] ====================
[04:05:09.582151] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:05:09.582182] ====================
[04:05:09.597548] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2543 | reduced: 0.2995]  time: 3.0851  data: 2.7829  max mem: 5776
[04:05:09.725979] Test: [0] Total time: 0:00:37 (3.0959 s / it)
[04:05:09.726111] ***************************************************************
[04:05:09.726141] ****An extra tail dataset should exist for accracy metrics!****
[04:05:09.726166] ***************************************************************
[04:05:09.726193] **** Length of tail: 36 ****
[04:05:31.378880] Actual Batchsize/ world_size {'_n': 3.0}
[04:05:31.379030] {'pixel-level F1': tensor(0.8396, device='cuda:0', dtype=torch.float64)}
[04:05:31.400416] Test <remaining>: [0]  [0/3]  eta: 0:01:05  pixel-level F1: [local: 0.2799 | reduced: 0.2991]  time: 21.6737  data: 21.3579  max mem: 5776
[04:05:53.138296] Actual Batchsize/ world_size {'_n': 3.0}
[04:05:53.138443] {'pixel-level F1': tensor(0.6581, device='cuda:0', dtype=torch.float64)}
[04:05:53.159853] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.2543 | reduced: 0.2975]  time: 21.7163  data: 21.4003  max mem: 5776
[04:06:14.005832] Actual Batchsize/ world_size {'_n': 3.0}
[04:06:14.005963] {'pixel-level F1': tensor(1.1654, device='cuda:0', dtype=torch.float64)}
[04:06:14.027461] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.2799 | reduced: 0.2993]  time: 21.4333  data: 21.1175  max mem: 5776
[04:06:14.027609] Test <remaining>: [0] Total time: 0:01:04 (21.4337 s / it)
[04:06:14.028256] ---syncronized---
[04:06:14.028289] pixel-level F1 reduced_count 600
[04:06:14.028319] pixel-level F1 reduced_sum 210.02298542459258
[04:06:14.028350] ---syncronized done ---
[04:06:16.113460] Averaged stats: pixel-level F1: [local: 0.2799 | reduced: 0.3500]
[04:06:16.116833] 
[ROBUST TEST] ColorJitterWrapper param=0.5
[04:06:16.117921] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[04:06:17.385307] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6458 | reduced: 0.6458]  time: 1.2632  data: 0.9532  max mem: 5776
[04:06:17.645228] ====================
[04:06:17.645306] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[04:06:17.645328] ====================
[04:06:17.664652] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5199 | reduced: 0.5856]  time: 0.7710  data: 0.4767  max mem: 5776
[04:06:17.738355] Test: [0] Total time: 0:00:01 (0.8084 s / it)
[04:06:17.738459] ***************************************************************
[04:06:17.738480] ****An extra tail dataset should exist for accracy metrics!****
[04:06:17.738497] ***************************************************************
[04:06:17.738514] **** Length of tail: 43 ****
[04:06:18.271205] Actual Batchsize/ world_size {'_n': 3.0}
[04:06:18.271328] {'pixel-level F1': tensor(1.7773, device='cuda:0', dtype=torch.float64)}
[04:06:18.293038] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.5924 | reduced: 0.5864]  time: 0.5541  data: 0.2398  max mem: 5776
[04:06:18.795386] Actual Batchsize/ world_size {'_n': 3.0}
[04:06:18.795508] {'pixel-level F1': tensor(1.3687, device='cuda:0', dtype=torch.float64)}
[04:06:18.817168] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5199 | reduced: 0.5729]  time: 0.5389  data: 0.2246  max mem: 5776
[04:06:19.359936] Actual Batchsize/ world_size {'_n': 3.0}
[04:06:19.360049] {'pixel-level F1': tensor(1.1298, device='cuda:0', dtype=torch.float64)}
[04:06:19.381698] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5199 | reduced: 0.5545]  time: 0.5473  data: 0.2329  max mem: 5776
[04:06:19.697783] ====================
[04:06:19.697875] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[04:06:19.697895] ====================
[04:06:19.698307] Actual Batchsize/ world_size {'_n': 1.75}
[04:06:19.698366] {'pixel-level F1': tensor(0.5295, device='cuda:0', dtype=torch.float64)}
[04:06:19.709936] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.4562 | reduced: 0.5414]  time: 0.4925  data: 0.2081  max mem: 5776
[04:06:19.710062] Test <remaining>: [0] Total time: 0:00:01 (0.4929 s / it)
[04:06:19.710595] ---syncronized---
[04:06:19.710627] pixel-level F1 reduced_count 135
[04:06:19.710657] pixel-level F1 reduced_sum 63.08692490356684
[04:06:19.710687] ---syncronized done ---
[04:06:22.220410] Averaged stats: pixel-level F1: [local: 0.4562 | reduced: 0.4673]
[04:06:22.223767] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[04:06:23.437506] Test: [0]  [ 0/20]  eta: 0:00:24  pixel-level F1: [local: 0.4503 | reduced: 0.4503]  time: 1.2069  data: 0.8960  max mem: 5776
[04:06:29.040377] ====================
[04:06:29.040467] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[04:06:29.040486] ====================
[04:06:29.042198] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7538 | reduced: 0.7444]  time: 0.3406  data: 0.0449  max mem: 5776
[04:06:29.131820] Test: [0] Total time: 0:00:06 (0.3451 s / it)
[04:06:29.131909] ***************************************************************
[04:06:29.131933] ****An extra tail dataset should exist for accracy metrics!****
[04:06:29.131949] ***************************************************************
[04:06:29.131967] **** Length of tail: 8 ****
[04:06:29.436574] ====================
[04:06:29.436655] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[04:06:29.436674] ====================
[04:06:29.437088] Actual Batchsize/ world_size {'_n': 2.0}
[04:06:29.437149] {'pixel-level F1': tensor(1.8420, device='cuda:0', dtype=torch.float64)}
[04:06:29.450510] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7859 | reduced: 0.7459]  time: 0.3181  data: 0.1058  max mem: 5776
[04:06:29.450633] Test <remaining>: [0] Total time: 0:00:00 (0.3185 s / it)
[04:06:29.451217] ---syncronized---
[04:06:29.451249] pixel-level F1 reduced_count 928
[04:06:29.451278] pixel-level F1 reduced_sum 691.6593824086269
[04:06:29.451309] ---syncronized done ---
[04:06:29.856612] Averaged stats: pixel-level F1: [local: 0.7859 | reduced: 0.7453]
[04:06:29.857743] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[04:06:31.600217] Test: [0]  [0/4]  eta: 0:00:06  pixel-level F1: [local: 0.8217 | reduced: 0.8217]  time: 1.7381  data: 1.4275  max mem: 5776
[04:06:32.436088] ====================
[04:06:32.436200] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:06:32.436220] ====================
[04:06:32.451493] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8217 | reduced: 0.8489]  time: 0.6472  data: 0.3570  max mem: 5776
[04:06:32.528184] Test: [0] Total time: 0:00:02 (0.6666 s / it)
[04:06:32.528301] ***************************************************************
[04:06:32.528321] ****An extra tail dataset should exist for accracy metrics!****
[04:06:32.528338] ***************************************************************
[04:06:32.528358] **** Length of tail: 36 ****
[04:06:33.471150] Actual Batchsize/ world_size {'_n': 3.0}
[04:06:33.471289] {'pixel-level F1': tensor(2.9560, device='cuda:0', dtype=torch.float64)}
[04:06:33.492916] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8541 | reduced: 0.8574]  time: 0.9641  data: 0.6487  max mem: 5776
[04:06:34.414887] Actual Batchsize/ world_size {'_n': 3.0}
[04:06:34.415015] {'pixel-level F1': tensor(2.4602, device='cuda:0', dtype=torch.float64)}
[04:06:34.436643] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8217 | reduced: 0.8552]  time: 0.9537  data: 0.6385  max mem: 5776
[04:06:35.306840] Actual Batchsize/ world_size {'_n': 3.0}
[04:06:35.306956] {'pixel-level F1': tensor(2.7235, device='cuda:0', dtype=torch.float64)}
[04:06:35.328594] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8541 | reduced: 0.8581]  time: 0.9330  data: 0.6179  max mem: 5776
[04:06:35.328729] Test <remaining>: [0] Total time: 0:00:02 (0.9334 s / it)
[04:06:35.329209] ---syncronized---
[04:06:35.329243] pixel-level F1 reduced_count 216
[04:06:35.329274] pixel-level F1 reduced_sum 183.78965224483912
[04:06:35.329304] ---syncronized done ---
[04:06:37.691311] Averaged stats: pixel-level F1: [local: 0.8541 | reduced: 0.8509]
[04:06:37.694353] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[04:07:03.536384] Test: [0]  [ 0/12]  eta: 0:05:10  pixel-level F1: [local: 0.4126 | reduced: 0.4126]  time: 25.8363  data: 25.5255  max mem: 5776
[04:07:14.895496] ====================
[04:07:14.895614] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:07:14.895634] ====================
[04:07:14.911017] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2558 | reduced: 0.2821]  time: 3.1009  data: 2.7985  max mem: 5776
[04:07:15.028074] Test: [0] Total time: 0:00:37 (3.1107 s / it)
[04:07:15.028175] ***************************************************************
[04:07:15.028196] ****An extra tail dataset should exist for accracy metrics!****
[04:07:15.028212] ***************************************************************
[04:07:15.028230] **** Length of tail: 36 ****
[04:07:36.697141] Actual Batchsize/ world_size {'_n': 3.0}
[04:07:36.697282] {'pixel-level F1': tensor(1.1121, device='cuda:0', dtype=torch.float64)}
[04:07:36.718681] Test <remaining>: [0]  [0/3]  eta: 0:01:05  pixel-level F1: [local: 0.2579 | reduced: 0.2840]  time: 21.6900  data: 21.3737  max mem: 5776
[04:07:58.392994] Actual Batchsize/ world_size {'_n': 3.0}
[04:07:58.393131] {'pixel-level F1': tensor(0.6236, device='cuda:0', dtype=torch.float64)}
[04:07:58.414667] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.2558 | reduced: 0.2824]  time: 21.6928  data: 21.3770  max mem: 5776
[04:08:18.759070] Actual Batchsize/ world_size {'_n': 3.0}
[04:08:18.759195] {'pixel-level F1': tensor(1.2688, device='cuda:0', dtype=torch.float64)}
[04:08:18.780834] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.2579 | reduced: 0.2852]  time: 21.2504  data: 20.9349  max mem: 5776
[04:08:18.780967] Test <remaining>: [0] Total time: 0:01:03 (21.2509 s / it)
[04:08:18.781604] ---syncronized---
[04:08:18.781638] pixel-level F1 reduced_count 600
[04:08:18.781670] pixel-level F1 reduced_sum 202.84115276675703
[04:08:18.781701] ---syncronized done ---
[04:08:20.890557] Averaged stats: pixel-level F1: [local: 0.2579 | reduced: 0.3381]
[04:08:20.893925] 
[ROBUST TEST] ColorJitterWrapper param=0.6
[04:08:20.894859] [DEBUG] [1/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/coverage_annotations_only_fake.json
[04:08:22.232911] Test: [0]  [0/2]  eta: 0:00:02  pixel-level F1: [local: 0.6393 | reduced: 0.6393]  time: 1.3341  data: 1.0240  max mem: 5776
[04:08:22.493124] ====================
[04:08:22.493195] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 11: The default batch size is 12
[04:08:22.493214] ====================
[04:08:22.512341] Test: [0]  [1/2]  eta: 0:00:00  pixel-level F1: [local: 0.5466 | reduced: 0.5950]  time: 0.8066  data: 0.5120  max mem: 5776
[04:08:22.586940] Test: [0] Total time: 0:00:01 (0.8443 s / it)
[04:08:22.587047] ***************************************************************
[04:08:22.587067] ****An extra tail dataset should exist for accracy metrics!****
[04:08:22.587084] ***************************************************************
[04:08:22.587102] **** Length of tail: 43 ****
[04:08:23.134261] Actual Batchsize/ world_size {'_n': 3.0}
[04:08:23.134389] {'pixel-level F1': tensor(1.8139, device='cuda:0', dtype=torch.float64)}
[04:08:23.156088] Test <remaining>: [0]  [0/4]  eta: 0:00:02  pixel-level F1: [local: 0.6046 | reduced: 0.5961]  time: 0.5686  data: 0.2541  max mem: 5776
[04:08:23.658279] Actual Batchsize/ world_size {'_n': 3.0}
[04:08:23.658394] {'pixel-level F1': tensor(1.3683, device='cuda:0', dtype=torch.float64)}
[04:08:23.680027] Test <remaining>: [0]  [1/4]  eta: 0:00:01  pixel-level F1: [local: 0.5466 | reduced: 0.5816]  time: 0.5461  data: 0.2316  max mem: 5776
[04:08:24.218670] Actual Batchsize/ world_size {'_n': 3.0}
[04:08:24.218789] {'pixel-level F1': tensor(1.0227, device='cuda:0', dtype=torch.float64)}
[04:08:24.240459] Test <remaining>: [0]  [2/4]  eta: 0:00:01  pixel-level F1: [local: 0.5466 | reduced: 0.5590]  time: 0.5507  data: 0.2363  max mem: 5776
[04:08:24.555728] ====================
[04:08:24.555836] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 7: The default batch size is 12
[04:08:24.555856] ====================
[04:08:24.556290] Actual Batchsize/ world_size {'_n': 1.75}
[04:08:24.556352] {'pixel-level F1': tensor(0.9424, device='cuda:0', dtype=torch.float64)}
[04:08:24.567853] Test <remaining>: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.5385 | reduced: 0.5580]  time: 0.4948  data: 0.2104  max mem: 5776
[04:08:24.568008] Test <remaining>: [0] Total time: 0:00:01 (0.4952 s / it)
[04:08:24.568655] ---syncronized---
[04:08:24.568701] pixel-level F1 reduced_count 135
[04:08:24.568731] pixel-level F1 reduced_sum 62.64831694957259
[04:08:24.568762] ---syncronized done ---
[04:08:27.081706] Averaged stats: pixel-level F1: [local: 0.5385 | reduced: 0.4641]
[04:08:27.085086] [DEBUG] [2/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/CASIA1_annotations_only_fake.json
[04:08:28.398917] Test: [0]  [ 0/20]  eta: 0:00:26  pixel-level F1: [local: 0.3745 | reduced: 0.3745]  time: 1.3070  data: 0.9961  max mem: 5776
[04:08:34.002525] ====================
[04:08:34.002616] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 2: The default batch size is 12
[04:08:34.002636] ====================
[04:08:34.004358] Test: [0]  [19/20]  eta: 0:00:00  pixel-level F1: [local: 0.7558 | reduced: 0.7238]  time: 0.3456  data: 0.0499  max mem: 5776
[04:08:34.090251] Test: [0] Total time: 0:00:06 (0.3499 s / it)
[04:08:34.090355] ***************************************************************
[04:08:34.090376] ****An extra tail dataset should exist for accracy metrics!****
[04:08:34.090392] ***************************************************************
[04:08:34.090410] **** Length of tail: 8 ****
[04:08:34.395539] ====================
[04:08:34.395618] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 8: The default batch size is 12
[04:08:34.395637] ====================
[04:08:34.396050] Actual Batchsize/ world_size {'_n': 2.0}
[04:08:34.396112] {'pixel-level F1': tensor(1.8543, device='cuda:0', dtype=torch.float64)}
[04:08:34.409505] Test <remaining>: [0]  [0/1]  eta: 0:00:00  pixel-level F1: [local: 0.7649 | reduced: 0.7256]  time: 0.3187  data: 0.1063  max mem: 5776
[04:08:34.409618] Test <remaining>: [0] Total time: 0:00:00 (0.3191 s / it)
[04:08:34.410155] ---syncronized---
[04:08:34.410186] pixel-level F1 reduced_count 928
[04:08:34.410217] pixel-level F1 reduced_sum 673.4241822820027
[04:08:34.410249] ---syncronized done ---
[04:08:34.809487] Averaged stats: pixel-level F1: [local: 0.7649 | reduced: 0.7257]
[04:08:34.810655] [DEBUG] [3/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/columbia_annotations_only_fake.json
[04:08:36.591733] Test: [0]  [0/4]  eta: 0:00:07  pixel-level F1: [local: 0.8203 | reduced: 0.8203]  time: 1.7768  data: 1.4668  max mem: 5776
[04:08:37.430613] ====================
[04:08:37.430709] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:08:37.430728] ====================
[04:08:37.446087] Test: [0]  [3/4]  eta: 0:00:00  pixel-level F1: [local: 0.8203 | reduced: 0.8289]  time: 0.6577  data: 0.3668  max mem: 5776
[04:08:37.524228] Test: [0] Total time: 0:00:02 (0.6774 s / it)
[04:08:37.524330] ***************************************************************
[04:08:37.524351] ****An extra tail dataset should exist for accracy metrics!****
[04:08:37.524367] ***************************************************************
[04:08:37.524384] **** Length of tail: 36 ****
[04:08:38.475230] Actual Batchsize/ world_size {'_n': 3.0}
[04:08:38.475367] {'pixel-level F1': tensor(2.9275, device='cuda:0', dtype=torch.float64)}
[04:08:38.496942] Test <remaining>: [0]  [0/3]  eta: 0:00:02  pixel-level F1: [local: 0.8422 | reduced: 0.8381]  time: 0.9721  data: 0.6566  max mem: 5776
[04:08:39.417720] Actual Batchsize/ world_size {'_n': 3.0}
[04:08:39.417840] {'pixel-level F1': tensor(2.2392, device='cuda:0', dtype=torch.float64)}
[04:08:39.439388] Test <remaining>: [0]  [1/3]  eta: 0:00:01  pixel-level F1: [local: 0.8203 | reduced: 0.8327]  time: 0.9571  data: 0.6417  max mem: 5776
[04:08:40.302970] Actual Batchsize/ world_size {'_n': 3.0}
[04:08:40.303089] {'pixel-level F1': tensor(2.7245, device='cuda:0', dtype=torch.float64)}
[04:08:40.324754] Test <remaining>: [0]  [2/3]  eta: 0:00:00  pixel-level F1: [local: 0.8422 | reduced: 0.8369]  time: 0.9330  data: 0.6178  max mem: 5776
[04:08:40.324874] Test <remaining>: [0] Total time: 0:00:02 (0.9334 s / it)
[04:08:40.325431] ---syncronized---
[04:08:40.325465] pixel-level F1 reduced_count 216
[04:08:40.325496] pixel-level F1 reduced_sum 183.54597124854337
[04:08:40.325527] ---syncronized done ---
[04:08:42.776063] Averaged stats: pixel-level F1: [local: 0.8422 | reduced: 0.8497]
[04:08:42.779103] [DEBUG] [4/4] loading JsonDataset, path is /home/aigc_account_1/data01/AIGC/dataset/json_files/NIST16_test.json
[04:09:07.931991] Test: [0]  [ 0/12]  eta: 0:05:01  pixel-level F1: [local: 0.3681 | reduced: 0.3681]  time: 25.1471  data: 24.8359  max mem: 5776
[04:09:19.871160] ====================
[04:09:19.871274] A batch that is not fully loaded was detected at the end of the dataset. The actual batch size for this batch is 9: The default batch size is 12
[04:09:19.871295] ====================
[04:09:19.886608] Test: [0]  [11/12]  eta: 0:00:03  pixel-level F1: [local: 0.2219 | reduced: 0.2558]  time: 3.0918  data: 2.7895  max mem: 5776
[04:09:20.013808] Test: [0] Total time: 0:00:37 (3.1025 s / it)
[04:09:20.013929] ***************************************************************
[04:09:20.013949] ****An extra tail dataset should exist for accracy metrics!****
[04:09:20.013966] ***************************************************************
[04:09:20.013983] **** Length of tail: 36 ****
[04:09:41.547840] Actual Batchsize/ world_size {'_n': 3.0}
[04:09:41.547996] {'pixel-level F1': tensor(0.8184, device='cuda:0', dtype=torch.float64)}
[04:09:41.568912] Test <remaining>: [0]  [0/3]  eta: 0:01:04  pixel-level F1: [local: 0.2467 | reduced: 0.2562]  time: 21.5543  data: 21.2366  max mem: 5776
[04:10:03.336056] Actual Batchsize/ world_size {'_n': 3.0}
[04:10:03.336182] {'pixel-level F1': tensor(0.6118, device='cuda:0', dtype=torch.float64)}
[04:10:03.357730] Test <remaining>: [0]  [1/3]  eta: 0:00:43  pixel-level F1: [local: 0.2219 | reduced: 0.2551]  time: 21.6714  data: 21.3547  max mem: 5776
[04:10:24.221870] Actual Batchsize/ world_size {'_n': 3.0}
[04:10:24.221999] {'pixel-level F1': tensor(1.3391, device='cuda:0', dtype=torch.float64)}
[04:10:24.243627] Test <remaining>: [0]  [2/3]  eta: 0:00:21  pixel-level F1: [local: 0.2467 | reduced: 0.2589]  time: 21.4094  data: 21.0933  max mem: 5776
[04:10:24.243768] Test <remaining>: [0] Total time: 0:01:04 (21.4099 s / it)
[04:10:24.244473] ---syncronized---
[04:10:24.244505] pixel-level F1 reduced_count 600
[04:10:24.244536] pixel-level F1 reduced_sum 198.3911319563833
[04:10:24.244581] ---syncronized done ---
[04:10:26.436469] Averaged stats: pixel-level F1: [local: 0.2467 | reduced: 0.3307]
[04:10:26.440144] Total testing time: 1:08:19
